{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "### *Purpose*: This notebook is designed to demonstrate an iterative approach to developing a refined system that utilizes a language model for analyzing and scoring resumes based on specific criteria.\n",
        "\n",
        "### *Summary*: The primary task is to guide the model in understanding a set of criteria and then use that understanding to assign scores to various skills present in a given resume. The evaluation mechanism is iteratively refined across five main steps, where each iteration is designed to improve upon the shortcomings of the previous one."
      ],
      "metadata": {
        "id": "FIGyUY1yu8zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now first install all the necessary libraries required to execute all functionalities within this notebook."
      ],
      "metadata": {
        "id": "00m1Ml2zfFo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install PyMuPDF\n",
        "!pip install textract\n",
        "!pip install python-docx\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "Y1zs4rpHv25q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "370f267a-c863-48b3-8817-53d211098228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.23.3-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.3 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.23.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.23.3 PyMuPDFb-1.23.3\n",
            "Collecting textract\n",
            "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
            "Collecting argcomplete~=1.10.0 (from textract)\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting beautifulsoup4~=4.8.0 (from textract)\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==3.* (from textract)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docx2txt~=0.8 (from textract)\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting extract-msg<=0.29.* (from textract)\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20191110 (from textract)\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-pptx~=0.6.18 (from textract)\n",
            "  Downloading python_pptx-0.6.22-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six~=1.12.0 (from textract)\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting SpeechRecognition~=3.8.1 (from textract)\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xlrd~=1.2.0 (from textract)\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodome (from pdfminer.six==20191110->textract)\n",
            "  Downloading pycryptodome-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4~=4.8.0->textract) (2.5)\n",
            "Collecting imapclient==2.1.0 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting olefile>=0.46 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tzlocal>=2.1 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (5.0.1)\n",
            "Collecting compressed-rtf>=1.0.6 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ebcdic>=1.1.1 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (4.9.3)\n",
            "Requirement already satisfied: Pillow<=9.5.0,>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (9.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx~=0.6.18->textract)\n",
            "  Downloading XlsxWriter-3.1.4-py3-none-any.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docx2txt, compressed-rtf, olefile\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=856c9f0857f34c2a206b51d8d7073eda3e50a621c2a69352c4c519da9dea210b\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6185 sha256=4b06ebfc238c07f79fd9d5670339788d8b4ebbd77cd78807ac806efa6d753bc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/3e/48/e7d833ecc516c36f8966d310b1a6386db091a718f1ff3bf85c\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=5cb4115450142db7c2779ca7ab7ca65bf1a111faef2f8e11e79612b79549e3b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/39/c0/9eb1f7a42b4b38f6f333b6314d4ed11c46f12a0f7b78194f0d\n",
            "Successfully built docx2txt compressed-rtf olefile\n",
            "Installing collected packages: SpeechRecognition, ebcdic, docx2txt, compressed-rtf, chardet, argcomplete, XlsxWriter, xlrd, six, pycryptodome, olefile, beautifulsoup4, python-pptx, pdfminer.six, imapclient, extract-msg, textract\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 2.0.1\n",
            "    Uninstalling xlrd-2.0.1:\n",
            "      Successfully uninstalled xlrd-2.0.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.11.2\n",
            "    Uninstalling beautifulsoup4-4.11.2:\n",
            "      Successfully uninstalled beautifulsoup4-4.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.6.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "yfinance 0.2.28 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SpeechRecognition-3.8.1 XlsxWriter-3.1.4 argcomplete-1.10.3 beautifulsoup4-4.8.2 chardet-3.0.4 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.19.0 python-pptx-0.6.22 six-1.12.0 textract-1.6.5 xlrd-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184487 sha256=b725ad7dc3fe33ac163483889ad9947c4220afe8a97458f965f66d073cd55985\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the .env file to the directory `/content/` which contains the \"OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "Nowsb4wjiW76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code snippet accesses sensitive values like the OpenAI API key"
      ],
      "metadata": {
        "id": "KK-Q-875glez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export your API Key to environment variable\n",
        "# Upload the .env file to the directory \"/content/\"\n",
        "!pip install python-dotenv\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n56dqFfeigAJ",
        "outputId": "d004c03b-3c1c-4cd4-8e92-b8634332a9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "# Retrieve the API key from environment variable\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Set the API key for OpenAI\n",
        "openai.api_key = openai_api_key"
      ],
      "metadata": {
        "id": "-SCsTPU2MSvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the json file containing important information about the Job requirements which was generated in Assignment1, the file containing information about the filtered resumes along with their summary generated from Assignment3 and the final JSON file containing the score criteria generated in Assignment6"
      ],
      "metadata": {
        "id": "ZDZ8rj4CMlFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the first file\n",
        "print(\"Please upload the first file (filtered_applications_summary.json):\")\n",
        "uploaded1 = files.upload()\n",
        "\n",
        "# Check to ensure a file was uploaded. If not, prompt again.\n",
        "while len(uploaded1) == 0:\n",
        "    print(\"No file uploaded. Please upload the first file (filtered_applications_summary.json) again:\")\n",
        "    uploaded1 = files.upload()\n",
        "\n",
        "# Upload the second file\n",
        "print(\"Please upload the second file (requirements_output.json):\")\n",
        "uploaded2 = files.upload()\n",
        "\n",
        "# Check to ensure a file was uploaded. If not, prompt again.\n",
        "while len(uploaded2) == 0:\n",
        "    print(\"No file uploaded. Please upload the second file (requirements_output.json) again:\")\n",
        "    uploaded2 = files.upload()\n",
        "\n",
        "# Upload the third file\n",
        "print(\"Please upload the third file (criterion_and_string_match_output.txt):\")\n",
        "uploaded3 = files.upload()\n",
        "\n",
        "# Check to ensure a file was uploaded. If not, prompt again.\n",
        "while len(uploaded3) == 0:\n",
        "    print(\"No file uploaded. Please upload the third file (criterion_and_string_match_output.txt) again:\")\n",
        "    uploaded3 = files.upload()\n",
        "\n",
        "# Merge the dictionaries to have all uploaded files in one\n",
        "uploaded = {**uploaded1, **uploaded2, **uploaded3}\n",
        "\n",
        "# Print details of uploaded files\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "GW9JnF_qMl-0",
        "outputId": "0d7708e8-43bf-466a-8732-b90d5b0d1f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload the first file (filtered_applications_summary.json):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-afc2eb93-5c75-499d-aa28-f65b7776f92a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-afc2eb93-5c75-499d-aa28-f65b7776f92a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving filtered_applications_summary.json to filtered_applications_summary.json\n",
            "Please upload the second file (requirements_output.json):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-28de33e4-5ea9-4e95-aa03-b9d1a25a784a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-28de33e4-5ea9-4e95-aa03-b9d1a25a784a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving requirements_output (3).json to requirements_output (3).json\n",
            "Please upload the third file (criterion_and_string_match_output.txt):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0da091b8-3e79-442a-a9e8-1c3edf3937be\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0da091b8-3e79-442a-a9e8-1c3edf3937be\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving criterion_and_string_match_output (1).txt to criterion_and_string_match_output (1).txt\n",
            "User uploaded file \"filtered_applications_summary.json\" with length 13380 bytes\n",
            "User uploaded file \"requirements_output (3).json\" with length 810 bytes\n",
            "User uploaded file \"criterion_and_string_match_output (1).txt\" with length 2531 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now download the `Webinar_resumes.zip` file which contains all the resumes\n"
      ],
      "metadata": {
        "id": "FD9p5Sz3MoJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(file_id, destination):\n",
        "    base_url = \"https://drive.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(base_url, params={'id': file_id}, stream=True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = {'id': file_id, 'confirm': token}\n",
        "        response = session.get(base_url, params=params, stream=True)\n",
        "\n",
        "    save_response_content(response, destination)\n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "# Example Usage\n",
        "file_id = '17V_o0Snt-Lj0FmegENPQ_rXpvWTWlZgQ'\n",
        "destination = 'Webinar_resumes.zip'  # Replace with your desired file name and extension\n",
        "download_file_from_google_drive(file_id, destination)"
      ],
      "metadata": {
        "id": "OjpqSCGBMw_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Import Statements`: Essential libraries are imported. These include:\n",
        "1. `openai` for interacting with the OpenAI API.\n",
        "2. Utilities like `json`, `os`, and `re` for handling data and file operations.\n",
        "3. Libraries for reading specific file formats (`docx` for Word documents, `textract` for older Word format, and `fitz` for PDFs).\n",
        "4. `pandas` for reading Excel files.\n",
        "5. `nltk` for natural language processing tasks.\n",
        "\n",
        "`summarize_resume` Function:\n",
        "\n",
        "1. This function interfaces with the GPT-3.5 model to summarize the content of a resume.\n",
        "2. The model is provided with a system message (prompt) and user message (resume text). It then returns a summarized version of the resume.\n",
        "\n",
        "`read_requirements` Function:\n",
        "\n",
        "1. Reads job requirements from a JSON file and returns the data. It includes error handling to manage potential reading errors.\n",
        "\n",
        "`read_json` Function:\n",
        "1. Simplified function to read data from a JSON file and return it.\n",
        "\n",
        "`read_document` Function:\n",
        "\n",
        "1. Reads content from various file types including .docx, .doc, .pdf, .xls, and .xlsx.\n",
        "2. Depending on the file type, different libraries/methods are utilized to extract the text.\n",
        "\n",
        "`check_and_trim` Function:\n",
        "1. Utilizes the nltk library to tokenize the text of a resume.\n",
        "2. If the text exceeds a specified token count (default is 1500 tokens), the function trims the text to fit within the limit.\n",
        "3. Returns the trimmed text and the original and new token lengths."
      ],
      "metadata": {
        "id": "yMgZeonCgsVx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0_XKFNSvt31"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import re\n",
        "from docx import Document\n",
        "import textract\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import math\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "def read_requirements(file_path):\n",
        "    # Reads the job requirements from a JSON file\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading requirements JSON: {e}\")\n",
        "        return None\n",
        "\n",
        "def read_json(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def read_document(file_path):\n",
        "    file_path = str(file_path)\n",
        "    _, file_extension = os.path.splitext(file_path)\n",
        "    text = \"\"\n",
        "    if file_extension == '.docx':\n",
        "        doc = Document(file_path)\n",
        "        for para in doc.paragraphs:\n",
        "            text = text + para.text + \" \"\n",
        "    elif file_extension == '.doc':\n",
        "        text = textract.process(file_path).decode()\n",
        "    elif file_extension.lower() == '.pdf':\n",
        "        doc = fitz.open(file_path)\n",
        "        for page_number in range(len(doc)):\n",
        "            page = doc[page_number]\n",
        "            text = text + page.get_text() + \" \"\n",
        "    elif file_extension.lower() in ['.xls', '.xlsx']:\n",
        "        data = pd.read_excel(file_path)\n",
        "        text = data.to_string(index=False)\n",
        "\n",
        "    else:\n",
        "        print(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def check_and_trim(resume_text, max_tokens=1500):\n",
        "    # tokens = nltk.word_tokenize(resume_text)\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tokens = enc.encode(resume_text)\n",
        "    old_len = len(tokens)\n",
        "    if len(tokens) > max_tokens:\n",
        "        tokens = tokens[:max_tokens]\n",
        "        resume_text = enc.decode(tokens)\n",
        "    return resume_text, old_len, len(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The provided code allows a user to select a desired number of resumes to process from a total set, with a default of 2 resumes if no input is given. The **user_select_number_of_resumes** function prompts the user for their choice, ensures valid input, and returns the selected number. The main execution block reads the **filtered_applications_summary** data from a JSON file, queries the user for their desired number of resumes using the aforementioned function, and then randomly selects the specified number of resumes from the total set, storing the result in the **selected_applications** variable."
      ],
      "metadata": {
        "id": "wGB2beTLkx_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "\n",
        "def user_select_number_of_resumes(total_resumes, default=2):\n",
        "    \"\"\"\n",
        "    Allow the user to input a number of resumes to process.\n",
        "    If no input is given, the default value is returned.\n",
        "\n",
        "    Args:\n",
        "    - total_resumes (int): Total number of resumes available.\n",
        "    - default (int): The default number to return if no input.\n",
        "\n",
        "    Returns:\n",
        "    - int: The number of resumes the user wants to process.\n",
        "    \"\"\"\n",
        "    print(f\"Total resumes available: {total_resumes}\")\n",
        "    user_input = input(f\"How many resumes do you want to process? (Default is {default}): \")\n",
        "\n",
        "    # If the user doesn't provide any input, return the default value.\n",
        "    if not user_input:\n",
        "        return default\n",
        "\n",
        "    try:\n",
        "        # Convert user input to an integer and ensure it's within the range.\n",
        "        selected_num = int(user_input)\n",
        "        if 1 <= selected_num <= total_resumes:\n",
        "            return selected_num\n",
        "        else:\n",
        "            print(f\"Please select a number between 1 and {total_resumes}.\")\n",
        "            return user_select_number_of_resumes(total_resumes, default)\n",
        "    except ValueError:\n",
        "        # If the user provides non-numeric input, prompt them again.\n",
        "        print(\"Please enter a valid number.\")\n",
        "        return user_select_number_of_resumes(total_resumes, default)\n",
        "\n",
        "# Read the filtered_applications_summary data from the JSON file\n",
        "json_data = read_json('/content/filtered_applications_summary.json')\n",
        "\n",
        "# Display total resumes and get the user's choice\n",
        "n = user_select_number_of_resumes(len(json_data))\n",
        "\n",
        "# Randomly select n resumes\n",
        "selected_applications = random.sample(json_data, n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdQwo5wqkyxD",
        "outputId": "7c08f498-dca7-40e4-b37a-489c10b95ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total resumes available: 6\n",
            "How many resumes do you want to process? (Default is 2): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code consists of two main functions: `score_from_criterion` and `read_from_textfile`. The `score_from_criterion` function takes in a prompt, a text, a previously generated text, a \"must-have\" criterion, and a string matching criterion. This function communicates with the GPT-3.5 Turbo model via the OpenAI API to generate a response. The purpose is to score a given text (like a resume) against specific criteria. The function returns the generated response from the model. The second function, `read_from_textfile`, reads content from a specified filename. It specifically extracts two sections from the content: the criterion generated output and a string match, which were saved by running the previous `Assignment6`. These sections are demarcated in the text file by specific headers, and the function returns the contents of these two sections separately. This extracted data can then be utilized to evaluate or assess the content of the file against certain predefined criteria.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Understanding the roles\n",
        "**assistant** - We use the role assistant for the critera that we generated in the last assignment.\n",
        "\n",
        "**system** - We define the high level function that we want the API to return in the system,\n",
        "\n",
        "**user** - We give the details from resumes which will change every time in user field as input to be processed."
      ],
      "metadata": {
        "id": "0-8s6dlmh7eW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score_from_criterion(prompt, text, generated_text, must_have, string_match):\n",
        "    model=\"gpt-3.5-turbo-16k\"\n",
        "    max_tokens=2000\n",
        "    # print(\"prompt\", prompt)\n",
        "    messages = [\n",
        "            {\"role\": \"assistant\", \"content\": f\"{generated_text}\"},\n",
        "            {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{text}\"},\n",
        "        ]\n",
        "    response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-16k\",\n",
        "            messages=messages,\n",
        "            temperature=0,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "    generated_texts = [\n",
        "        choice.message[\"content\"].strip() for choice in response[\"choices\"]\n",
        "    ]\n",
        "    return generated_texts[0]\n",
        "\n",
        "def read_from_textfile(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Separate criterion_gen_output and string_match\n",
        "    criterion_start = content.find(\"----Criterion Generated Output----\") + len(\"----Criterion Generated Output----\")\n",
        "    criterion_end = content.find(\"----String Match----\")\n",
        "\n",
        "    criterion_gen_output = content[criterion_start:criterion_end].strip()\n",
        "    string_match = content[criterion_end + len(\"----String Match----\"):].strip()\n",
        "\n",
        "    return criterion_gen_output, string_match\n"
      ],
      "metadata": {
        "id": "0wYv72kSwtUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code provides functionality to extract and reorganize files from a given zip archive. After reading job requirements from a JSON file, the **extract_and_rename** function unzips the contents of a specified zip file (like \"**Webinar_resumes.zip**\") into a directory (defaulted as \"**extracted_files**\"). If the directory to extract to doesn't exist, it's created; if it's already populated, extraction is skipped. Post-extraction, the function scans the contents, and if it finds any directories with spaces in their names, it renames them by replacing spaces with underscores. If the directory with the new name already exists, it transfers files from the old directory to the new one and then deletes the old directory. The function finally returns the path of the reorganized or main content directory. The main execution block then calls this function with the given zip file path and stores the result in the **resume_path** variable."
      ],
      "metadata": {
        "id": "ckj9hcR7gvQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import shutil\n",
        "job_requirements = read_requirements('/content/requirements_output.json')\n",
        "must_have_skills = job_requirements[\"must_have_skills\"]\n",
        "zip_file_path = \"/content/Webinar_resumes.zip\" # For example give the path to resume_data.zip\n",
        "\n",
        "def extract_and_rename(zip_file_path, extract_path=\"extracted_files\"):\n",
        "    \"\"\"\n",
        "    Extract files from a zip archive to a specified directory.\n",
        "    Rename directories containing spaces to use underscores instead.\n",
        "\n",
        "    Args:\n",
        "    - zip_file_path (str): The path to the zip file to be extracted.\n",
        "    - extract_path (str, optional): The path where the zip file content should be extracted to.\n",
        "                                    Defaults to \"extracted_files\".\n",
        "\n",
        "    Returns:\n",
        "    - str: Path to the resume or directory.\n",
        "    \"\"\"\n",
        "    # Check if extract_path exists, if not, create it\n",
        "    if not os.path.exists(extract_path):\n",
        "        os.makedirs(extract_path)\n",
        "\n",
        "    # If extract_path is not empty, skip extraction\n",
        "    if not os.listdir(extract_path):\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "\n",
        "    resume_path = extract_path\n",
        "    for item in os.listdir(extract_path):\n",
        "        item_path = os.path.join(extract_path, item)\n",
        "\n",
        "        # Check if the current item is a directory and if it has spaces in its name\n",
        "        if os.path.isdir(item_path) and ' ' in item:\n",
        "            new_name = item.replace(' ', '_')\n",
        "            new_path = os.path.join(extract_path, new_name)\n",
        "\n",
        "            # If the new directory name doesn't already exist, create it\n",
        "            if not os.path.exists(new_path):\n",
        "                os.makedirs(new_path)\n",
        "\n",
        "            # Copying contents from the old directory to the new one\n",
        "            for sub_item in os.listdir(item_path):\n",
        "                shutil.copy2(os.path.join(item_path, sub_item), new_path)\n",
        "\n",
        "            # Removing the old directory\n",
        "            shutil.rmtree(item_path)\n",
        "            resume_path = new_path\n",
        "        else:\n",
        "            resume_path = item_path\n",
        "\n",
        "    return resume_path\n",
        "resume_path = extract_and_rename(zip_file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "4fgCV1u2v43j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iteration 1:\n",
        "### *Goal*: Introduce the model to the task of using the generated criteria to judge a resume and give scores to each of the skills.\n",
        "### *Prompt*:\n",
        "```\n",
        "# You are an assistant to a recruiter. Use the criteria given by {criterion_gen_output} to judge the resume given to you and give \\\n",
        "score to each of the skills present in {must_have_skills}.\n",
        "```\n",
        "\n",
        "### *Reason for Change*: We start with a basic prompt to get the inital results from the assistant.\n"
      ],
      "metadata": {
        "id": "LmZ-fD7gHPrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_score_func(resume_text, must_have_skills):\n",
        "    # Read the criteria and string_match from the text file\n",
        "    criterion_gen_output, string_match = read_from_textfile(\"/content/criterion_and_string_match_output.txt\")\n",
        "    prompt=f'''You are an assistant to a recruiter. \\\n",
        "    Use the criteria given by {criterion_gen_output} to judge the resume given to you and give score to \\\n",
        "    each of the skills present in {must_have_skills}.'''\n",
        "\n",
        "    # Assuming the function score_from_criterion exists and takes these parameters\n",
        "    score_output = score_from_criterion(prompt, resume_text, criterion_gen_output, must_have_skills, string_match)\n",
        "\n",
        "    return score_output\n",
        "\n",
        "for application in selected_applications:\n",
        "    if 'resume_path' in application and 'email_id' in application:\n",
        "\n",
        "        # Extract resume text\n",
        "        resume_text = read_document(os.path.join(resume_path, application['resume_path']))\n",
        "        resume_text, _, _ = check_and_trim(resume_text)\n",
        "\n",
        "        # Directly assign the resume_summary without json.loads()\n",
        "        resume_summary = application['resume_summary']\n",
        "\n",
        "        final_score_out = final_score_func(resume_text, must_have_skills)\n",
        "        print(f'''[Score Request] for {resume_summary[\"name_of_candidate\"]} ''', final_score_out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVagtcV0z-PO",
        "outputId": "14af5e07-8841-4b4a-de24-ab6cc8615d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Score Request] for Soso Sukhitashvili  Based on the information provided in the resume, the scores for each skill can be given as follows:\n",
            "\n",
            "- TensorFlow: \n",
            "  - Score: 0\n",
            "  - Justification: The candidate has no mention or projects related to TensorFlow.\n",
            "\n",
            "- Keras: \n",
            "  - Score: 2\n",
            "  - Justification: The candidate mentions experience in computer vision projects, which likely involved the use of Keras.\n",
            "\n",
            "- PyTorch: \n",
            "  - Score: 5\n",
            "  - Justification: The candidate mentions working as a deep learning engineer and algorithm developer, with a focus on computer vision projects. They specifically mention using PyTorch, indicating significant experience in this skill.\n",
            "\n",
            "- Computer Vision: \n",
            "  - Score: 3\n",
            "  - Justification: The candidate mentions working on computer vision projects, including face recognition, image similarity search, object tracking, object detection, object segmentation, and OCR. This indicates good experience in computer vision.\n",
            "\n",
            "The scores for each skill can be summarized as follows:\n",
            "\n",
            "{\n",
            "  \"TensorFlow\": {\n",
            "    \"score\": 0,\n",
            "    \"justification\": \"The candidate has no experience in TensorFlow.\"\n",
            "  },\n",
            "  \"Keras\": {\n",
            "    \"score\": 2,\n",
            "    \"justification\": \"The candidate has some experience in Keras.\"\n",
            "  },\n",
            "  \"PyTorch\": {\n",
            "    \"score\": 5,\n",
            "    \"justification\": \"The candidate has significant experience in PyTorch, as mentioned in the resume.\"\n",
            "  },\n",
            "  \"Computer Vision\": {\n",
            "    \"score\": 3,\n",
            "    \"justification\": \"The candidate has good experience in Computer Vision.\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output:\n",
        "\n",
        "\n",
        "```\n",
        "[Score Request] for Abhilash Babu  Based on the provided resume, the scores for each skill can be given as follows:\n",
        "\n",
        "- TensorFlow:\n",
        "  - Score: 5\n",
        "  - Justification: The candidate has mentioned experience in developing deep learning models using TensorFlow for detecting fraudulent identity documents and optimizing model performance.\n",
        "\n",
        "- Keras:\n",
        "  - Score: 5\n",
        "  - Justification: The candidate has mentioned experience in developing deep learning models using Keras for various scenarios of identity verification in the KYC domain.\n",
        "\n",
        "- PyTorch:\n",
        "  - Score: 5\n",
        "  - Justification: The candidate has mentioned experience in developing deep learning models using PyTorch for tasks such as facial attribute detection, object detection, and background elimination. They have also mentioned optimizing and tuning SOTA models using Apache TVM.\n",
        "\n",
        "- Computer Vision:\n",
        "  - Score: 5\n",
        "  - Justification: The candidate has mentioned 18 years of experience in successfully delivering projects in the domain of Computer Vision and Image processing. They have developed machine learning solutions for tasks such as object detection, image classification, and image segmentation.\n",
        "\n",
        "The final scores for each skill are as follows:\n",
        "\n",
        "{\n",
        "  \"TensorFlow\": {\n",
        "    \"score\": 5,\n",
        "    \"justification\": \"The candidate has mentioned experience in developing deep learning models using TensorFlow for detecting fraudulent identity documents and optimizing model performance.\"\n",
        "  },\n",
        "  \"Keras\": {\n",
        "    \"score\": 5,\n",
        "    \"justification\": \"The candidate has mentioned experience in developing deep learning models using Keras for various scenarios of identity verification in the KYC domain.\"\n",
        "  },\n",
        "  \"PyTorch\": {\n",
        "    \"score\": 5,\n",
        "    \"justification\": \"The candidate has mentioned experience in developing deep learning models using PyTorch for tasks such as facial attribute detection, object detection, and background elimination. They have also mentioned optimizing and tuning SOTA models using Apache TVM.\"\n",
        "  },\n",
        "  \"Computer Vision\": {\n",
        "    \"score\": 5,\n",
        "    \"justification\": \"The candidate has mentioned 18 years of experience in successfully delivering projects in the domain of Computer Vision and Image processing. They have developed machine learning solutions for tasks such as object detection, image classification, and image segmentation.\"\n",
        "  }\n",
        "}\n",
        "[Score Request] for Pankaj Kumar Goyal  Based on the provided resume, the scores for each skill can be given as follows:\n",
        "\n",
        "- TensorFlow: 2\n",
        "- Keras: 2\n",
        "- PyTorch: 0\n",
        "- Computer Vision: 5\n",
        "\n",
        "Justifications:\n",
        "\n",
        "- TensorFlow: The candidate has mentioned experience in Deep learning, which often involves the use of TensorFlow. However, there are no specific projects mentioned, so the score is 2.\n",
        "- Keras: The candidate has mentioned experience in Deep learning, which often involves the use of Keras. There is one project mentioned that involves fine-tuning a BERT model, so the score is 2.\n",
        "- PyTorch: There is no mention of PyTorch in the resume, so the score is 0.\n",
        "- Computer Vision: The candidate has mentioned a project related to skin cancer classification using CNN and CNN+LSTM, which indicates experience in Computer Vision. The score is 5.\n",
        "\n",
        "The scores and justifications can be given in JSON format as shown below:\n",
        "\n",
        "{\n",
        "  \"TensorFlow\": {\n",
        "    \"score\": 2,\n",
        "    \"justification\": \"The candidate has some experience in Deep learning, but no specific projects mentioned.\"\n",
        "  },\n",
        "  \"Keras\": {\n",
        "    \"score\": 2,\n",
        "    \"justification\": \"The candidate has experience in Deep learning and has mentioned a project involving fine-tuning a BERT model.\"\n",
        "  },\n",
        "  \"PyTorch\": {\n",
        "    \"score\": 0,\n",
        "    \"justification\": \"There is no mention of PyTorch in the resume.\"\n",
        "  },\n",
        "  \"Computer Vision\": {\n",
        "    \"score\": 5,\n",
        "    \"justification\": \"The candidate has experience in Computer Vision, as indicated by the skin cancer classification project.\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "5DsNMV4Lml5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iteration 2:\n",
        "### *Goal*: Refine the output by asking the model to return it in a JSON format to structure the data.\n",
        "\n",
        "### *Prompt*:\n",
        "\n",
        "\n",
        "```\n",
        "You are an assistant to a recruiter. Use the criteria given by {criterion_gen_output} to judge the resume given to you and give \\\n",
        "score to each of the skills present in {must_have_skills}. \\\n",
        "Return the output in JSON format.\n",
        "```\n",
        "\n",
        "### *Reason for Change*: The problem with the above is that, there are a lot of stuff in the function output that are irrelevant. So in order to fix it we ask the assistant to return the final output in JSON format.\n"
      ],
      "metadata": {
        "id": "MbpXkUtaHg_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def final_score_func(resume_text, must_have_skills):\n",
        "    # Read the criteria and string_match from the text file\n",
        "    criterion_gen_output, string_match = read_from_textfile(\"/content/criterion_and_string_match_output.txt\")\n",
        "    prompt=f'''You are an assistant to a recruiter. \\\n",
        "    Use the criteria given by {criterion_gen_output} to judge the resume given to you and give score to \\\n",
        "    each of the skills present in {must_have_skills}. Return the output in JSON format.'''\n",
        "\n",
        "    # Assuming the function score_from_criterion exists and takes these parameters\n",
        "    score_output = score_from_criterion(prompt, resume_text, criterion_gen_output, must_have_skills, string_match)\n",
        "\n",
        "    return score_output\n",
        "\n",
        "for application in selected_applications:\n",
        "    if 'resume_path' in application and 'email_id' in application:\n",
        "\n",
        "        # Extract resume text\n",
        "        resume_text = read_document(os.path.join(resume_path, application['resume_path']))\n",
        "        resume_text, _, _ = check_and_trim(resume_text)\n",
        "\n",
        "        # Directly assign the resume_summary without json.loads()\n",
        "        resume_summary = application['resume_summary']\n",
        "\n",
        "        final_score_out = final_score_func(resume_text, must_have_skills)\n",
        "        print(f'''[Score Request] for {resume_summary[\"name_of_candidate\"]} ''', final_score_out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNOmTf_w1gTC",
        "outputId": "bf12546c-593c-4180-9cf1-c8bea601a9c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Score Request] for Soso Sukhitashvili  {\n",
            "  \"TensorFlow\": {\n",
            "    \"score\": 0,\n",
            "    \"justification\": \"The candidate has no experience in TensorFlow.\"\n",
            "  },\n",
            "  \"Keras\": {\n",
            "    \"score\": 2,\n",
            "    \"justification\": \"The candidate has some experience in Keras, as mentioned in the resume.\"\n",
            "  },\n",
            "  \"PyTorch\": {\n",
            "    \"score\": 5,\n",
            "    \"justification\": \"The candidate has significant experience in PyTorch, as mentioned in the resume.\"\n",
            "  },\n",
            "  \"Computer Vision\": {\n",
            "    \"score\": 5,\n",
            "    \"justification\": \"The candidate has good experience in Computer Vision, as mentioned in the resume.\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output:\n",
        "\n",
        "\n",
        "```\n",
        "[Score Request] for Abhilash Babu  Based on the provided resume, the scores for each skill can be given as follows:\n",
        "\n",
        "{\n",
        "  \"TensorFlow\": {\n",
        "    \"score\": 2,\n",
        "    \"justification\": \"The candidate has mentioned experience in developing deep learning models using TensorFlow in their current and previous roles.\"\n",
        "  },\n",
        "  \"Keras\": {\n",
        "    \"score\": 2,\n",
        "    \"justification\": \"The candidate has mentioned experience in using Keras for developing machine learning models in their technical skills section.\"\n",
        "  },\n",
        "  \"PyTorch\": {\n",
        "    \"score\": 2,\n",
        "    \"justification\": \"The candidate has mentioned experience in using PyTorch and PyTorch-Lightning for developing machine learning models in their technical skills section.\"\n",
        "  },\n",
        "  \"Computer Vision\": {\n",
        "    \"score\": 5,\n",
        "    \"justification\": \"The candidate has mentioned 18 years of experience in delivering projects in the domain of Computer Vision and Image processing. They have also mentioned developing and deploying machine learning solutions for various applications in Computer Vision.\"\n",
        "  }\n",
        "}\n",
        "[Score Request] for Pankaj Kumar Goyal  Based on the provided resume, the scores for each skill can be given as follows:\n",
        "\n",
        "{\n",
        "  \"TensorFlow\": {\n",
        "    \"score\": 2,\n",
        "    \"justification\": \"The candidate has mentioned experience in TensorFlow in the skills section.\"\n",
        "  },\n",
        "  \"Keras\": {\n",
        "    \"score\": 2,\n",
        "    \"justification\": \"The candidate has mentioned experience in Keras in the skills section.\"\n",
        "  },\n",
        "  \"PyTorch\": {\n",
        "    \"score\": 0,\n",
        "    \"justification\": \"The candidate has no mention of PyTorch in the resume.\"\n",
        "  },\n",
        "  \"Computer Vision\": {\n",
        "    \"score\": 5,\n",
        "    \"justification\": \"The candidate has significant experience in Computer Vision, as mentioned in the resume.\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "uGEWxIlmnXI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iteration 3:\n",
        "### *Goal*: Improve the accuracy of the scores by emphasizing that scores should be based on skills found in the resume.\n",
        "\n",
        "### *Prompt*:\n",
        "\n",
        "\n",
        "```\n",
        "You are an assistant to a recruiter. Use the criteria given by {criterion_gen_output} to judge the resume given to you. \\\n",
        " Scores must be given with respect to each of the must have \\\n",
        " skills present here {must_have_skills}, which can be found inside \\\n",
        " the resume. Return the output in JSON format.\n",
        "```\n",
        "\n",
        "### *Reason for Change*: With the above prompt we are able to get rid of the irrelevant statements, but now we have seen in some cases that the scores are not accurate.\n"
      ],
      "metadata": {
        "id": "HR9FHKaR4Yux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def final_score_func(resume_text, must_have_skills):\n",
        "    # Read the criteria and string_match from the text file\n",
        "    criterion_gen_output, string_match = read_from_textfile(\"/content/criterion_and_string_match_output.txt\")\n",
        "    prompt=f'''You are an assistant to a recruiter. \\\n",
        "    Use the criteria given by {criterion_gen_output} to judge the resume given to you. \\\n",
        "    The scores must be given with respect to each of the must have skills present here {must_have_skills}, which \\\n",
        "    can be found inside the resume. \\\n",
        "    Return the output in JSON format.'''\n",
        "\n",
        "    # Assuming the function score_from_criterion exists and takes these parameters\n",
        "    score_output = score_from_criterion(prompt, resume_text, criterion_gen_output, must_have_skills, string_match)\n",
        "\n",
        "    return score_output\n",
        "\n",
        "for application in selected_applications:\n",
        "    if 'resume_path' in application and 'email_id' in application:\n",
        "\n",
        "        # Extract resume text\n",
        "        resume_text = read_document(os.path.join(resume_path, application['resume_path']))\n",
        "        resume_text, _, _ = check_and_trim(resume_text)\n",
        "\n",
        "        # Directly assign the resume_summary without json.loads()\n",
        "        resume_summary = application['resume_summary']\n",
        "\n",
        "        final_score_out = final_score_func(resume_text, must_have_skills)\n",
        "        print(f'''[Score Request] for {resume_summary[\"name_of_candidate\"]} ''', final_score_out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woDvBM-c11Ty",
        "outputId": "6f9c42fc-91e5-4815-a6f1-3aa2baf9ec62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Score Request] for Soso Sukhitashvili  {\n",
            "  \"TensorFlow\": {\n",
            "    \"score\": 0,\n",
            "    \"justification\": \"The candidate has no experience in TensorFlow.\"\n",
            "  },\n",
            "  \"Keras\": {\n",
            "    \"score\": 2,\n",
            "    \"justification\": \"The candidate has some experience in Keras, as mentioned in the resume.\"\n",
            "  },\n",
            "  \"PyTorch\": {\n",
            "    \"score\": 5,\n",
            "    \"justification\": \"The candidate has significant experience in PyTorch, as mentioned in the resume.\"\n",
            "  },\n",
            "  \"Computer Vision\": {\n",
            "    \"score\": 5,\n",
            "    \"justification\": \"The candidate has good experience in Computer Vision, as mentioned in the resume.\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output:\n",
        "\n",
        "```\n",
        "[Score Request] for Abhilash Babu  {\n",
        "  \"TensorFlow\": {\n",
        "    \"score\": 2,\n",
        "    \"justification\": \"The candidate has mentioned experience in developing deep learning models using TensorFlow in their previous role at Bundesdruckerei GmbH.\"\n",
        "  },\n",
        "  \"Keras\": {\n",
        "    \"score\": 2,\n",
        "    \"justification\": \"The candidate has mentioned experience in using Keras as one of the machine learning libraries they are familiar with.\"\n",
        "  },\n",
        "  \"PyTorch\": {\n",
        "    \"score\": 5,\n",
        "    \"justification\": \"The candidate has mentioned significant experience in developing deep learning models using PyTorch in their previous role at Bundesdruckerei GmbH. They have also mentioned using PyTorch-Lightning, a PyTorch wrapper library.\"\n",
        "  },\n",
        "  \"Computer Vision\": {\n",
        "    \"score\": 5,\n",
        "    \"justification\": \"The candidate has mentioned 18 years of experience in delivering projects in the domain of Computer Vision and Image processing. They have also mentioned developing and deploying machine learning solutions for various computer vision applications.\"\n",
        "  }\n",
        "}\n",
        "[Score Request] for Pankaj Kumar Goyal  {\n",
        "  \"TensorFlow\": {\n",
        "    \"score\": 2,\n",
        "    \"justification\": \"The candidate has mentioned experience in TensorFlow in the skills section.\"\n",
        "  },\n",
        "  \"Keras\": {\n",
        "    \"score\": 2,\n",
        "    \"justification\": \"The candidate has mentioned experience in Keras in the skills section.\"\n",
        "  },\n",
        "  \"PyTorch\": {\n",
        "    \"score\": 0,\n",
        "    \"justification\": \"The candidate has no mention of PyTorch in the resume.\"\n",
        "  },\n",
        "  \"Computer Vision\": {\n",
        "    \"score\": 5,\n",
        "    \"justification\": \"The candidate has significant experience in Computer Vision, as mentioned in the resume.\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "khqVtKbNqGGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, the development of the prompt was an iterative process with the goal of maximizing accuracy and minimizing irrelevant or redundant outputs. Each iteration was driven by feedback from the previous run, and adjustments were made to improve clarity and specificity."
      ],
      "metadata": {
        "id": "8ck6gdgerrrp"
      }
    }
  ]
}