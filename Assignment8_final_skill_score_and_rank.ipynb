{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Description\n",
        "This notebook automates the task of evaluating and ranking job applications in alignment with specific job prerequisites. Initially, the program identifies numerical values from textual content and rates candidates based on their work experience. Skill evaluation is undertaken through two distinct methods. The first approach deploys a binary classification: a score of `0` is assigned if a `must-have` skill is absent from the resume, and a score of `1` is given if it's present as shown in the `Assignment4`. The second scoring mechanism involves the formulation of a criteria set which is generated in `Assignment6`, wherein skills are rated on a scale from `0` to `5` based on their alignment and relevance to the job's requirements as we see in `Assignment7`. The final skill score for each candidate is derived by multiplying the scores obtained from both methods. After extracting and processing resume data from a zip archive, the script summarizes pivotal details from each resume, including the candidate's personal information and technical competencies. Subsequently, these extracted data points assist in score computation, which integrates both skill relevance and years of experience. Concluding the process, candidates are ranked by determining the mean value of their skill and experience scores, and the consolidated details, including their rankings, are showcased in order of their job compatibility."
      ],
      "metadata": {
        "id": "oonkniwdGtay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now first install all the necessary libraries required to execute all functionalities within this notebook."
      ],
      "metadata": {
        "id": "GxDN3LQwTo3x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QmgPkHnbRACw",
        "outputId": "aca20e4b-bb9e-4874-b081-e7a4ec4a59db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.23.3-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.3 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.23.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.23.3 PyMuPDFb-1.23.3\n",
            "Collecting textract\n",
            "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
            "Collecting argcomplete~=1.10.0 (from textract)\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting beautifulsoup4~=4.8.0 (from textract)\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==3.* (from textract)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docx2txt~=0.8 (from textract)\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting extract-msg<=0.29.* (from textract)\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20191110 (from textract)\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-pptx~=0.6.18 (from textract)\n",
            "  Downloading python_pptx-0.6.22-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six~=1.12.0 (from textract)\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting SpeechRecognition~=3.8.1 (from textract)\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xlrd~=1.2.0 (from textract)\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodome (from pdfminer.six==20191110->textract)\n",
            "  Downloading pycryptodome-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4~=4.8.0->textract) (2.5)\n",
            "Collecting imapclient==2.1.0 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting olefile>=0.46 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tzlocal>=2.1 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (5.0.1)\n",
            "Collecting compressed-rtf>=1.0.6 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ebcdic>=1.1.1 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (4.9.3)\n",
            "Requirement already satisfied: Pillow<=9.5.0,>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (9.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx~=0.6.18->textract)\n",
            "  Downloading XlsxWriter-3.1.3-py3-none-any.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docx2txt, compressed-rtf, olefile\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=36135f58b2689c535516f1549a008d28d357e745dcf61c2a934efc611edcebef\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6185 sha256=df318034897c90ca65c9d48f6db01d1c1f6d6136647a32e3f17e168e11ea6ec8\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/3e/48/e7d833ecc516c36f8966d310b1a6386db091a718f1ff3bf85c\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=cab5a332ae916bb6037852953d66456902958d2e1f85ace68f12d24f4f43fd73\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/39/c0/9eb1f7a42b4b38f6f333b6314d4ed11c46f12a0f7b78194f0d\n",
            "Successfully built docx2txt compressed-rtf olefile\n",
            "Installing collected packages: SpeechRecognition, ebcdic, docx2txt, compressed-rtf, chardet, argcomplete, XlsxWriter, xlrd, six, pycryptodome, olefile, beautifulsoup4, python-pptx, pdfminer.six, imapclient, extract-msg, textract\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 2.0.1\n",
            "    Uninstalling xlrd-2.0.1:\n",
            "      Successfully uninstalled xlrd-2.0.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.11.2\n",
            "    Uninstalling beautifulsoup4-4.11.2:\n",
            "      Successfully uninstalled beautifulsoup4-4.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.6.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "yfinance 0.2.28 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SpeechRecognition-3.8.1 XlsxWriter-3.1.3 argcomplete-1.10.3 beautifulsoup4-4.8.2 chardet-3.0.4 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.18.0 python-pptx-0.6.22 six-1.12.0 textract-1.6.5 xlrd-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184487 sha256=45c6a986118b912ac741579a8c3626bb6aab0e21b024fb20c6fd7e364bf28676\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install PyMuPDF\n",
        "!pip install textract\n",
        "!pip install python-docx\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the .env file to the directory `/content/` which contains the \"OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "FwgwqxfgxoFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code snippet accesses sensitive values like the OpenAI API key"
      ],
      "metadata": {
        "id": "EF4K3hZIT5e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export your API Key to environment variable\n",
        "# Upload the .env file to the directory \"/content/\"\n",
        "!pip install python-dotenv\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7-YHyH2kj6i",
        "outputId": "95ffbd7b-509a-460c-8ea6-e62b029aaca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "# Retrieve the API key from environment variable\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Set the API key for OpenAI\n",
        "openai.api_key = openai_api_key"
      ],
      "metadata": {
        "id": "GzCftbdQVFFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the json file containing important information about the Job requirements which was generated in Assignment1, the file containing information about the all resumes along with their summary generated from Assignment3 and the final JSON file containing the score criteria generated in Assignment6"
      ],
      "metadata": {
        "id": "5znW6pSeVuRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the first file\n",
        "print(\"Please upload the first file (all_applications_summary.json):\")\n",
        "uploaded1 = files.upload()\n",
        "\n",
        "# Check to ensure a file was uploaded. If not, prompt again.\n",
        "while len(uploaded1) == 0:\n",
        "    print(\"No file uploaded. Please upload the first file (all_applications_summary.json) again:\")\n",
        "    uploaded1 = files.upload()\n",
        "\n",
        "# Upload the second file\n",
        "print(\"Please upload the second file (requirements_output.json):\")\n",
        "uploaded2 = files.upload()\n",
        "\n",
        "# Check to ensure a file was uploaded. If not, prompt again.\n",
        "while len(uploaded2) == 0:\n",
        "    print(\"No file uploaded. Please upload the second file (requirements_output.json) again:\")\n",
        "    uploaded2 = files.upload()\n",
        "\n",
        "# Upload the third file\n",
        "print(\"Please upload the third file (criterion_and_string_match_output.txt):\")\n",
        "uploaded3 = files.upload()\n",
        "\n",
        "# Check to ensure a file was uploaded. If not, prompt again.\n",
        "while len(uploaded3) == 0:\n",
        "    print(\"No file uploaded. Please upload the third file (criterion_and_string_match_output.txt) again:\")\n",
        "    uploaded3 = files.upload()\n",
        "\n",
        "# Merge the dictionaries to have all uploaded files in one\n",
        "uploaded = {**uploaded1, **uploaded2, **uploaded3}\n",
        "\n",
        "# Print details of uploaded files\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "RxbnzwKlVi_r",
        "outputId": "d8e158d9-a35e-42b7-c4ae-877784941810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload the first file (all_applications_summary.json):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f4547074-970b-4fb0-8933-86e10a63a680\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f4547074-970b-4fb0-8933-86e10a63a680\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving all_applications_summary.json to all_applications_summary.json\n",
            "Please upload the second file (requirements_output.json):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bd589678-d8a8-4139-9b4f-8dcc12fb9f1f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bd589678-d8a8-4139-9b4f-8dcc12fb9f1f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving requirements_output.json to requirements_output.json\n",
            "Please upload the third file (criterion_and_string_match_output.txt):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4b37ef9a-856e-4b39-b1f9-2cbd4850923b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4b37ef9a-856e-4b39-b1f9-2cbd4850923b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving criterion_and_string_match_output (1).txt to criterion_and_string_match_output (1).txt\n",
            "User uploaded file \"all_applications_summary.json\" with length 34793 bytes\n",
            "User uploaded file \"requirements_output.json\" with length 810 bytes\n",
            "User uploaded file \"criterion_and_string_match_output (1).txt\" with length 2531 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now download the `Webinar_resumes.zip` file which contains all the resumes"
      ],
      "metadata": {
        "id": "MIaxUFAkWD99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(file_id, destination):\n",
        "    base_url = \"https://drive.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(base_url, params={'id': file_id}, stream=True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = {'id': file_id, 'confirm': token}\n",
        "        response = session.get(base_url, params=params, stream=True)\n",
        "\n",
        "    save_response_content(response, destination)\n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "# Example Usage\n",
        "file_id = '17V_o0Snt-Lj0FmegENPQ_rXpvWTWlZgQ'\n",
        "destination = 'Webinar_resumes.zip'  # Replace with your desired file name and extension\n",
        "download_file_from_google_drive(file_id, destination)"
      ],
      "metadata": {
        "id": "cdOdlySoWG3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code is designed to process and extract relevant details from different types of document files, specifically resumes, and subsequently summarize them. Key components include:\n",
        "\n",
        "Importing necessary libraries such as OpenAI for interaction with the GPT model, docx for reading Word documents, textract for processing .doc files, fitz for handling PDFs, pandas for working with Excel sheets, and tiktoken for natural language processing.\n",
        "1. `find_json()` searches for a JSON structure within a text.\n",
        "2. `read_requirements()` and `read_json()` load data from a\n",
        "specified JSON file.\n",
        "3. `read_document()` reads content from various file formats including .docx, .doc, .pdf, and .xls/xlsx, converting them to plain text.\n",
        "4. `check_and_trim()` trims the input text to a specified token limit using tokenization from the tiktoken library.\n",
        "5. `contains_zero_or_one()` checks for the presence of either a \"0\" or \"1\" in a string, returning the respective digit if found."
      ],
      "metadata": {
        "id": "Umt3dq2DmQVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import re\n",
        "from docx import Document\n",
        "import textract\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import math\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "def find_json(input_text):\n",
        "    open_braces = 0\n",
        "    start_idx = -1\n",
        "    end_idx = -1\n",
        "\n",
        "    for idx, char in enumerate(input_text):\n",
        "        if char == \"{\":\n",
        "            if start_idx == -1:  # Start of the outermost JSON object\n",
        "                start_idx = idx\n",
        "            open_braces += 1\n",
        "        elif char == \"}\":\n",
        "            open_braces -= 1\n",
        "            if open_braces == 0 and start_idx != -1:  # End of the outermost JSON object\n",
        "                end_idx = idx\n",
        "                break  # Exiting as soon as we find the outermost closing brace\n",
        "\n",
        "    return None if start_idx == -1 or end_idx == -1 else input_text[start_idx:end_idx+1]\n",
        "\n",
        "def read_requirements(file_path):\n",
        "    # Reads the job requirements from a JSON file\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading requirements JSON: {e}\")\n",
        "        return None\n",
        "\n",
        "def read_json(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def read_document(file_path):\n",
        "    file_path = str(file_path)\n",
        "    _, file_extension = os.path.splitext(file_path)\n",
        "    text = \"\"\n",
        "    if file_extension == '.docx':\n",
        "        doc = Document(file_path)\n",
        "        for para in doc.paragraphs:\n",
        "            text = text + para.text + \" \"\n",
        "    elif file_extension == '.doc':\n",
        "        text = textract.process(file_path).decode()\n",
        "    elif file_extension.lower() == '.pdf':\n",
        "        doc = fitz.open(file_path)\n",
        "        for page_number in range(len(doc)):\n",
        "            page = doc[page_number]\n",
        "            text = text + page.get_text() + \" \"\n",
        "    elif file_extension.lower() in ['.xls', '.xlsx']:\n",
        "        data = pd.read_excel(file_path)\n",
        "        text = data.to_string(index=False)\n",
        "\n",
        "    else:\n",
        "        print(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "    return text\n",
        "\n",
        "def check_and_trim(resume_text, max_tokens=1500):\n",
        "    # tokens = nltk.word_tokenize(resume_text)\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tokens = enc.encode(resume_text)\n",
        "    old_len = len(tokens)\n",
        "    if len(tokens) > max_tokens:\n",
        "        tokens = tokens[:max_tokens]\n",
        "        resume_text = enc.decode(tokens)\n",
        "    return resume_text, old_len, len(tokens)\n",
        "\n",
        "def contains_zero_or_one(s):\n",
        "    if re.search(r'0', s):\n",
        "        return 0\n",
        "    elif re.search(r'1', s):\n",
        "        return 1\n",
        "    return None"
      ],
      "metadata": {
        "id": "MpmQT6NWSpv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines functions to evaluate a resume based on specific skills and criteria, leveraging OpenAI's GPT model.\n",
        "1. `score_from_criterion()` takes a prompt, resume text, and generated text, and uses OpenAI's API to evaluate the resume based on the prompt. The working of it has already been discussed in Assignment7. It takes the criteria generated from Assignment6 and calculates scores (between 0 and 5) for each of the must have skills present in the resume.\n",
        "2. `read_from_textfile()` reads from a predefined text file which contains the score criteria generated from Assignment6 and retrieves specific outputs related to criteria and string matching.\n",
        "3. `final_score_calculation()` uses the retrieved criteria and string matches to create a prompt, which is then passed to `score_from_criterion()` to generate a score for the resume based on the required skills.\n",
        "4. `initial_score()` is another function that, given a list of \"must-have\" skills, evaluates the resume to find projects where these skills were applied, summarizing the project and assigning a score `(0 or 1)` based on the presence or absence of the skill in the project. The results from both scoring methods are represented in JSON format. This binary scoring system has already been discussed in Assignment4\n"
      ],
      "metadata": {
        "id": "YwGKvcI5pWZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score_from_criterion(prompt, text, generated_text):\n",
        "    model=\"gpt-3.5-turbo-16k\"\n",
        "    max_tokens=2000\n",
        "    # print(\"prompt\", prompt)\n",
        "    messages = [\n",
        "            {\"role\": \"assistant\", \"content\": f\"{generated_text}\"},\n",
        "            {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{text}\"},\n",
        "        ]\n",
        "    response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-16k\",\n",
        "            messages=messages,\n",
        "            temperature=0,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "    generated_texts = [\n",
        "        choice.message[\"content\"].strip() for choice in response[\"choices\"]\n",
        "    ]\n",
        "    return generated_texts[0]\n",
        "\n",
        "def read_from_textfile(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Separate criterion_gen_output and string_match\n",
        "    criterion_start = content.find(\"----Criterion Generated Output----\") + len(\"----Criterion Generated Output----\")\n",
        "    criterion_end = content.find(\"----String Match----\")\n",
        "\n",
        "    criterion_gen_output = content[criterion_start:criterion_end].strip()\n",
        "    string_match = content[criterion_end + len(\"----String Match----\"):].strip()\n",
        "\n",
        "    return criterion_gen_output, string_match\n",
        "\n",
        "\n",
        "def final_score_calculation(resume_text, must_have_skills):\n",
        "    # Read the criteria and string_match from the text file\n",
        "    criterion_gen_output, string_match = read_from_textfile(\"/content/criterion_and_string_match_output.txt\")\n",
        "    prompt=f'''You are an assistant to a recruiter. \\\n",
        "    Use the criteria given by {criterion_gen_output} to judge the resume given to you. \\\n",
        "    The scores must be given with respect to each of the must have skills present here {must_have_skills}, which \\\n",
        "    can be found inside the resume. \\\n",
        "    Return the output in JSON format.'''\n",
        "\n",
        "    # Assuming the function score_from_criterion exists and takes these parameters\n",
        "    score_output = score_from_criterion(prompt, resume_text, criterion_gen_output)\n",
        "    # print(\"final scoreeee final_score_func\", score_output)\n",
        "    return score_output\n",
        "\n",
        "def initial_score(text, must_have_skills):\n",
        "    model=\"gpt-3.5-turbo-16k\"\n",
        "    max_tokens=2000\n",
        "    first_prompt = f'''You are an assistant to a recruiter. Your job is to evaluate a resume for a particular skill.  The skills for which you need to do\n",
        "    evaluation are {must_have_skills}. You need to find the projects in which a particular skill from this list has been applied. For each skill, \\\n",
        "    use the technical skill as the key and this key will further have 2 more keys \"summary\" and \"score\". \"score\" is 0 if there is no project using this \\\n",
        "    particular skill and \"summary\" is empty and \"score\" is 1 if you find a project with the particular technical skill, then use \"summary\" to \\\n",
        "    explain the project. Now do this for all the must have skills. \\\n",
        "    Return your response as a JSON'''\n",
        "    messages = [\n",
        "            {\"role\": \"system\", \"content\": f\"{first_prompt}\"},\n",
        "            {\"role\": \"user\", \"content\": text },\n",
        "        ]\n",
        "    response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-16k\",\n",
        "            messages=messages,\n",
        "            temperature=1,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "    generated_texts = [\n",
        "        choice.message[\"content\"].strip() for choice in response[\"choices\"]\n",
        "    ]\n",
        "    # print(\"initial score calculation\", generated_texts)\n",
        "    return generated_texts[0]"
      ],
      "metadata": {
        "id": "BDYVgIWPT70F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `calculate_score` assesses a candidate's resume against a set of \"must-have\" skills by leveraging two different evaluation functions (`initial_score` and `final_score_calculation`). The results of both scoring methods are then combined, producing a cumulative score for each skill, which is determined by multiplying the scores from both evaluations. It also accumulates justifications and summaries. The function then computes an overall score by considering the number of skills with non-zero scores relative to the total number of skills evaluated, providing a normalized representation of a candidate's qualifications. The final output includes the overall score, count of evaluated skills, individual skill scores, and the combined scores with associated justifications and summaries."
      ],
      "metadata": {
        "id": "np_hvnCky-aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def calculate_score(technical_skills, text_sum, mskill, max_retries=1):\n",
        "    def process_request(text_sum, mskill, match_func, max_retries):\n",
        "        retries = 0\n",
        "        req_dict = {}\n",
        "\n",
        "        while retries < max_retries:\n",
        "            req_json = match_func(text_sum, mskill)\n",
        "            # print(\"ssssssssss\", req_json, match_func)\n",
        "            req_json = find_json(req_json)\n",
        "            # print(\"s11111\", req_json)\n",
        "\n",
        "            if isinstance(req_json, dict):\n",
        "                req_dict = req_json\n",
        "                if req_dict:  # If the dictionary is not empty, return it\n",
        "                    return req_dict\n",
        "\n",
        "            elif isinstance(req_json, str):\n",
        "                try:\n",
        "                    req_dict = json.loads(req_json)\n",
        "                    if req_dict:  # If the dictionary is not empty, return it\n",
        "                        return req_dict\n",
        "                except json.JSONDecodeError:\n",
        "                    try:\n",
        "                        dict_from_str_repr = eval(req_json)\n",
        "                        if isinstance(dict_from_str_repr, dict):\n",
        "                            req_dict = dict_from_str_repr\n",
        "                            if req_dict:  # If the dictionary is not empty, return it\n",
        "                                return req_dict\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "            retries += 1\n",
        "        return req_dict  # Returning the processed req_dict if not returned earlier\n",
        "\n",
        "    req_dict_v1 = process_request(text_sum, mskill, initial_score, max_retries)\n",
        "    req_dict_v2 = process_request(text_sum, mskill, final_score_calculation, max_retries)\n",
        "\n",
        "    # Combine scores from both dictionaries and their justifications/summaries\n",
        "    combined_scores = {}\n",
        "    for skill in set(list(req_dict_v1.keys()) + list(req_dict_v2.keys())):  # ensuring we get all skills from both dicts\n",
        "        score_v1 = req_dict_v1.get(skill, {}).get('score', 0)\n",
        "        score_v2 = req_dict_v2.get(skill, {}).get('score', 0)\n",
        "        summary_v1 = req_dict_v1.get(skill, {}).get('summary', '')\n",
        "        justification_v2 = req_dict_v2.get(skill, {}).get('justification', '')\n",
        "        final_score = score_v1 * score_v2\n",
        "        combined_scores[skill] = {\n",
        "            'combined_score': final_score,\n",
        "            'summary': summary_v1,\n",
        "            'justification': justification_v2\n",
        "        }\n",
        "\n",
        "    non_zero_skills = 0  # Count of skills with non-zero combined scores\n",
        "    total_score = 0\n",
        "\n",
        "    store_js = {}\n",
        "    for skill, data in combined_scores.items():\n",
        "        score = data.get('combined_score', 0)\n",
        "        store_js[skill] = score\n",
        "        if score > 0:\n",
        "            non_zero_skills += 1\n",
        "        total_score += score\n",
        "\n",
        "    count = len(combined_scores)\n",
        "\n",
        "    factor = 1 + non_zero_skills\n",
        "    if count == 0:\n",
        "        overall_score = 0\n",
        "    else:\n",
        "        overall_score = (factor / (count + 1)) * (total_score / count)\n",
        "\n",
        "    return overall_score, count + 1, store_js, combined_scores\n"
      ],
      "metadata": {
        "id": "PL68Ssa-RTrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code includes several utility functions to aid in evaluating and ranking job applications based on the experience of candidates:\n",
        "\n",
        "**get_numerical_val(s):**\n",
        "This function takes a string s as input and returns the first numerical value found in it. If the string represents an integer, it's returned as an int; if it's a floating point, it's returned as a float. If no numerical value is found, it returns None.\n",
        "\n",
        "**score_experience(candidate_experience, job_experience_range):**\n",
        "Accepting a candidate's experience (candidate_experience) and a desired job experience range (job_experience_range), it computes a score for the candidate based on how closely their experience matches the desired range. If the candidate's experience exceeds the maximum expected experience, a decay factor is applied to lower their score.\n",
        "\n",
        "**rank_applications(applications):**\n",
        "This function sorts the list of applications based on a mean score, which is the average of the general score (score) and the skills score (skill_score) of each application, normalized by the number of must-have skills. Applications are ranked in descending order of this mean score, and a rank is assigned to each application.\n",
        "\n",
        "**extract_years(text):**\n",
        "Given a text string, this function aims to extract year values. If it finds only one year value, it returns that year and a subsequent year (the given year + 5). If two year values are detected, it returns them both as a range. If no valid year range can be extracted, it returns None.\n",
        "\n",
        "**years_of_exp(application, range_list, skill_score):**\n",
        "This function updates the provided application dictionary with scores calculated based on the years of experience. Using the get_numerical_val function, it retrieves the years of experience from the application and calculates a score using the score_experience function. It updates the score and skill_score of the application based on this.\n",
        "\n",
        "These utilities collectively aim to rank candidates based on the importance of their skills and the match between their years of experience and the job's requirements."
      ],
      "metadata": {
        "id": "Cp-DnKfJ56b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import shutil\n",
        "def get_numerical_val(s):\n",
        "    # number = re.findall(r'[+-]?([0-9]*[.])?[0-9]+', s)\n",
        "    s = str(s)\n",
        "    number = re.findall(r'([0-9]*[.])?[0-9]+', s)\n",
        "    if s.isdigit():\n",
        "        return int(s)\n",
        "    if number:\n",
        "        # If there is a decimal point in the number,\n",
        "        # return it as a float; otherwise, return as int.\n",
        "        if '.' in number[0]:\n",
        "            return float(number[0])\n",
        "        else:\n",
        "            number = re.findall(r'[0-9]+', s)\n",
        "            return int(number[0])\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def score_experience(candidate_experience, job_experience_range):\n",
        "    min_exp, max_exp = job_experience_range\n",
        "    alpha = 1 / (max_exp - min_exp)**2\n",
        "    decay_multiplier = 0.5\n",
        "\n",
        "    if candidate_experience > max_exp:\n",
        "        alpha *= decay_multiplier\n",
        "    score = math.exp(-alpha * (candidate_experience - max_exp)**2)\n",
        "\n",
        "    return score\n",
        "def rank_applications(applications):\n",
        "    # Calculate the mean for each application\n",
        "    for application in applications:\n",
        "        if application[\"score\"] is None or application[\"skill_score\"] is None:\n",
        "            application[\"mean\"] = -1\n",
        "        else:\n",
        "            application[\"mean\"] = (float(application[\"score\"]) + float(application[\"skill_score\"])) / application[\"len_must_have\"]\n",
        "\n",
        "    # Sort applications based on the mean\n",
        "    applications.sort(key=lambda x: x[\"mean\"], reverse=True)\n",
        "\n",
        "    # Rank the applications\n",
        "    for rank, app in enumerate(applications):\n",
        "        app[\"rank\"] = rank + 1  # Update rank\n",
        "\n",
        "    return applications\n",
        "\n",
        "def extract_years(text):\n",
        "    numbers = re.findall(r'(\\d+)', text)\n",
        "    if len(numbers) == 1:\n",
        "        return [int(numbers[0]), int(numbers[0])+5]\n",
        "    elif len(numbers) >= 2:\n",
        "        return [int(numbers[0]), int(numbers[1])]\n",
        "    return None\n",
        "\n",
        "def years_of_exp(application, range_list, skill_score):\n",
        "    output = 0\n",
        "    print(get_numerical_val(application[\"years_of_experience\"]), \"ssssss\")\n",
        "    yoe = int(get_numerical_val(application[\"years_of_experience\"]))\n",
        "    if len(range_list) == 1:\n",
        "        val = range_list.pop()\n",
        "\n",
        "        range_list.append(int(val)-1)\n",
        "        range_list.append(int(val))\n",
        "    output = score_experience(yoe, range_list)\n",
        "    application[\"score\"] = output\n",
        "    application[\"skill_score\"] = skill_score\n",
        "    return application"
      ],
      "metadata": {
        "id": "2vYiRDWG02Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code provided aims to rank job applications based on candidates' experience and technical skills, in accordance with a predefined set of job requirements. Here's a step-by-step breakdown:\n",
        "\n",
        "**Loading Job Requirements:**\n",
        "The code starts by reading the job requirements from JSON file generated in Assignment1. The job requirements include a list of essential skills (must_have_skills) and the desired years of experience (yoe).\n",
        "\n",
        "**Loading Candidate Data: **\n",
        "The code then loads the data of filtered applications from JSON file generated in Assignment3\n",
        "\n",
        "**Extract and Rename Resumes: **\n",
        "Using the extract_and_rename function, the resumes of applicants (zipped in Webinar_resumes.zip) are extracted to a directory. This function also renames directories that have spaces in their names, replacing spaces with underscores.\n",
        "\n",
        "**Processing Applications:**\n",
        "In the **process_applications** function, each candidate's application is processed:\n",
        "\n",
        "1. For each application, it reads the resume text and trims it if needed.\n",
        "2. It then extracts the candidate's name and years of experience from the summarized resume data.\n",
        "3. The candidate's technical skills are evaluated against the must_have_skills using the calculate_score function (not provided in the shared code). This function apparently returns a score based on the match between the candidate's skills and the job requirements.\n",
        "4. Additionally, the desired years of experience is processed, and a score is assigned to the candidate based on their years of experience.\n",
        "5. Finally, relevant data points such as the name, score, and skills of the candidate are saved to the application.\n",
        "6. Ranking the Applications: The rank_applications function (explained in a previous breakdown) is then called to rank the applications based on their mean score, which is the average of their general score and skills score normalized by the number of must-have skills.\n",
        "\n",
        "**Displaying the Rankings:**\n",
        "After ranking, the code displays the ranking details for each candidate, showing:\n",
        "\n",
        "1. The candidate's name\n",
        "2. Years of experience\n",
        "3. Skill score\n",
        "4. List of skills\n",
        "5. The final score (mean score)\n",
        "\n",
        "In essence, this code provides an automated system to score and rank job applications based on how well the candidate's skills and experience align with the job's requirements."
      ],
      "metadata": {
        "id": "qOtjB4_e6eRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job_requirements = read_requirements('/content/requirements_output.json')\n",
        "must_have_skills = job_requirements[\"must_have_skills\"]\n",
        "json_data = read_json('/content/all_applications_summary.json')\n",
        "yoe = job_requirements[\"year_of_experience\"]\n",
        "\n",
        "# Process and score each application\n",
        "zip_file_path = \"/content/Webinar_resumes.zip\" # For example give the path to resume_data.zip\n",
        "\n",
        "def extract_and_rename(zip_file_path, extract_path=\"extracted_files\"):\n",
        "    \"\"\"\n",
        "    Extract files from a zip archive to a specified directory.\n",
        "    Rename directories containing spaces to use underscores instead.\n",
        "\n",
        "    Args:\n",
        "    - zip_file_path (str): The path to the zip file to be extracted.\n",
        "    - extract_path (str, optional): The path where the zip file content should be extracted to.\n",
        "                                    Defaults to \"extracted_files\".\n",
        "\n",
        "    Returns:\n",
        "    - str: Path to the resume or directory.\n",
        "    \"\"\"\n",
        "    # Check if extract_path exists, if not, create it\n",
        "    if not os.path.exists(extract_path):\n",
        "        os.makedirs(extract_path)\n",
        "\n",
        "    # If extract_path is not empty, skip extraction\n",
        "    if not os.listdir(extract_path):\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "\n",
        "    resume_path = extract_path\n",
        "    for item in os.listdir(extract_path):\n",
        "        item_path = os.path.join(extract_path, item)\n",
        "\n",
        "        # Check if the current item is a directory and if it has spaces in its name\n",
        "        if os.path.isdir(item_path) and ' ' in item:\n",
        "            new_name = item.replace(' ', '_')\n",
        "            new_path = os.path.join(extract_path, new_name)\n",
        "\n",
        "            # If the new directory name doesn't already exist, create it\n",
        "            if not os.path.exists(new_path):\n",
        "                os.makedirs(new_path)\n",
        "\n",
        "            # Copying contents from the old directory to the new one\n",
        "            for sub_item in os.listdir(item_path):\n",
        "                shutil.copy2(os.path.join(item_path, sub_item), new_path)\n",
        "\n",
        "            # Removing the old directory\n",
        "            shutil.rmtree(item_path)\n",
        "            resume_path = new_path\n",
        "        else:\n",
        "            resume_path = item_path\n",
        "\n",
        "    return resume_path\n",
        "resume_path = extract_and_rename(zip_file_path)\n",
        "\n",
        "\n",
        "def process_applications(json_data, must_have_skills, yoe):\n",
        "\n",
        "    for application in json_data:\n",
        "        # print(application, \"application\")\n",
        "        app_data = {}\n",
        "        if 'resume_path' in application and 'email_id' in application:\n",
        "            resume_text = read_document(os.path.join(resume_path, application['resume_path']))\n",
        "            resume_text, _, _ = check_and_trim(resume_text)\n",
        "            resume_summary = application['resume_summary']\n",
        "            application[\"name_of_candidate\"] = resume_summary[\"name_of_candidate\"]\n",
        "            application[\"years_of_experience\"] = resume_summary[\"years_of_experience\"]\n",
        "            skill_score, len_must_have, store_js, skill_data = calculate_score(resume_summary[\"technical_skills\"], resume_text, must_have_skills)\n",
        "            application[\"len_must_have\"] = len_must_have\n",
        "            application[\"skills\"] = store_js\n",
        "            yoe = extract_years(str(yoe))  # Define appropriately\n",
        "            print(application, \"resume_summary[name_of_candidate]\")\n",
        "            application = years_of_exp(application, yoe, skill_score)\n",
        "            name = application[\"name_of_candidate\"]\n",
        "            score = application[\"score\"]\n",
        "\n",
        "\n",
        "            # print(\"resume summary\", resume_summary)\n",
        "\n",
        "    return json_data\n",
        "\n",
        "\n",
        "applications = process_applications(json_data, must_have_skills, yoe)\n",
        "application = rank_applications(applications)\n",
        "for app in application:\n",
        "    name = app[\"name_of_candidate\"]\n",
        "    yoe = app[\"years_of_experience\"]\n",
        "    skill_score = app[\"skill_score\"]\n",
        "    store_js = app[\"skills\"]\n",
        "    final_score = app[\"mean\"]  # Assuming mean is the final score\n",
        "\n",
        "    print(f\"[Final Ranking] Candidate Name: {name}\")\n",
        "    print(f\"[Final Ranking] Years of Experience: {yoe}\")\n",
        "    print(f\"[Final Ranking] Skill Score: {skill_score}\")\n",
        "    print(f\"[Final Ranking] Skills: {store_js}\")\n",
        "    print(f\"[Final Ranking] Final Score: {final_score}\")\n",
        "    print(\"-\" * 40)  # Separator"
      ],
      "metadata": {
        "id": "7CE8dPMlRmWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42de0706-8399-4f12-8767-e59b91ddec2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'current_ctc': 125503.5, 'expected_ctc': 138444.2, 'willing_to_relocate': 'yes', 'current_location': 'San Francisco', 'notice_period': '34 days', 'email_id': 'Julian-Barcenas-Fake-Resume-64fbe45abfd46@example.com', 'resume_path': 'Julian-Barcenas-Fake-Resume-64fbe45abfd46.pdf', 'resume_summary': {'name_of_candidate': 'Julian Barcenas', 'mobile_number': '(604) 123-4567', 'email_id': 'julian.barcenas@email.com', 'years_of_experience': 5, 'education': 'Master’s in Computer Science (Specialization in Machine Learning)', 'university': 'University of British Columbia', 'linkedin_profile': 'linkedin.com/in/julianbarcenas', 'technical_skills': ['Python', 'R', 'Machine Learning Algorithms', 'TensorFlow', 'PyTorch', 'scikit-learn', 'Data Preprocessing Tools', 'Matplotlib', 'Seaborn', 'AWS'], 'years_of_jobs': ['2018-2023'], 'year_in_current_position': 2, 'Present_Organization': 'TechFlow Corp', 'summary': \"Julian Barcenas is an AI/ML Specialist with 5 years of experience in the development and deployment of machine learning models. With a Master's degree in Computer Science, specializing in Machine Learning, from the University of British Columbia, Julian has a strong proficiency in Python and R programming. He has extensive experience with machine learning algorithms, including deep learning techniques, and has worked with libraries such as TensorFlow, PyTorch, and scikit-learn. Julian is also skilled in data preprocessing, data visualization using Matplotlib and Seaborn, and deploying models on cloud platforms like AWS. He is a detail-oriented problem solver and an exceptional team player with strong communication skills.\"}, 'name_of_candidate': 'Julian Barcenas', 'years_of_experience': 5, 'len_must_have': 5, 'skills': {'PyTorch': 0, 'Computer Vision': 0, 'Keras': 0, 'TensorFlow': 2}} resume_summary[name_of_candidate]\n",
            "5 ssssss\n",
            "{'current_ctc': 124107.2, 'expected_ctc': 143100.4, 'willing_to_relocate': 'yes', 'current_location': 'Sacramento', 'notice_period': '11 days', 'email_id': 'Pankaj_kumar_Goyal-4-1-64f824abdacdc@example.com', 'resume_path': 'Pankaj_kumar_Goyal-4-1-64f824abdacdc.pdf', 'resume_summary': {'name_of_candidate': 'PANKAJ KUMAR GOYAL', 'mobile_number': 'pankaj10032', 'email_id': 'pankajgoyal02003@gmail.com', 'education': 'B.Tech in Electronics and Communication Engineering', 'university': 'Indian Institute of Information Technology, Allahabad', 'linkedin_profile': 'pankaj_Goyal', 'technical_skills': ['Python', 'Machine learning', 'Computer Vision', 'Deep learning', 'Data Cleaning', 'Feature engineering', 'Data Analysis', 'Data Science', 'Natural Language Processing', 'Large language models (BERT, Roberta, XLM-R, T5, Distil-BERT)', 'Prompt Engineering', 'Generative AI', 'LangChain', 'pinecone (vector databases)', 'chatbot development', 'Numpy', 'Pandas', 'Scikit Learn', 'TensorFlow', 'Keras', 'Seaborn', 'Matplotlib', 'SQL', 'MYSQL', 'PosgreySQL', 'AWS', 'Vertex AI (AUTOML, CustomML)', 'MLOPS (MLflow)', 'PowerBI'], 'years_of_jobs': ['Dec 2021-Present', 'April 2019-March 2020', 'April 2017-March 2018'], 'year_in_current_position': 2, 'Present_Organization': 'Indian Institute of Information Technology, Allahabad', 'years_of_experience': 3, 'summary': 'PANKAJ KUMAR GOYAL is a Data Science and Machine learning professional with hands-on experience in Natural Language Processing, prompt engineering, deep learning, and computer vision. He has expertise in Python, Machine Learning, Computer Vision, Data Cleaning, Feature Engineering, and Data Analysis. He has also worked with large language models like BERT, Roberta, XLM-R, T5, and Distil-BERT. PANKAJ has a strong academic background with a B.Tech degree in Electronics and Communication Engineering from the Indian Institute of Information Technology, Allahabad. He is skilled in SQL, MYSQL, PosgreySQL, AWS, Vertex AI, MLOPS, and PowerBI. PANKAJ has 3 years of experience, including 2 years in his current position at the Indian Institute of Information Technology, Allahabad.'}, 'name_of_candidate': 'PANKAJ KUMAR GOYAL', 'years_of_experience': 3, 'len_must_have': 5, 'skills': {'PyTorch': 0, 'Computer Vision': 5, 'Keras': 2, 'TensorFlow': 2}} resume_summary[name_of_candidate]\n",
            "3 ssssss\n",
            "{'current_ctc': 114614.2, 'expected_ctc': 129948.5, 'willing_to_relocate': 'yes', 'current_location': 'San Diego', 'notice_period': '12 days', 'email_id': 'YogesshJBhatiDATAS-64f88c328fa04@example.com', 'resume_path': 'YogesshJBhatiDATAS-64f88c328fa04.pdf', 'resume_summary': {'name_of_candidate': 'Yogessh J Bhati', 'mobile_number': '+91 7048538187', 'email_id': 'yogeshbhati157@gmail.com', 'years_of_experience': 5, 'education': 'B.Tech in Computer Science', 'university': 'Pandit Deendayal Petroleum University', 'linkedin_profile': 'linkedin.com/in/yogesshjbhati', 'technical_skills': ['Hadoop', 'Sqoop', 'Hive', 'Apache Spark', 'AWS', 'Azure Databricks', 'SQL Server', 'MySQL', 'Python', 'Scala', 'SQL', 'UNIX Shell Script', 'C', 'Git', 'Linux', 'Windows', 'Mac', 'Pandas', 'NumPy', 'Matplotlib', 'Seaborn', 'Plotly', 'Scikit-Learn', 'TensorFlow', 'Keras', 'PyTorch', 'Flask', 'FastAPI'], 'years_of_jobs': ['March 2022-Present', 'August 2021-March 2022', 'September 2020-July 2021', 'March 2019-September 2020'], 'year_in_current_position': 1, 'Present_Organization': 'Networth Data Products Pvt. Ltd.', 'summary': 'Experienced data scientist with 5 years of experience in designing, developing, and maintaining large business applications. Skilled in machine learning algorithms, Hadoop/Big Data technologies, time series analysis, Flask API development, and AWS/Azure components. Proficient in data analysis and visualization using Python libraries. Strong problem-solving and analytical skills, with a passion for innovation. Experienced in Agile methodology and collaborating with cross-functional teams. B.Tech in Computer Science from Pandit Deendayal Petroleum University. LinkedIn: linkedin.com/in/yogesshjbhati'}, 'name_of_candidate': 'Yogessh J Bhati', 'years_of_experience': 5, 'len_must_have': 5, 'skills': {'PyTorch': 5, 'Computer Vision': 5, 'Keras': 2, 'TensorFlow': 2}} resume_summary[name_of_candidate]\n",
            "5 ssssss\n",
            "{'current_ctc': 138189.0, 'expected_ctc': 141001.2, 'willing_to_relocate': 'yes', 'current_location': 'San Diego', 'notice_period': '12 days', 'email_id': 'Aysin_Sanci-64f855fbdd892@example.com', 'resume_path': 'Aysin_Sanci-64f855fbdd892.pdf', 'resume_summary': {'name_of_candidate': 'Ayşin Sancı', 'mobile_number': '0090 (507) 120-7666', 'email_id': 'aysin.sanci@gmail.com', 'linkedin_profile': 'linkedin.com/in/ayşin-sancı', 'years_of_experience': 14, 'education': 'Bogazici University – Business Information Systems, M.A.', 'university': 'Bogazici University', 'technical_skills': ['C#', 'Python', 'Java', 'C++', 'Pytorch', 'ASP.NET', 'MVC', 'Objective-C', 'MySQL', 'Sharepoint', 'CoolGen', 'PLSQL', 'MS SQL', 'JQuery', 'MongoDB', 'PostgreSQL', 'Redis', 'Docker', 'Git', 'Linux'], 'years_of_jobs': ['2021-2022', '2019-2021', '2018-2019', '2013-2017', '2012-2013', '2008-2012'], 'year_in_current_position': 1, 'Present_Organization': 'Revolvind', 'summary': 'Ayşin Sancı is a highly experienced software engineer with 14 years of experience in various roles. She has expertise in programming languages such as C#, Python, Java, and C++. She has worked on diverse projects including IOT solutions, computer vision applications, AI, and machine learning projects. Ayşin has a strong educational background with an M.A. in Business Information Systems from Bogazici University. She is skilled in using various tools and technologies such as Pytorch, ASP.NET, Docker, and Git. Ayşin is currently working as a Software Team Lead at Revolvind, where she is leading a team in developing innovative software solutions.'}, 'name_of_candidate': 'Ayşin Sancı', 'years_of_experience': 14, 'len_must_have': 5, 'skills': {'PyTorch': 5, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 0}} resume_summary[name_of_candidate]\n",
            "14 ssssss\n",
            "{'current_ctc': 134090.3, 'expected_ctc': 149321.0, 'willing_to_relocate': 'yes', 'current_location': 'San Diego', 'notice_period': '13 days', 'email_id': 'Resume-2023-6500e7a88155a@example.com', 'resume_path': 'Resume-2023-6500e7a88155a.pdf', 'resume_summary': {'name_of_candidate': 'Tahsin Samia', 'mobile_number': '+61435034240', 'email_id': 'tahsinsamia28@gmail.com', 'years_of_experience': 4, 'education': 'Bachelor of Advanced Computing (Honours)', 'university': 'University of Sydney', 'linkedin_profile': 'https://www.linkedin.com/in/tsamia/', 'technical_skills': ['Alteryx', 'SQL', 'SSMS', 'Python', 'R', 'Java', 'C', 'C++', 'HTML', 'CSS', 'JavaScript', 'Excel', 'Tableau', 'PowerBI', 'Qlik Sense', 'RStudio'], 'years_of_jobs': ['2022-Present', '2020-2021', '2020-2021', '2019-2019'], 'year_in_current_position': 2, 'Present_Organization': 'PriceWaterhouseCoopers', 'summary': 'Experienced and highly skilled data consultant and academic tutor with a strong background in ETL, data analysis, visualization, and database administration across multiple industry sectors, with a passion for leveraging data-driven insights to optimize business outcomes and drive success.'}, 'name_of_candidate': 'Tahsin Samia', 'years_of_experience': 4, 'len_must_have': 5, 'skills': {'PyTorch': 0, 'Computer Vision': 0, 'Keras': 0, 'TensorFlow': 0}} resume_summary[name_of_candidate]\n",
            "4 ssssss\n",
            "{'current_ctc': 108554.3, 'expected_ctc': 111333.4, 'willing_to_relocate': 'yes', 'current_location': 'Oakland', 'notice_period': '13 days', 'email_id': 'CV_Paula_Ramos_2023-64fa0a12b5483@example.com', 'resume_path': 'CV_Paula_Ramos_2023-64fa0a12b5483.pdf', 'resume_summary': {'name_of_candidate': 'Paula Ramos', 'mobile_number': '919-786-3615', 'email_id': 'pjramg@gmail.com', 'years_of_experience': 9, 'education': 'Ph.D. in Engineering (Computer Science & Image Processing)', 'university': 'Universidad Nacional de Colombia', 'linkedin_profile': 'linkedin.com/in/paula-ramos-41097319', 'technical_skills': ['Computer Vision', 'Machine Learning', 'Image Processing', 'Signal Processing', 'Control and Automation', 'Robotics', 'Embedded Systems', 'Mobile Devices'], 'years_of_jobs': ['Nov 2021 - present', 'Jul 2020 - Oct 2022', 'Jun 2019 - Jun 2020', 'Feb 2019 - Jun 2019', 'Feb 2010 - Dec 2018', 'Sept 2014 - Marzo 2015', 'June 2004 - Jan 2010', 'June 2006 - Dec 2011'], 'year_in_current_position': 1, 'Present_Organization': 'Intel Corporation', 'summary': \"Paula Ramos is a Computer Vision and Machine Learning expert with a Ph.D. in Engineering. With 9 years of experience, she has a strong background in image and signal processing, control and automation, robotics, and embedded systems. She has worked on various research projects related to agricultural systems, deep learning, and computer vision. Paula has developed new technologies and obtained patents for coffee growers in Colombia. She has a proven track record of building and fostering developer communities. Paula's technical skills include computer vision, machine learning, and image processing. She is currently working as an AI Software Development Engineer at Intel Corporation.\"}, 'name_of_candidate': 'Paula Ramos', 'years_of_experience': 9, 'len_must_have': 5, 'skills': {'PyTorch': 0, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 0}} resume_summary[name_of_candidate]\n",
            "9 ssssss\n",
            "{'current_ctc': 149818.4, 'expected_ctc': 149895.0, 'willing_to_relocate': 'yes', 'current_location': 'Sacramento', 'notice_period': '14 days', 'email_id': 'Resume-Naren-Sadhwani-64f87b18d511f@example.com', 'resume_path': 'Resume-Naren-Sadhwani-64f87b18d511f.pdf', 'resume_summary': {'name_of_candidate': 'Naren Sadhwani', 'mobile_number': '+4917661507970', 'email_id': 'sadhwaninaren@gmail.com', 'years_of_experience': 9, 'education': 'M.Sc. Robotics Systems Engineering', 'university': 'RWTH Aachen', 'linkedin_profile': 'LinkedIn', 'technical_skills': ['MATLAB', 'Python', 'Machine Learning', 'Data Structures and Algorithms', 'Tensorflow', 'Docker', 'C++', 'Pytorch', 'Git', 'ROS2', 'SQL', 'HTML & CSS'], 'years_of_jobs': ['Apr 2022 - Oct 2022', 'Jul 2016 - Jul 2019', 'Jul 2013 - Apr 2016'], 'year_in_current_position': 1, 'Present_Organization': 'Newwork Softwares GmbH', 'summary': \"Innovative Robotics Engineer with experience in machine learning, design, prototyping, and testing. Skilled in MATLAB, Python, and various deep learning architectures. Strong knowledge of data structures and algorithms. Proficient in technologies like Tensorflow, Docker, C++, Pytorch, Git, ROS2, SQL, and HTML & CSS. Holds a master's degree in Robotics Systems Engineering from RWTH Aachen. Has worked at Newwork Softwares GmbH as a Machine Learning Intern and founded IIR Technologies. Previous experience includes roles at Rolls Royce (RR) Plc. as an Advanced Thermofluids Technologist. A collaborative team member committed to building optimal engineering solutions.\", 'certifications': ['Project Management and System Design'], 'languages': ['English', 'German', 'Hindi']}, 'name_of_candidate': 'Naren Sadhwani', 'years_of_experience': 9, 'len_must_have': 5, 'skills': {'PyTorch': 5, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 0}} resume_summary[name_of_candidate]\n",
            "9 ssssss\n",
            "{'current_ctc': 141876.2, 'expected_ctc': 142072.6, 'willing_to_relocate': 'yes', 'current_location': 'San Diego', 'notice_period': '14 days', 'email_id': 'JWD-Res-txt-6500b15d09846@example.com', 'resume_path': 'JWD-Res-txt-6500b15d09846.pdf', 'resume_summary': {'name_of_candidate': 'Derrick I.C. VAN FRAUSUM', 'mobile_number': '+32 471 55 10 95', 'email_id': 'derrick.vanfrausum@gmail.com', 'years_of_experience': 9, 'education': 'Master of Science in Business Engineering', 'university': 'UCLouvain LSM, Louvain-la-Neuve', 'linkedin_profile': 'github.com/DerrickDDInAI', 'technical_skills': ['Python', 'OpenAI', 'HuggingFace', 'OpenCV', 'MediaPipe', 'Tensorflow', 'PyTorch', 'Lightkurve', 'PySpark', 'Scikit-learn', 'SQL', 'Adobe Creative Suite', 'Photo & Video', 'Blender'], 'years_of_jobs': ['2015-2019', '2021-2022'], 'year_in_current_position': 1, 'Present_Organization': 'VRT, Brussels', 'summary': \"Experienced Data Scientist with a proven track record in data analytics and AI. Holds a Master's degree in Business Engineering and has 9 years of experience in the field. Skilled in Python, OpenAI, HuggingFace, OpenCV, and more. Strong background in computer vision, natural language processing, and customer clustering. Proficient in SQL and experienced in big data optimization and analysis. Currently working as a Data Scientist at VRT, focused on developing innovative solutions for sustainable development and biodiversity conservation. Passionate about using AI for the betterment of society and general well-being.\"}, 'name_of_candidate': 'Derrick I.C. VAN FRAUSUM', 'years_of_experience': 9, 'len_must_have': 5, 'skills': {'PyTorch': 0, 'Computer Vision': 0, 'Keras': 0, 'TensorFlow': 0}} resume_summary[name_of_candidate]\n",
            "9 ssssss\n",
            "{'current_ctc': 142547.1, 'expected_ctc': 143077.6, 'willing_to_relocate': 'yes', 'current_location': 'San Diego', 'notice_period': '12 days', 'email_id': 'Adeola_Joseph_cv_updated-64f88c610a4f4@example.com', 'resume_path': 'Adeola_Joseph_cv_updated-64f88c610a4f4.pdf', 'resume_summary': {'name_of_candidate': 'Joseph Adeola', 'mobile_number': '+34 610-423-930', 'email_id': 'adeola.jo@outlook.com', 'years_of_experience': 4, 'education': 'Erasmus Mundus Masters in Intelligent Field Robotic Systems', 'university': 'Universitat De Girona, Spain', 'linkedin_profile': 'linkedin.com/in/adeola-joseph', 'technical_skills': ['Python', 'Pandas', 'Numpy', 'Matplotlib', 'PyTorch', 'TensorFlow', 'Keras', 'Scikit-Learn', 'Kivy', 'Matlab', 'C++', 'R', 'C', 'OMPL', 'PDDL', 'Robot Operating System (ROS)', 'Git', 'OpenCV', 'Linux OS', 'MS Office Suites'], 'years_of_jobs': ['2023-Present', '2022-2022', '2021-2021', '2022-2022'], 'year_in_current_position': 1, 'Present_Organization': 'Computer Vision and Robotics Research Institute, Universitat De Girona', 'summary': \"Joseph Adeola is a highly skilled graduate student specializing in Intelligent Robotics with 4 years of experience in software development. He has a strong academic background with a Masters in Intelligent Field Robotic Systems and a Bachelor of Science in Mathematics. Joseph is proficient in Python, Matlab, C++, R, and has expertise in robotics, machine learning, computer vision, and control. He has a proven track record of successfully collaborating with multidisciplinary teams to achieve project objectives. Joseph is currently seeking a Master's thesis research position in Multi-robot Systems to contribute innovative research and create a significant impact.\"}, 'name_of_candidate': 'Joseph Adeola', 'years_of_experience': 4, 'len_must_have': 5, 'skills': {'PyTorch': 0, 'Computer Vision': 5, 'Keras': 2, 'TensorFlow': 2}} resume_summary[name_of_candidate]\n",
            "4 ssssss\n",
            "{'current_ctc': 142210.2, 'expected_ctc': 144425.6, 'willing_to_relocate': 'yes', 'current_location': 'San Francisco', 'notice_period': '13 days', 'email_id': 'Abhilash_CV-64f821119bbfe@example.com', 'resume_path': 'Abhilash_CV-64f821119bbfe.pdf', 'resume_summary': {'name_of_candidate': 'Abhilash Babu', 'mobile_number': '+49 17647165848', 'email_id': 'abhilashbabuj@gmail.com', 'summary': \"Senior Machine Learning Engineer with 18 years of experience in delivering projects in Computer Vision and Image processing. Proficient in classical computer vision techniques and deep learning frameworks like TensorFlow and PyTorch. Skilled in developing machine learning applications for object detection, image classification, and image segmentation. Experienced in mentoring and collaborating with junior colleagues. Holds a Master's degree in Communication Engineering and is certified as a Scrum Product Owner and Software Architect.\", 'technical_skills': ['C++', 'C', 'C#', 'Python', 'OpenCV', 'Halcon Machine vision library', 'Tensorflow', 'PyTorch', 'PyTorch-Lightning', 'Scikit-Learn', 'Pandas', 'Keras', 'ONNX', 'ApacheTVM', 'MLFlow', 'Optuna', 'MySQL', 'SQLite', 'Boost', 'ZeroMQ', 'Protocol Buffer', 'gRPC', 'MQTT', 'RabbitMQ', 'Qt', 'WPF', 'DearImGUI', 'pytest', 'GoogleTest', 'Catch2', 'Docker', 'Jenkins', 'Bamboo', 'Jupyter Notebooks'], 'years_of_jobs': ['2022-present', '2020-2022', '2016-2019', '2013-2016', '2011-2011', '2008-2010', '2005-2008'], 'year_in_current_position': 1, 'Present_Organization': 'IDnow GmbH, München', 'years_of_experience': 18, 'education': 'MS in Communication Engineering, Technische Universität, München, Germany', 'university': 'Technische Universität, München, Germany', 'linkedin_profile': '', 'summary_word_count': 99}, 'name_of_candidate': 'Abhilash Babu', 'years_of_experience': 18, 'len_must_have': 5, 'skills': {'PyTorch': 5, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 2}} resume_summary[name_of_candidate]\n",
            "18 ssssss\n",
            "{'current_ctc': 142587.0, 'expected_ctc': 147241.6, 'willing_to_relocate': 'yes', 'current_location': 'San Francisco', 'notice_period': '11 days', 'email_id': 'Soso_Sukhitashvili-64f85393cd422@example.com', 'resume_path': 'Soso_Sukhitashvili-64f85393cd422.pdf', 'resume_summary': {'name_of_candidate': 'Soso Sukhitashvili', 'mobile_number': '+995 598 55 65 74', 'email_id': 'sukhitashvili.soso@gmail.com', 'years_of_experience': 5, 'education': 'Master in Artificial Intelligence', 'university': 'IU International University of Applied Sciences', 'linkedin_profile': 'linkedin.com/in/soso-sukhitashvili', 'technical_skills': ['Python', 'Pytorch', 'OpenCV', 'Huggingface'], 'years_of_jobs': ['Sep 2018 - Present'], 'year_in_current_position': 2, 'present_organization': 'Cortica AI', 'summary': \"Soso Sukhitashvili is a machine learning engineer and algorithm developer with over 5 years of experience. His expertise lies in designing solutions for complex challenges in computer vision, NLP, and time-series analysis. He has a strong programming background and is skilled in Python, Pytorch, OpenCV, and Huggingface. Soso holds a Master's degree in Artificial Intelligence and is currently working at Cortica AI. He has contributed significantly to increasing AI model accuracy and reducing processing speed. With excellent communication and collaboration skills, Soso is a valuable asset to any team.\"}, 'name_of_candidate': 'Soso Sukhitashvili', 'years_of_experience': 5, 'len_must_have': 5, 'skills': {'PyTorch': 5, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 0}} resume_summary[name_of_candidate]\n",
            "5 ssssss\n",
            "{'current_ctc': 142711.1, 'expected_ctc': 148467.8, 'willing_to_relocate': 'yes', 'current_location': 'Oakland', 'notice_period': '14 days', 'email_id': 'Naukri_SAIKIRAN2y_0m-64f81dbf2960f@example.com', 'resume_path': 'Naukri_SAIKIRAN2y_0m-64f81dbf2960f.pdf', 'resume_summary': {'name_of_candidate': 'SAI KIRAN', 'mobile_number': '9182393691', 'email_id': 'nuthanchoudari@gmail.com', 'education': 'B.Tech. (ECE) - 7.1 CGPA', 'university': 'G. Pullaiah College of Engineering and Technology', 'linkedin_profile': 'https://www.linkedin.com/in/n-sai-kiran-chowdary-798943197/', 'technical_skills': ['Python', 'PyTorch', 'Opencv', 'SQL(Beginner)', 'Linux', 'Pandas', 'Matplotlib', 'Image Processing', 'GIT', 'Numpy', 'Pandas', 'PIL', 'Computer Vision', 'Dataloop', 'Azure Custom Vision', 'Basic understanding of NLP, DeepStream, ONNX, IOT, Prompting with CHATGPT'], 'years_of_jobs': ['August 2021 - November 2021', 'November 2021 - Present'], 'year_in_current_position': 1, 'Present_Organization': 'Infosys Autonomous Store', 'years_of_experience': 2, 'summary': 'SAI KIRAN is a Computer Vision Practitioner with 2 years of experience. He holds a B.Tech. degree in Electronics and Communication Engineering from G. Pullaiah College of Engineering and Technology, with a CGPA of 7.1. He has expertise in Python, PyTorch, Opencv, and Image Processing. SAI KIRAN has worked on projects like Road Accident Severity Prediction and Plant Seedlings Classification, and has received 2 Insta Awards for his exceptional work in data-engineering track and model evaluations. Currently, he is working as a Systems Engineer at Infosys Autonomous Store, where he is involved in building a cashierless store using computer vision and deep learning models. SAI KIRAN is a proactive learner and has a desire to explore new technologies and tasks.'}, 'name_of_candidate': 'SAI KIRAN', 'years_of_experience': 2, 'len_must_have': 5, 'skills': {'PyTorch': 5, 'Computer Vision': 2, 'Keras': 0, 'TensorFlow': 0}} resume_summary[name_of_candidate]\n",
            "2 ssssss\n",
            "{'current_ctc': 82538.3, 'expected_ctc': 90773.9, 'willing_to_relocate': 'yes', 'current_location': 'San Diego', 'notice_period': '11 days', 'email_id': 'cv_jjpc_english_new@example.com', 'resume_path': 'cv_jjpc_english_new.docx-64f866c66c579.pdf', 'resume_summary': {'name_of_candidate': 'Jose Jesus Cabrera Pantoja', 'mobile_number': '+591 75574466', 'email_id': 'cp.josejesus@gmail.com', 'education': 'MSc, Mechatronics and Robotics', 'university': 'Bauman Moscow State Technical University (BMSTU)', 'linkedin_profile': None, 'technical_skills': ['MATLAB', 'ROS', 'OpenCV', 'Python'], 'years_of_jobs': ['09.2022-present', '07.2021-07.2022', '02.2021-07.2022', '2018-2020', '01.2018-06.2018', '09.2017-12.2017', '06.2017-08.2017', '09.2018-06.2019', '08.2018', '07.2016-08.2016', '05.2022-06.2022', '02.09.2019', '08.2019-09.2019', '10.2018-12.2018'], 'year_in_current_position': 1, 'Present_Organization': 'Universidad Catolica Boliviana', 'years_of_experience': 5, 'summary': \"Jose Jesus Cabrera Pantoja is an experienced Mechatronics and Robotics professional with a Master's degree from Bauman Moscow State Technical University (BMSTU). He has expertise in MATLAB, ROS, OpenCV, and Python. With over 5 years of experience in various academic and industrial roles, including teaching, research, and internships, Jose is skilled in control systems, computer vision, and mathematical modeling. He has a strong interest in research and development, innovation, bio-inspired robotics, mobile robots, artificial intelligence, reinforcement learning, and programming. Jose is currently working as a full-time educational coordinator at Universidad Catolica Boliviana.\"}, 'name_of_candidate': 'Jose Jesus Cabrera Pantoja', 'years_of_experience': 5, 'len_must_have': 5, 'skills': {'PyTorch': 0, 'Computer Vision': 2, 'Keras': 0, 'TensorFlow': 0}} resume_summary[name_of_candidate]\n",
            "5 ssssss\n",
            "{'current_ctc': 57844.1, 'expected_ctc': 92667.3, 'willing_to_relocate': 'no', 'current_location': 'Sacramento', 'notice_period': '15 days', 'email_id': 'MyCV-64ff7289e2657@example.com', 'resume_path': 'MyCV-64ff7289e2657.pdf', 'resume_summary': {'name_of_candidate': 'Lucky Dube', 'mobile_number': '+44 (0) 1234 5678912', 'email_id': 'mymail@mail.com', 'years_of_experience': 2, 'education': 'Electronic Engineering and Computer Systems (1st)', 'university': 'University of Maddersfield', 'linkedin_profile': '', 'technical_skills': ['ASPNET / .NET', 'C/C++', 'Python', 'Bash/Linux', 'SQL', 'OOP', 'Git'], 'years_of_jobs': ['Jul 2022 - Present'], 'year_in_current_position': 1, 'Present_Organization': 'FC WorkForce', 'summary': 'Proactive and technically-astute software engineer well-versed in object-oriented programming principles. Adept in programming languages such as ASPNET / .NET, C/C++, Python, Bash/Linux, SQL, OOP, and Git. Experienced in Azure and Docker Desktop. Completed Electronic Engineering and Computer Systems (1st) from University of Maddersfield. Currently working at FC WorkForce as an Agency Worker since Jul 2022, with a total of 2 years of experience.'}, 'name_of_candidate': 'Lucky Dube', 'years_of_experience': 2, 'len_must_have': 5, 'skills': {'PyTorch': 0, 'Computer Vision': 2, 'Keras': 0, 'TensorFlow': 0}} resume_summary[name_of_candidate]\n",
            "2 ssssss\n",
            "{'current_ctc': 73949.8, 'expected_ctc': 84979.9, 'willing_to_relocate': 'no', 'current_location': 'Sacramento', 'notice_period': '12 days', 'email_id': 'AhmedY@example.com', 'resume_path': 'AhmedY.Resume-64f82d3f922d1.pdf', 'resume_summary': {'name_of_candidate': 'AHMED YASSIN', 'mobile_number': '+201128832884', 'email_id': 'ahmedyassin@skiff.com', 'years_of_experience': 1, 'education': \"Bachelor's degree in Engineering\", 'university': 'Al-Azhar University', 'linkedin_profile': 'linkedin.com/in/yassin01', 'technical_skills': ['Python', 'SQL', 'Jupyter Notebook', 'Google Colab', 'Git/GitHub', 'Scikit-Learn', 'Keras', 'Tensorflow', 'PyTorch', 'NumPy', 'OpenCV', 'NLTK', 'Matplotlib'], 'years_of_jobs': ['2023 - Present', '2021 - 2021'], 'year_in_current_position': 0, 'Present_Organization': 'Zaka AI', 'summary': 'AHMED YASSIN is a highly motivated Computer Engineering graduate with a strong passion for Data Science and AI. He has recently completed a machine learning specialization training program and has hands-on experience in data science and machine learning projects. With a comprehensive understanding of various machine learning algorithms and deep learning techniques, AHMED has also explored the applications of natural language processing (NLP) and computer vision. His technical skills include Python, SQL, Jupyter Notebook, Git/GitHub, Scikit-Learn, Keras, TensorFlow, PyTorch, NumPy, OpenCV, NLTK, and Matplotlib. AHMED has 1 year of overall work experience and is currently working at Zaka AI. He is eager to contribute to end-to-end data science projects and make an impact.'}, 'name_of_candidate': 'AHMED YASSIN', 'years_of_experience': 1, 'len_must_have': 6, 'skills': {'PyTorch': 0, 'TensorFlow': 0, 'skills': 0, 'Computer Vision': 0, 'Keras': 0}} resume_summary[name_of_candidate]\n",
            "1 ssssss\n",
            "[Final Ranking] Candidate Name: Yogessh J Bhati\n",
            "[Final Ranking] Years of Experience: 5\n",
            "[Final Ranking] Skill Score: 3.5\n",
            "[Final Ranking] Skills: {'PyTorch': 5, 'Computer Vision': 5, 'Keras': 2, 'TensorFlow': 2}\n",
            "[Final Ranking] Final Score: 0.8601474805833617\n",
            "----------------------------------------\n",
            "[Final Ranking] Candidate Name: PANKAJ KUMAR GOYAL\n",
            "[Final Ranking] Years of Experience: 3\n",
            "[Final Ranking] Skill Score: 1.8\n",
            "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 5, 'Keras': 2, 'TensorFlow': 2}\n",
            "[Final Ranking] Final Score: 0.5599999999999999\n",
            "----------------------------------------\n",
            "[Final Ranking] Candidate Name: Joseph Adeola\n",
            "[Final Ranking] Years of Experience: 4\n",
            "[Final Ranking] Skill Score: 1.8\n",
            "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 5, 'Keras': 2, 'TensorFlow': 2}\n",
            "[Final Ranking] Final Score: 0.5491918937813531\n",
            "----------------------------------------\n",
            "[Final Ranking] Candidate Name: Abhilash Babu\n",
            "[Final Ranking] Years of Experience: 18\n",
            "[Final Ranking] Skill Score: 2.4000000000000004\n",
            "[Final Ranking] Skills: {'PyTorch': 5, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 2}\n",
            "[Final Ranking] Final Score: 0.4800007453306345\n",
            "----------------------------------------\n",
            "[Final Ranking] Candidate Name: Soso Sukhitashvili\n",
            "[Final Ranking] Years of Experience: 5\n",
            "[Final Ranking] Skill Score: 1.5\n",
            "[Final Ranking] Skills: {'PyTorch': 5, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 0}\n",
            "[Final Ranking] Final Score: 0.46014748058336163\n",
            "----------------------------------------\n",
            "[Final Ranking] Candidate Name: SAI KIRAN\n",
            "[Final Ranking] Years of Experience: 2\n",
            "[Final Ranking] Skill Score: 1.05\n",
            "[Final Ranking] Skills: {'PyTorch': 5, 'Computer Vision': 2, 'Keras': 0, 'TensorFlow': 0}\n",
            "[Final Ranking] Final Score: 0.388967863362874\n",
            "----------------------------------------\n",
            "[Final Ranking] Candidate Name: Naren Sadhwani\n",
            "[Final Ranking] Years of Experience: 9\n",
            "[Final Ranking] Skill Score: 1.5\n",
            "[Final Ranking] Skills: {'PyTorch': 5, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 0}\n",
            "[Final Ranking] Final Score: 0.32706705664732255\n",
            "----------------------------------------\n",
            "[Final Ranking] Candidate Name: Ayşin Sancı\n",
            "[Final Ranking] Years of Experience: 14\n",
            "[Final Ranking] Skill Score: 1.5\n",
            "[Final Ranking] Skills: {'PyTorch': 5, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 0}\n",
            "[Final Ranking] Final Score: 0.30024077199896565\n",
            "----------------------------------------\n",
            "[Final Ranking] Candidate Name: Lucky Dube\n",
            "[Final Ranking] Years of Experience: 2\n",
            "[Final Ranking] Skill Score: 0.2\n",
            "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 2, 'Keras': 0, 'TensorFlow': 0}\n",
            "[Final Ranking] Final Score: 0.21896786336287394\n",
            "----------------------------------------\n",
            "[Final Ranking] Candidate Name: Julian Barcenas\n",
            "[Final Ranking] Years of Experience: 5\n",
            "[Final Ranking] Skill Score: 0.2\n",
            "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 0, 'Keras': 0, 'TensorFlow': 2}\n",
            "[Final Ranking] Final Score: 0.20014748058336163\n",
            "----------------------------------------\n",
            "[Final Ranking] Candidate Name: Jose Jesus Cabrera Pantoja\n",
            "[Final Ranking] Years of Experience: 5\n",
            "[Final Ranking] Skill Score: 0.2\n",
            "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 2, 'Keras': 0, 'TensorFlow': 0}\n",
            "[Final Ranking] Final Score: 0.20014748058336163\n",
            "----------------------------------------\n",
            "[Final Ranking] Candidate Name: Tahsin Samia\n",
            "[Final Ranking] Years of Experience: 4\n",
            "[Final Ranking] Skill Score: 0.0\n",
            "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 0, 'Keras': 0, 'TensorFlow': 0}\n",
            "[Final Ranking] Final Score: 0.1891918937813531\n",
            "----------------------------------------\n",
            "[Final Ranking] Candidate Name: Paula Ramos\n",
            "[Final Ranking] Years of Experience: 9\n",
            "[Final Ranking] Skill Score: 0.5\n",
            "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 0}\n",
            "[Final Ranking] Final Score: 0.12706705664732254\n",
            "----------------------------------------\n",
            "[Final Ranking] Candidate Name: AHMED YASSIN\n",
            "[Final Ranking] Years of Experience: 1\n",
            "[Final Ranking] Skill Score: 0.0\n",
            "[Final Ranking] Skills: {'PyTorch': 0, 'TensorFlow': 0, 'skills': 0, 'Computer Vision': 0, 'Keras': 0}\n",
            "[Final Ranking] Final Score: 0.10686339807165911\n",
            "----------------------------------------\n",
            "[Final Ranking] Candidate Name: Derrick I.C. VAN FRAUSUM\n",
            "[Final Ranking] Years of Experience: 9\n",
            "[Final Ranking] Skill Score: 0.0\n",
            "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 0, 'Keras': 0, 'TensorFlow': 0}\n",
            "[Final Ranking] Final Score: 0.027067056647322542\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output\n",
        "\n",
        "```\n",
        "[Final Ranking] Candidate Name: Yogessh J Bhati\n",
        "[Final Ranking] Years of Experience: 5\n",
        "[Final Ranking] Skill Score: 5.0\n",
        "[Final Ranking] Skills: {'PyTorch': 5, 'Computer Vision': 5, 'Keras': 5, 'TensorFlow': 5}\n",
        "[Final Ranking] Final Score: 1.1601474805833616\n",
        "----------------------------------------\n",
        "[Final Ranking] Candidate Name: PANKAJ KUMAR GOYAL\n",
        "[Final Ranking] Years of Experience: 3\n",
        "[Final Ranking] Skill Score: 1.8\n",
        "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 5, 'Keras': 2, 'TensorFlow': 2}\n",
        "[Final Ranking] Final Score: 0.5599999999999999\n",
        "----------------------------------------\n",
        "[Final Ranking] Candidate Name: Soso Sukhitashvili\n",
        "[Final Ranking] Years of Experience: 5\n",
        "[Final Ranking] Skill Score: 1.5\n",
        "[Final Ranking] Skills: {'PyTorch': 5, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 0}\n",
        "[Final Ranking] Final Score: 0.46014748058336163\n",
        "----------------------------------------\n",
        "[Final Ranking] Candidate Name: SAI KIRAN\n",
        "[Final Ranking] Years of Experience: 2\n",
        "[Final Ranking] Skill Score: 1.05\n",
        "[Final Ranking] Skills: {'PyTorch': 5, 'Computer Vision': 2, 'Keras': 0, 'TensorFlow': 0}\n",
        "[Final Ranking] Final Score: 0.388967863362874\n",
        "----------------------------------------\n",
        "[Final Ranking] Candidate Name: Naren Sadhwani\n",
        "[Final Ranking] Years of Experience: 9\n",
        "[Final Ranking] Skill Score: 1.5\n",
        "[Final Ranking] Skills: {'PyTorch': 5, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 0}\n",
        "[Final Ranking] Final Score: 0.32706705664732255\n",
        "----------------------------------------\n",
        "[Final Ranking] Candidate Name: Ayşin Sancı\n",
        "[Final Ranking] Years of Experience: 14\n",
        "[Final Ranking] Skill Score: 1.5\n",
        "[Final Ranking] Skills: {'PyTorch': 5, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 0}\n",
        "[Final Ranking] Final Score: 0.30024077199896565\n",
        "----------------------------------------\n",
        "[Final Ranking] Candidate Name: Joseph Adeola\n",
        "[Final Ranking] Years of Experience: 4\n",
        "[Final Ranking] Skill Score: 0.5\n",
        "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 0}\n",
        "[Final Ranking] Final Score: 0.28919189378135307\n",
        "----------------------------------------\n",
        "[Final Ranking] Candidate Name: AHMED YASSIN\n",
        "[Final Ranking] Years of Experience: 1\n",
        "[Final Ranking] Skill Score: 0.6\n",
        "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 2, 'Keras': 0, 'TensorFlow': 2}\n",
        "[Final Ranking] Final Score: 0.24823607768599093\n",
        "----------------------------------------\n",
        "[Final Ranking] Candidate Name: Lucky Dube\n",
        "[Final Ranking] Years of Experience: 2\n",
        "[Final Ranking] Skill Score: 0.2\n",
        "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 2, 'Keras': 0, 'TensorFlow': 0}\n",
        "[Final Ranking] Final Score: 0.21896786336287394\n",
        "----------------------------------------\n",
        "[Final Ranking] Candidate Name: Abhilash Babu\n",
        "[Final Ranking] Years of Experience: 18\n",
        "[Final Ranking] Skill Score: 1.05\n",
        "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 2}\n",
        "[Final Ranking] Final Score: 0.21000074533063443\n",
        "----------------------------------------\n",
        "[Final Ranking] Candidate Name: Julian Barcenas\n",
        "[Final Ranking] Years of Experience: 5\n",
        "[Final Ranking] Skill Score: 0.2\n",
        "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 0, 'Keras': 0, 'TensorFlow': 2}\n",
        "[Final Ranking] Final Score: 0.20014748058336163\n",
        "----------------------------------------\n",
        "[Final Ranking] Candidate Name: Jose Jesus Cabrera Pantoja\n",
        "[Final Ranking] Years of Experience: 5\n",
        "[Final Ranking] Skill Score: 0.2\n",
        "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 2, 'Keras': 0, 'TensorFlow': 0}\n",
        "[Final Ranking] Final Score: 0.20014748058336163\n",
        "----------------------------------------\n",
        "[Final Ranking] Candidate Name: Tahsin Samia\n",
        "[Final Ranking] Years of Experience: 4\n",
        "[Final Ranking] Skill Score: 0.0\n",
        "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 0, 'Keras': 0, 'TensorFlow': 0}\n",
        "[Final Ranking] Final Score: 0.1891918937813531\n",
        "----------------------------------------\n",
        "[Final Ranking] Candidate Name: Paula Ramos\n",
        "[Final Ranking] Years of Experience: 9\n",
        "[Final Ranking] Skill Score: 0.5\n",
        "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 5, 'Keras': 0, 'TensorFlow': 0}\n",
        "[Final Ranking] Final Score: 0.12706705664732254\n",
        "----------------------------------------\n",
        "[Final Ranking] Candidate Name: Derrick I.C. VAN FRAUSUM\n",
        "[Final Ranking] Years of Experience: 9\n",
        "[Final Ranking] Skill Score: 0.0\n",
        "[Final Ranking] Skills: {'PyTorch': 0, 'Computer Vision': 0, 'Keras': 0, 'TensorFlow': 0}\n",
        "[Final Ranking] Final Score: 0.027067056647322542\n",
        "----------------------------------------\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "EQHnhSDMt2Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ilez-kudYzvm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}