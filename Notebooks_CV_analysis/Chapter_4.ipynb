{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Description:\n",
        "The notebook focuses on automating the process of extracting and summarizing information from resumes. It uses OpenAI's GPT model to help in achieving this task. Specifically, it aims to read the content of the resumes and summarize their details, presenting them in both textual and JSON formats.\n",
        "# Learning Objectives:\n",
        "1. Learn how to set up the necessary environment and install required packages in a Colab notebook.\n",
        "2. Discover ways to preprocess and trim long texts for model consumption.\n",
        "3. Familiarize oneself with the OpenAI API and understand how to construct meaningful prompts for better output.\n",
        "Extract and represent resume details in multiple formats, focusing on standard text and JSON formats."
      ],
      "metadata": {
        "id": "PTIDgeSiFEjE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuWiHw2WUlyj"
      },
      "source": [
        "Upload the .env file to the directory `/content/` which contains the \"OPENAI_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "plnlXJ4D06Wc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40977b16-c34a-47c6-f18d-c4394fd7ae80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/76.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "# Libraries Installation\n",
        "!pip install openai\n",
        "# Required Libraries\n",
        "import openai\n",
        "import json\n",
        "import os\n",
        "from collections import OrderedDict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNlDscmRX--1"
      },
      "source": [
        "We set up our environment to use OpenAI's API for extracting information from Job Descriptions (JD). We'll use Python as our primary language and leverage the OpenAI library to interact with OpenAI's services\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the \"OPENAI_API_KEY\" from the .env file"
      ],
      "metadata": {
        "id": "z4044lWbZVAp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7PyU_wIbTXm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feca0a1d-f47d-4e9e-c510-173a3a271caf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Export your API Key to environment variable\n",
        "# Upload the .env file to the directory \"/content/\"\n",
        "!pip install python-dotenv\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "# Retrieve the API key from environment variable\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Set the API key for OpenAI\n",
        "openai.api_key = openai_api_key"
      ],
      "metadata": {
        "id": "zGb-g0ogat2Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the json file containing important information about the Job requirements which was generated in Assignment1 and the file containing information about the filtered resumes generated from Assignment2"
      ],
      "metadata": {
        "id": "EnWN2xeiMolr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the first file\n",
        "print(\"Please upload the file (filtered_applications.json):\")\n",
        "uploaded1 = files.upload()\n",
        "\n",
        "# Check to ensure a file was uploaded. If not, prompt again.\n",
        "while len(uploaded1) == 0:\n",
        "    print(\"No file uploaded. Please upload the first file (filtered_applications.json) again:\")\n",
        "    uploaded1 = files.upload()\n",
        "print(\"Please upload the file (all_applications.json):\")\n",
        "uploaded1 = files.upload()\n",
        "\n",
        "# Check to ensure a file was uploaded. If not, prompt again.\n",
        "while len(uploaded1) == 0:\n",
        "    print(\"No file uploaded. Please upload the first file (filtered_applications.json) again:\")\n",
        "    uploaded1 = files.upload()\n",
        "# Upload the second file\n",
        "print(\"Please upload the file (requirements_output.json):\")\n",
        "uploaded2 = files.upload()\n",
        "\n",
        "# Check to ensure a file was uploaded. If not, prompt again.\n",
        "while len(uploaded2) == 0:\n",
        "    print(\"No file uploaded. Please upload the second file (requirements_output.json) again:\")\n",
        "    uploaded2 = files.upload()\n",
        "\n",
        "# Merge the dictionaries to have all uploaded files in one\n",
        "uploaded = {**uploaded1, **uploaded2}\n",
        "\n",
        "# Print details of uploaded files\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "ZlbgaPtoMcc4",
        "outputId": "a80bded3-c6c6-47d8-9b65-94bfc1e47b30"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload the file (filtered_applications.json):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9a8ac557-8f21-4adf-b03b-b3f80149e3d3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9a8ac557-8f21-4adf-b03b-b3f80149e3d3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving filtered_applications.json to filtered_applications.json\n",
            "Please upload the file (all_applications.json):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-af432869-e270-4012-94ae-d0d56beb13f5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-af432869-e270-4012-94ae-d0d56beb13f5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving all_applications.json to all_applications.json\n",
            "Please upload the file (requirements_output.json):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9b98098b-08fb-4cc2-b834-3820cd0fa120\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9b98098b-08fb-4cc2-b834-3820cd0fa120\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving requirements_output.json to requirements_output.json\n",
            "User uploaded file \"all_applications.json\" with length 4809 bytes\n",
            "User uploaded file \"requirements_output.json\" with length 764 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now download the `Webinar_resumes.zip` file which contains all the resumes"
      ],
      "metadata": {
        "id": "4CXeqdALvLVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(file_id, destination):\n",
        "    base_url = \"https://drive.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(base_url, params={'id': file_id}, stream=True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = {'id': file_id, 'confirm': token}\n",
        "        response = session.get(base_url, params=params, stream=True)\n",
        "\n",
        "    save_response_content(response, destination)\n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "# Example Usage\n",
        "# file_id = '1HaM3IeK2-iqyZzeQmCnAzKLcF9NF-mSo'  # Replace with your file's ID\n",
        "# destination = 'resume_data.zip'  # Replace with your desired file name and extension\n",
        "file_id = '17V_o0Snt-Lj0FmegENPQ_rXpvWTWlZgQ'\n",
        "destination = 'Webinar_resumes.zip'  # Replace with your desired file name and extension\n",
        "download_file_from_google_drive(file_id, destination)"
      ],
      "metadata": {
        "id": "q4KdnZe0vKf5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code imports various libraries to facilitate file handling and natural language processing. Libraries like docx and textract process Word documents, while fitz handles PDFs. The os, json, and pandas libraries aid in file operations and data management. The following script's functions read job requirements from JSON files and content from different file formats such as DOCX, DOC, PDF, and Excel, and condenses resume texts to a specified number of tokens using nltk to maintain manageable input sizes."
      ],
      "metadata": {
        "id": "RZGpj_qLDZn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import important libraries\n",
        "!pip install PyMuPDF\n",
        "!pip install textract\n",
        "!pip install python-docx\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "MrXQhPFHKqnE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ddcc3b7c-d270-44a7-805e-5e687fe08168"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.23.3-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.3 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.23.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.23.3 PyMuPDFb-1.23.3\n",
            "Collecting textract\n",
            "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
            "Collecting argcomplete~=1.10.0 (from textract)\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting beautifulsoup4~=4.8.0 (from textract)\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==3.* (from textract)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docx2txt~=0.8 (from textract)\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting extract-msg<=0.29.* (from textract)\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20191110 (from textract)\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-pptx~=0.6.18 (from textract)\n",
            "  Downloading python_pptx-0.6.22-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six~=1.12.0 (from textract)\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting SpeechRecognition~=3.8.1 (from textract)\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xlrd~=1.2.0 (from textract)\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodome (from pdfminer.six==20191110->textract)\n",
            "  Downloading pycryptodome-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4~=4.8.0->textract) (2.5)\n",
            "Collecting imapclient==2.1.0 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting olefile>=0.46 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tzlocal>=2.1 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (5.0.1)\n",
            "Collecting compressed-rtf>=1.0.6 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ebcdic>=1.1.1 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (4.9.3)\n",
            "Requirement already satisfied: Pillow<=9.5.0,>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (9.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx~=0.6.18->textract)\n",
            "  Downloading XlsxWriter-3.1.3-py3-none-any.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docx2txt, compressed-rtf, olefile\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=d4f2eb75a839203a6242e642c07a06544629639f67cd1d12ca2ddc3492a28c6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6185 sha256=499a8e6525493aafab0ed34715b8259d5292fbe06a3b9776bca79ebacfd4e9a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/3e/48/e7d833ecc516c36f8966d310b1a6386db091a718f1ff3bf85c\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=23b57bfe270b526b6eb1ff9327cea2c0f578d4f1a8f6fcf75bd26bb93cd80714\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/39/c0/9eb1f7a42b4b38f6f333b6314d4ed11c46f12a0f7b78194f0d\n",
            "Successfully built docx2txt compressed-rtf olefile\n",
            "Installing collected packages: SpeechRecognition, ebcdic, docx2txt, compressed-rtf, chardet, argcomplete, XlsxWriter, xlrd, six, pycryptodome, olefile, beautifulsoup4, python-pptx, pdfminer.six, imapclient, extract-msg, textract\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 2.0.1\n",
            "    Uninstalling xlrd-2.0.1:\n",
            "      Successfully uninstalled xlrd-2.0.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.11.2\n",
            "    Uninstalling beautifulsoup4-4.11.2:\n",
            "      Successfully uninstalled beautifulsoup4-4.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.6.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "yfinance 0.2.28 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SpeechRecognition-3.8.1 XlsxWriter-3.1.3 argcomplete-1.10.3 beautifulsoup4-4.8.2 chardet-3.0.4 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.19.0 python-pptx-0.6.22 six-1.12.0 textract-1.6.5 xlrd-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/5.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/5.6 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184487 sha256=270dbefed10a63204399666e94ad0d4ae8fff4c4d45b3c78acddd96b0e2cfee8\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries and modules\n",
        "from docx import Document\n",
        "import textract\n",
        "import fitz  # PyMuPDF\n",
        "import openai\n",
        "import json\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import re\n",
        "import pandas as pd\n",
        "import math\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "def read_requirements(file_path):\n",
        "    \"\"\"\n",
        "    Read the job requirements from a given JSON file.\n",
        "\n",
        "    Args:\n",
        "    - file_path (str): Path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Job requirements if successfully read, otherwise None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading requirements JSON: {e}\")\n",
        "        return None\n",
        "\n",
        "def read_json(file_path):\n",
        "    \"\"\"\n",
        "    Read data from a given JSON file.\n",
        "\n",
        "    Args:\n",
        "    - file_path (str): Path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Data from the JSON file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def read_document(file_path):\n",
        "    \"\"\"\n",
        "    Read and extract text from various document types (.docx, .doc, .pdf, .xls, .xlsx).\n",
        "\n",
        "    Args:\n",
        "    - file_path (str): Path to the document file.\n",
        "\n",
        "    Returns:\n",
        "    - str: Extracted text from the document.\n",
        "    \"\"\"\n",
        "    file_path = str(file_path)\n",
        "    _, file_extension = os.path.splitext(file_path)\n",
        "    text = \"\"\n",
        "    if file_extension == '.docx':\n",
        "        doc = Document(file_path)\n",
        "        for para in doc.paragraphs:\n",
        "            text = text + para.text + \" \"\n",
        "    elif file_extension == '.doc':\n",
        "        text = textract.process(file_path).decode()\n",
        "    elif file_extension.lower() == '.pdf':\n",
        "        doc = fitz.open(file_path)\n",
        "        for page_number in range(len(doc)):\n",
        "            page = doc[page_number]\n",
        "            text = text + page.get_text() + \" \"\n",
        "    elif file_extension.lower() in ['.xls', '.xlsx']:\n",
        "        data = pd.read_excel(file_path)\n",
        "        text = data.to_string(index=False)\n",
        "    else:\n",
        "        print(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def check_and_trim(resume_text, max_tokens=1500):\n",
        "    \"\"\"\n",
        "    Trim the text to a specified number of tokens if it exceeds the limit.\n",
        "\n",
        "    Args:\n",
        "    - resume_text (str): Text to be trimmed.\n",
        "    - max_tokens (int, optional): Maximum number of tokens allowed. Defaults to 1500.\n",
        "\n",
        "    Returns:\n",
        "    - str: Trimmed text.\n",
        "    - int: Original number of tokens.\n",
        "    - int: Number of tokens after trimming.\n",
        "    \"\"\"\n",
        "    # tokens = nltk.word_tokenize(resume_text)\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tokens = enc.encode(resume_text)\n",
        "    old_len = len(tokens)\n",
        "    if len(tokens) > max_tokens:\n",
        "        tokens = tokens[:max_tokens]\n",
        "        resume_text = enc.decode(tokens)\n",
        "    return resume_text, old_len, len(tokens)\n"
      ],
      "metadata": {
        "id": "hHREyeznc5yi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below function `summarize_resume` takes two arguments: `prompt` and `text`. Its primary purpose is to summarize the content of a resume.\n",
        "\n",
        "`Building the Conversation Messages`: Inside the function, a messages list is initialized with two entries. The first entry has the role `system` and provides a contextual instruction or prompt (specified by the prompt argument) to the model. The second entry has the role `user` and contains the content of the resume (specified by the text argument). This list emulates a conversation where the system sets the context and the user provides the input.\n",
        "\n",
        "`Generating the Response`: The `openai.ChatCompletion.create` method is then called with several parameters:\n",
        "\n",
        "`model=\"gpt-3.5-turbo-16k\"`: Specifies the model variant to be used for the task.\n",
        "\n",
        "`messages`: Provides the constructed conversation to the model.\n",
        "\n",
        "`temperature=1`: This dictates the randomness of the model's output. A value closer to 1 makes the model's responses more random, while a value closer to 0 makes them more deterministic.\n",
        "\n",
        "`max_tokens=13000`: Limits the response to a maximum of 13,000 tokens to ensure the output isn't too lengthy.\n",
        "\n",
        "`Extracting the Summary`: After the response is generated, the content of the response is extracted, stripped of any leading or trailing white spaces, and stored in the `generated_texts` list.\n",
        "\n",
        "`Returning the Result`: Finally, the function returns the first (and only) item in the generated_texts list, which is the summarized content of the resume."
      ],
      "metadata": {
        "id": "PPEvBf7iDsnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_resume(prompt, text):\n",
        "    \"\"\"\n",
        "    Summarize the given resume text using the OpenAI API with a specified prompt.\n",
        "\n",
        "    Args:\n",
        "    - prompt (str): The leading instruction or question for the model.\n",
        "    - text (str): The resume text that needs to be summarized.\n",
        "\n",
        "    Returns:\n",
        "    - str: Summarized text as returned by the OpenAI model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a list of messages to simulate a conversation with the OpenAI model.\n",
        "    # The system starts with a prompt and the user provides the resume text.\n",
        "    messages = [\n",
        "            {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
        "            {\"role\": \"user\", \"content\": text },\n",
        "        ]\n",
        "\n",
        "    # Make a request to the OpenAI API to get the summary.\n",
        "    # Using the 'gpt-3.5-turbo-16k' model for completion.\n",
        "    response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-16k\",\n",
        "            messages=messages,\n",
        "            temperature=1,\n",
        "            max_tokens=13000  # Setting a maximum token limit for the model's output\n",
        "        )\n",
        "\n",
        "    # Extract the generated text from the response.\n",
        "    # Since there's only one message in the choices, we're taking the first message's content.\n",
        "    generated_texts = [\n",
        "        choice.message[\"content\"].strip() for choice in response[\"choices\"]\n",
        "    ]\n",
        "\n",
        "    return generated_texts[0]"
      ],
      "metadata": {
        "id": "e1Od9U8Jf25m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "In the subsequent sections, we'll iteratively craft a prompt designed to extract pertinent information from the curated resumes. The extracted data will be presented in JSON format, ensuring that the keys remain consistent across multiple runs."
      ],
      "metadata": {
        "id": "RDfaxQDJORKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The provided code imports the necessary zipfile module and then reads job requirements from a JSON file, extracting the `must_have_skills`. It also reads filtered job applications from another JSON, focusing on the first two. The ZIP file named `resume_data.zip` located in the `/content/` directory is then extracted to a folder named `extracted_files`. Post extraction, the code iterates through the items in this folder and, if any directory names contain spaces, renames them by replacing spaces with underscores, ensuring a clean directory structure for the resumes. The final path to this directory of resumes is stored in the `resume_path` variable."
      ],
      "metadata": {
        "id": "SRUi44OCyA6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "def read_json(filepath):\n",
        "    \"\"\"\n",
        "    Reads a JSON file and returns the data.\n",
        "\n",
        "    Args:\n",
        "    - filepath (str): Path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Data from the JSON file.\n",
        "    \"\"\"\n",
        "    with open(filepath, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "def user_select_number_of_resumes(total_resumes, default=2):\n",
        "    \"\"\"\n",
        "    Allow the user to input a number of resumes to process.\n",
        "    If no input is given, the default value is returned.\n",
        "\n",
        "    Args:\n",
        "    - total_resumes (int): Total number of resumes available.\n",
        "    - default (int): The default number to return if no input.\n",
        "\n",
        "    Returns:\n",
        "    - int: The number of resumes the user wants to process.\n",
        "    \"\"\"\n",
        "    print(f\"Total resumes available: {total_resumes}\")\n",
        "    user_input = input(f\"How many resumes do you want to process? (Default is {default}): \")\n",
        "\n",
        "    # If the user doesn't provide any input, return the default value.\n",
        "    if not user_input:\n",
        "        return default\n",
        "\n",
        "    try:\n",
        "        # Convert user input to an integer and ensure it's within the range.\n",
        "        selected_num = int(user_input)\n",
        "        if 1 <= selected_num <= total_resumes:\n",
        "            return selected_num\n",
        "        else:\n",
        "            print(f\"Please select a number between 1 and {total_resumes}.\")\n",
        "            return user_select_number_of_resumes(total_resumes, default)\n",
        "    except ValueError:\n",
        "        # If the user provides non-numeric input, prompt them again.\n",
        "        print(\"Please enter a valid number.\")\n",
        "        return user_select_number_of_resumes(total_resumes, default)\n",
        "\n",
        "# Read the filtered_applications data from the JSON file\n",
        "json_data = read_json('/content/filtered_applications.json')\n",
        "\n",
        "# Display total resumes and get the user's choice\n",
        "n = user_select_number_of_resumes(len(json_data))\n",
        "\n",
        "# Randomly select n resumes\n",
        "selected_applications = random.sample(json_data, n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoE2N19aoMkM",
        "outputId": "592cf593-56d7-4d4b-c570-30919cf2dec4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total resumes available: 12\n",
            "How many resumes do you want to process? (Default is 2): 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "job_requirements = read_requirements('/content/requirements_output.json')\n",
        "must_have_skills = job_requirements[\"must_have_skills\"]\n",
        "zip_file_path = \"/content/Webinar_resumes.zip\" # For example give the path to resume_data.zip\n",
        "\n",
        "\n",
        "def extract_and_rename(zip_file_path, extract_path=\"extracted_files\"):\n",
        "    \"\"\"\n",
        "    Extract files from a zip archive to a specified directory.\n",
        "    Rename directories containing spaces to use underscores instead.\n",
        "\n",
        "    Args:\n",
        "    - zip_file_path (str): The path to the zip file to be extracted.\n",
        "    - extract_path (str, optional): The path where the zip file content should be extracted to.\n",
        "                                    Defaults to \"extracted_files\".\n",
        "\n",
        "    Returns:\n",
        "    - str: Path to the resume or directory.\n",
        "    \"\"\"\n",
        "    # Check if extract_path exists, if not, create it\n",
        "    if not os.path.exists(extract_path):\n",
        "        os.makedirs(extract_path)\n",
        "\n",
        "    # If extract_path is not empty, skip extraction\n",
        "    if not os.listdir(extract_path):\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "\n",
        "    resume_path = extract_path\n",
        "    for item in os.listdir(extract_path):\n",
        "        item_path = os.path.join(extract_path, item)\n",
        "\n",
        "        # Check if the current item is a directory and if it has spaces in its name\n",
        "        if os.path.isdir(item_path) and ' ' in item:\n",
        "            new_name = item.replace(' ', '_')\n",
        "            new_path = os.path.join(extract_path, new_name)\n",
        "\n",
        "            # If the new directory name doesn't already exist, create it\n",
        "            if not os.path.exists(new_path):\n",
        "                os.makedirs(new_path)\n",
        "\n",
        "            # Copying contents from the old directory to the new one\n",
        "            for sub_item in os.listdir(item_path):\n",
        "                shutil.copy2(os.path.join(item_path, sub_item), new_path)\n",
        "\n",
        "            # Removing the old directory\n",
        "            shutil.rmtree(item_path)\n",
        "            resume_path = new_path\n",
        "        else:\n",
        "            resume_path = item_path\n",
        "\n",
        "    return resume_path\n",
        "resume_path = extract_and_rename(zip_file_path)"
      ],
      "metadata": {
        "id": "u_Ip_nvzgO2X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Version 1:\n",
        "\n",
        "### *Extract key details from the resume using a basic prompt.*\n",
        "### *This is a more open-ended prompt to get an initial sense of what the model understands and extracts from a resume without any constraints.*"
      ],
      "metadata": {
        "id": "1Pq3zmvhkZX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "prompt_v1=f'''Read the given resume and extract information such as the candidate name, mobile number, email_id, total years of experience, the candidate's last education degree \\\n",
        "last university attended by the candidate University, extract the candidate's linkedin profile, record all the technical skills, years spent in different jobs,\n",
        "years spent in the current organization, name of the present organization and the summary'''\n",
        "\n",
        "for application in selected_applications:\n",
        "    if 'resume_path' in application and 'email_id' in application:\n",
        "        resume_text = read_document(os.path.join(resume_path, application['resume_path']))\n",
        "        resume_text, _, _ = check_and_trim(resume_text)\n",
        "        resume_summary = summarize_resume(prompt_v1, resume_text)\n",
        "        print(\"[Resume Summary] \", resume_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-79dgoQcwaN",
        "outputId": "b9548049-a0c5-4456-9c50-e8c95634bbd8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Resume Summary]  Candidate Name: Paula Ramos\n",
            "Mobile Number: 919-786-3615\n",
            "Email: pjramg@gmail.com\n",
            "Total Years of Experience: Ph.D in Engineering (Computer Science & Image Processing) from Jan 2012 - Apr 2018\n",
            "Last Education Degree: Ph.D in Engineering (Computer Science & Image Processing)\n",
            "Last University Attended: Universidad Nacional de Colombia\n",
            "LinkedIn Profile: linkedin.com/in/paula-ramos-41097319\n",
            "Technical Skills: Computer Vision, Machine Learning, Image Processing, Signal Processing, Control and Automation, Robotics, Embedded Systems, Mobile Devices\n",
            "Years Spent in Different Jobs:\n",
            "- AI Software Development Engineer at Intel Corporation: Nov 2021 - On going\n",
            "- Research Scholar at North Carolina State University - USDA ARS: Jul 2020 - Oct 2022\n",
            "- Postdoctoral Researcher at North Carolina State University - USDA ARS: Jun 2019 - Jun 2020\n",
            "- Research Scholar at North Carolina State University: Feb 2019 - Jun 2019\n",
            "- Research Scientist at National Research Center of Coffee: Feb 2010 - Dec 2018\n",
            "- Doctoral Internship at Institut Valencià d’Investigacions Agraries: Sept 2014 - Marzo 2015\n",
            "- Junior Researcher at National Research Center of Coffee: June 2004 - Jan 2010\n",
            "- Lecturer at Universidad Tecnológica de Pereira: June 2006 - Dic 2011\n",
            "Years Spent in Current Organization: AI Software Development Engineer at Intel Corporation: Nov 2021 - On going\n",
            "Present Organization: Intel Corporation\n",
            "Summary: Research in new AI technologies based on image (2D - 3D) and signal processing, control and automation, robotics, machine learning, embedded systems, and mobile devices.\n",
            "[Resume Summary]  Name: Soso Sukhitashvili\n",
            "Mobile Number: +995 598 55 65 74\n",
            "Email: sukhitashvili.soso@gmail.com\n",
            "Total years of experience: 5 years\n",
            "Last education degree: Master in Artificial Intelligence (ongoing)\n",
            "Last university attended: IU International University of Applied Sciences\n",
            "LinkedIn Profile: linkedin.com/in/soso-sukhitashvili\n",
            "\n",
            "Technical Skills:\n",
            "- Strong Programming Skills\n",
            "- Statistical Analysis and Mathematics\n",
            "- Data Science / Machine Learning / Deep Learning Research\n",
            "- Generative modeling: GANs, stable diffusion\n",
            "- Data Analysis & Cluster Analysis\n",
            "- Forecasting & Predictive Analytics\n",
            "- Python\n",
            "- Pytorch\n",
            "- OpenCV\n",
            "- Huggingface\n",
            "- Spacy\n",
            "- Data Mining & Data Visualization\n",
            "- Pattern Recognition\n",
            "- Amazon Web Services (AWS)\n",
            "\n",
            "Work Experience:\n",
            "- Cortica AI: May 2021 - Present\n",
            "- MaxinAI: Sep 2018 - May 2021\n",
            "\n",
            "Years spent in the current organization: 1 year\n",
            "\n",
            "Present Organization: Cortica AI\n",
            "\n",
            "Summary:\n",
            "I am a machine learning engineer and algorithm developer with over 5 years of experience in the field. My expertise lies in designing and developing cutting-edge solutions for complex challenges in various domains, such as computer vision, natural language processing, and time-series analysis. I excel in swiftly adjusting to and acquiring new technologies/methods, which enables me to seamlessly blend in with a team. I seek a demanding, dynamic setting where I can continuously enhance my skills.\n",
            "[Resume Summary]  Candidate Name: Ahmed Yassin\n",
            "Mobile Number: +201128832884\n",
            "Email: ahmedyassin@skiff.com\n",
            "Total Years of Experience: N/A (No information given)\n",
            "Last Education Degree: Bachelor's degree in Engineering specializing in Computers and Systems Engineering\n",
            "Last University: Al-Azhar University, Cairo, Egypt\n",
            "LinkedIn Profile: linkedin.com/in/yassin01\n",
            "\n",
            "Technical Skills:\n",
            "- Python\n",
            "- SQL\n",
            "- Jupyter Notebook\n",
            "- Google Colab\n",
            "- Git/GitHub\n",
            "- Scikit-Learn\n",
            "- Keras\n",
            "- Tensorflow\n",
            "- PyTorch\n",
            "- NumPy\n",
            "- OpenCV\n",
            "- NLTK\n",
            "- Matplotlib\n",
            "\n",
            "Years Spent in Different Jobs:\n",
            "- Machine Learning Engineer Graduate at Zaka AI: Jan 2023 - May 2023\n",
            "- Event Coordinator at Strategic Holding: Nov 2021 - Dec 2021\n",
            "\n",
            "Years Spent in Current Organization: N/A (No information given)\n",
            "Name of Present Organization: N/A (No information given)\n",
            "\n",
            "Summary:\n",
            "Highly motivated Computer Engineering graduate with a strong passion for Data Science and AI. Currently expanding AI knowledge through workshops and certifications. Equipped with a solid foundation in Python programming. Demonstrated commitment to continuous learning and professional growth.\n",
            "[Resume Summary]  Candidate Name: Pankaj Kumar Goyal \n",
            "Mobile Number: Not provided \n",
            "Email ID: pankajgoyal02003@gmail.com \n",
            "Total Years of Experience: Not mentioned \n",
            "Last Education Degree: B.Tech in Electronics and Communication Engineering \n",
            "Last University Attended: Indian Institute of Information Technology, Allahabad \n",
            "LinkedIn Profile: Not provided \n",
            "\n",
            "Technical Skills:\n",
            "- Python, Machine learning, Computer Vision, Deep learning, Data Cleaning, Feature engineering, Data Analysis, Data Science\n",
            "- Natural Language Processing\n",
            "- Large language models (BERT, Roberta, XLM-R, T5, Distil-BERT)\n",
            "- Prompt Engineering, Generative AI, LangChain, pinecone (vector databases), chatbot development\n",
            "- Numpy, Pandas, Scikit Learn, TensorFlow, Keras, Seaborn, Matplotlib\n",
            "- Database: SQL, MYSQL, PosgreySQL\n",
            "- Cloud: AWS, Vertex AI (AUTOML, CustomML), MLOPS (MLflow)\n",
            "- BI tools: PowerBI\n",
            "\n",
            "Years Spent in Different Jobs: Not mentioned\n",
            "Years Spent in Current Organization: Not mentioned\n",
            "Present Organization: Not mentioned\n",
            "Summary: Hands-on experience in Data Science and Machine Learning with expertise in Natural Language Processing, prompt engineering, deep learning, and computer vision. Strong skills in Python, machine learning, computer vision, and data analysis. Experience working with large language models. Achievements include participating in Hack-out 2022 and completing Intro to deep learning course provided by Kaggle. Deployed projects in NLP on HuggingFace.\n",
            "[Resume Summary]  Candidate Name: Joseph Adeola\n",
            "Mobile Number: +34 610-423-930\n",
            "Email ID: adeola.jo@outlook.com\n",
            "Last Education Degree: Erasmus Mundus Masters in Intelligent Field Robotic Systems\n",
            "Last University Attended: Universitat De Girona, Spain\n",
            "Linkedin Profile: linkedin.com/in/adeola-joseph\n",
            "Total Years of Experience: 3 years\n",
            "Present Organization: Computer Vision and Robotics Research Institute, Universitat De Girona\n",
            "Years in Present Organization: Jun 2023 - Present\n",
            "Summary: Graduate student specializing in Intelligent Robotics with experience in software development. Proficient in robotics, machine learning, computer vision, programming, control, and motion planning. Seeking a Master's thesis research position in Multi-robot Systems to contribute innovative research and create a significant impact.\n",
            "Technical Skills: Python, Pandas, Numpy, Matplotlib, PyTorch, TensorFlow, Keras, Scikit-Learn, Kivy, Matlab, C++, R, C, OMPL, PDDL, Robot Operating System (ROS), Git, OpenCV, Linux OS, MS Office Suites\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output:\n",
        "\n",
        "\n",
        "```\n",
        "[Resume Summary]  Candidate Name: Pankaj Kumar Goyal\n",
        "Mobile Number: Not mentioned\n",
        "Email ID: pankajgoyal02003@gmail.com\n",
        "Total Years of Experience: Not mentioned\n",
        "\n",
        "Last Education Degree: B.Tech in Electronics and Communication Engineering\n",
        "Last University Attended: Indian Institute of Information Technology, Allahabad\n",
        "\n",
        "LinkedIn Profile: https://www.linkedin.com/in/pankaj10032\n",
        "\n",
        "Technical Skills:\n",
        "- Python\n",
        "- Machine Learning\n",
        "- Computer Vision\n",
        "- Deep Learning\n",
        "- Data Cleaning\n",
        "- Feature Engineering\n",
        "- Data Analysis\n",
        "- Data Science\n",
        "- Natural Language Processing\n",
        "- Large language models (BERT, Roberta, XLM-R, T5, Distil-BERT)\n",
        "- Prompt Engineering\n",
        "- Generative AI\n",
        "- LangChain\n",
        "- Pinecone (vector databases)\n",
        "- Chatbot Development\n",
        "- Numpy\n",
        "- Pandas\n",
        "- Scikit Learn\n",
        "- TensorFlow\n",
        "- Keras\n",
        "- Seaborn\n",
        "- Matplotlib\n",
        "- SQL\n",
        "- MySQL\n",
        "- PosgreySQL\n",
        "- AWS\n",
        "- Vertex AI (AUTOML, CustomML)\n",
        "- MLOPS (MLflow)\n",
        "- PowerBI\n",
        "\n",
        "Years Spent in Different Jobs: Not mentioned\n",
        "Years Spent in Current Organization: Not mentioned\n",
        "Name of Present Organization: Not mentioned\n",
        "\n",
        "Summary: The resume highlights the candidate's hands-on experience in data science and machine learning, with expertise in natural language processing, prompt engineering, deep learning, and computer vision. The candidate has worked on projects related to cross-lingual and multilingual language modeling, Twitter hate speech detection, skin cancer classification, and data extraction in NLP. They possess strong technical skills in Python, machine learning, computer vision, deep learning, and data analysis. The candidate has completed a B.Tech degree in Electronics and Communication Engineering from the Indian Institute of Information Technology, Allahabad. They have also participated in a hackathon and completed a course on deep learning provided by Kaggle.\n",
        "[Resume Summary]  Candidate Name: Abhilash Babu\n",
        "Mobile Number: +49 17647165848\n",
        "Email ID: abhilashbabuj@gmail.com\n",
        "Total Years of Experience: 18 years\n",
        "Last Education Degree: MS in Communication Engineering\n",
        "Last University Attended: Technische Universität, München, Germany\n",
        "LinkedIn Profile: https://www.linkedin.com/in/abhilashbabu\n",
        "Technical Skills:\n",
        "- Languages: C++, C, C#, Python\n",
        "- Computer Vision: OpenCV, Halcon Machine vision library\n",
        "- Machine Learning: Tensorflow, PyTorch, PyTorch-Lightning, Scikit-Learn, Pandas, Keras, ONNX, ApacheTVM, MLFlow, Optuna\n",
        "- Database: MySQL, SQLite\n",
        "- Libraries: Boost, ZeroMQ, Protocol Buffer, gRPC, MQTT, RabbitMQ\n",
        "- GUI Frameworks: Qt, WPF, DearImGUI\n",
        "- Testing frameworks: pytest, GoogleTest, Catch2\n",
        "- Miscellaneous: Docker, Jenkins, Bamboo, Jupyter Notebooks\n",
        "\n",
        "Experience:\n",
        "- Apr 2022 - Present: Senior Machine Learning Engineer at IDnow GmbH, München\n",
        "- Jan 2020 - Feb 2022: Senior Developer Vision Systems at Bundesdruckerei GmbH, München\n",
        "- Aug 2016 - Dec 2019: Developer Vision Systems at Bundesdruckerei GmbH, München\n",
        "- Jan 2013 - Jul 2016: Software Development Engineer at Stratus Vision GmbH, München\n",
        "- Aug 2011 - Oct 2011: Praktikant at Rohde & Schwarz, Berlin, Germany\n",
        "- Apr 2008 - Sep 2010: Lead Engineer at Samsung India Software Operations, Bangalore, India\n",
        "- Nov 2005 - Apr 2008: Software Engineer at Wipro Technologies, Bangalore, India\n",
        "\n",
        "Summary:\n",
        "Senior Machine Learning Engineer with 18 years of experience in successfully delivering projects in the domain of Computer Vision and Image processing. Deep understanding of classical computer vision techniques as well as the latest advancements in deep learning frameworks. Experienced in developing and deploying machine learning solutions for various applications. Strong expertise in languages like C++, C, C#, and Python. Certified Software Architect and Certified Scrum Product Owner. Proficient in mentoring junior colleagues and interns.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "8VIRV_9ZO2eR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Version 2:\n",
        "\n",
        "### *Extract key details from the resume and represent the output in JSON format.*\n",
        "### *The output in the above is in plain text, so the below prompt narrows down the desired format of the output. JSON is a commonly used data interchange format and provides structured data which can be easily parsed and utilized.*\n"
      ],
      "metadata": {
        "id": "jq2d_i4akcl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "prompt_v2=f'''Read the given resume and extract information such as the candidate name, mobile number, email_id, total years of experience, the candidate's last education degree \\\n",
        "last university attended by the candidate University, extract the candidate's linkedin profile, record all the technical skills, years spent in different jobs,\n",
        "years spent in the current organization, name of the present organization and the summary. The final output must be in JSON'''\n",
        "\n",
        "for application in selected_applications:\n",
        "    if 'resume_path' in application and 'email_id' in application:\n",
        "        resume_text = read_document(os.path.join(resume_path, application['resume_path']))\n",
        "        resume_text, _, _ = check_and_trim(resume_text)\n",
        "        resume_summary = summarize_resume(prompt_v2, resume_text)\n",
        "        print(\"[Resume Summary] \", resume_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PIblsLWiEt2",
        "outputId": "c45a493f-c5b2-4e52-8e1f-1e78ed9f17f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Resume Summary]  {\n",
            "  \"Candidate Name\": \"Paula Ramos\",\n",
            "  \"Mobile Number\": \"919-786-3615\",\n",
            "  \"Email\": \"pjramg@gmail.com\",\n",
            "  \"Address\": \"3357 Bordwell Ridge Drive, New Hill, NC, 27562\",\n",
            "  \"LinkedIn\": \"linkedin.com/in/paula-ramos-41097319\",\n",
            "  \"Total Experience\": \"10 years\",\n",
            "  \"Last Education Degree\": \"Ph.D. in Engineering (Computer Science & Image Processing)\",\n",
            "  \"Last University Attended\": \"Universidad Nacional de Colombia\",\n",
            "  \"Present Organization\": \"Intel Corporation\",\n",
            "  \"Years in Current Organization\": \"1 year\",\n",
            "  \"Summary\": \"Research in new AI technologies based on image (2D - 3D) and signal processing, control and automation, robotics, machine learning, embedded systems, and mobile devices.\",\n",
            "  \"Technical Skills\": [\n",
            "    \"Computer Vision\",\n",
            "    \"Machine Learning\",\n",
            "    \"AI Software Development\",\n",
            "    \"Image Processing\",\n",
            "    \"Signal Processing\",\n",
            "    \"Control Systems\",\n",
            "    \"Automation\",\n",
            "    \"Robotics\",\n",
            "    \"Embedded Systems\",\n",
            "    \"Mobile Devices\",\n",
            "    \"IoT\",\n",
            "    \"Deep Learning\",\n",
            "    \"Precision Agriculture\",\n",
            "    \"Statistical Modeling\"\n",
            "  ],\n",
            "  \"Experience\": [\n",
            "    {\n",
            "      \"Job Title\": \"AI Software Development Engineer\",\n",
            "      \"Company\": \"Intel Corporation\",\n",
            "      \"Duration\": \"Nov 2021 - On going\",\n",
            "      \"Location\": \"Raleigh, NC\",\n",
            "      \"Description\": \"Build and foster developer communities around Intel AI Inference software. Create transformational initiative to build stronger direct and programmatic relationships with AI Inference ISVs and AI Inference Developers. Grow self-sustaining and scalable meetups and user groups. Shepherd the open-source community and help ensure it grows healthy and strong. Internally advocate for the needs of the developer community, working with product, marketing, and engineering teams to determine features and technology strategy. Regularly interacts directly with HQ and regional marketing teams and centers of excellence (COEs), Intel field and regional sales teams, regional ISVs, and academia, 3rd party communities associations, etc. Evangelize the development of community tools and services to accelerate the growth and engagement of developers. Work at times directly with customers and prospects to deliver executive-level demonstrations and presentations. Evangelize Intel inference software relative to the status quo/alternatives. Collaborate with internal developers/documentation team to create, contribute to, and publish technical content. Present technical sessions at conferences, workshops, and online events. Analyze the effectiveness of community advocacy and continually strategize and iterate.\"\n",
            "    },\n",
            "    {\n",
            "      \"Job Title\": \"Research Scholar\",\n",
            "      \"Company\": \"North Carolina State University - USDA ARS\",\n",
            "      \"Duration\": \"Jul 2020 - Oct 2022\",\n",
            "      \"Location\": \"Raleigh, NC\",\n",
            "      \"Description\": \"Lead and manage engineering applied to agricultural research activities for grant-funded projects designed to assess the status of North Carolina cropping systems, remote-sensing, computer vision, robotics, and AI. Improve communication systems in the field, integrate IoT for field data-driven monitoring, and develop new technologies to monitor crops in the field creating feedback control to improve crop performance through deep learning and machine learning. Work with different teams in Ag and Eng, be mentor and supervisor of graduate students and postdocs, assist in the management and conduct engineering research related to diverse project objectives.\"\n",
            "    },\n",
            "    {\n",
            "      \"Job Title\": \"Postdoctoral Researcher\",\n",
            "      \"Company\": \"North Carolina State University - USDA ARS\",\n",
            "      \"Duration\": \"Jun 2019 - Jun 2020\",\n",
            "      \"Location\": \"Raleigh, NC\",\n",
            "      \"Description\": \"Supporting efforts from the Organic Cropping Systems Lab and Sustainable Agricultural Systems Lab under Dr. Reberg-Horton and Dr. Mirsky on the development of new technologies in Precision Sustainable Agriculture project. Developing image and data processing systems, making hardware, software and analytical platforms for the analysis of the performance of different practices in the field. Generating new technology to connect farmers through IoT and Edge Computing. Detection of drought in different soybean genotypes and corn, also 3D plant mapping, through the use of computer vision and machine learning. Development of research proposals, papers, and posters.\"\n",
            "    },\n",
            "    {\n",
            "      \"Job Title\": \"Research Scholar\",\n",
            "      \"Company\": \"North Carolina State University\",\n",
            "      \"Duration\": \"Feb 2019 - Jun 2019\",\n",
            "      \"Location\": \"Raleigh, NC\",\n",
            "      \"Description\": \"Supporting efforts from the Active Robotic Sensing (ARoS) Laboratory under Dr. Lobaton on the development of Explanatory AI approaches for deep learning models for classification of microscopic fossils called foraminifera.\"\n",
            "    },\n",
            "    {\n",
            "      \"Job Title\": \"Research Scientist\",\n",
            "      \"Company\": \"National Research Center of Coffee\",\n",
            "      \"Duration\": \"Feb 2010 - Dec 2018\",\n",
            "      \"Location\": \"Caldas, COL\",\n",
            "      \"Description\": \"More than 15 years of experience in the development of new technologies for coffee growers in Colombia. Obtained resources for three large projects financed by the government. In each one of them, I obtained real products for coffee growers, scientific papers, seminars, conferences, and invention patents.\"\n",
            "    },\n",
            "    {\n",
            "      \"Job Title\": \"Doctoral Internship\",\n",
            "      \"Company\": \"Institut Valencià d’Investigacions Agraries\",\n",
            "      \"Duration\": \"Sept 2014 - Marzo 2015\",\n",
            "      \"Location\": \"Valencia, SPA\",\n",
            "      \"Description\": \"Participated actively in a project funded by the European Union, developing algorithms for field estimation based on 2D image processing algorithms and supporting electronic systems for image acquisition automatically in field conditions.\"\n",
            "    },\n",
            "    {\n",
            "      \"Job Title\": \"Junior Researcher\",\n",
            "      \"Company\": \"National Research Center of Coffee\",\n",
            "      \"Duration\": \"June 2004 - Jan 2010\",\n",
            "      \"Location\": \"Caldas, COL\",\n",
            "      \"Description\": \"Participated in multiple research projects and developed different control, identification, and classification systems based on signal and image processing and embedded systems.\"\n",
            "    },\n",
            "    {\n",
            "      \"Job Title\": \"Lecturer\",\n",
            "      \"Company\": \"Universidad Tecnológica de Pereira\",\n",
            "      \"Duration\": \"June 2006 - Dic 2011\",\n",
            "      \"Location\": \"Risaralda, COL\",\n",
            "      \"Description\": \"Lectured different courses related to digital systems and image processing. Supported the creation of a research group in electronic engineering.\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "[Resume Summary]  {\n",
            "  \"candidate_name\": \"Soso Sukhitashvili\",\n",
            "  \"mobile_number\": \"+995 598 55 65 74\",\n",
            "  \"email_id\": \"sukhitashvili.soso@gmail.com\",\n",
            "  \"total_years_of_experience\": \"5\",\n",
            "  \"last_education_degree\": \"Master in Artificial Intelligence\",\n",
            "  \"last_university_attended\": \"IU International University of Applied Sciences\",\n",
            "  \"linkedin_profile\": \"linkedin.com/in/soso-sukhitashvili\",\n",
            "  \"technical_skills\": [\n",
            "    \"Strong Programming Skills\",\n",
            "    \"Statistical Analysis and Mathematics\",\n",
            "    \"Data Science / Machine Learning / Deep Learning Research\",\n",
            "    \"Generative modeling: GANs, stable diffusion\",\n",
            "    \"Data Analysis & Cluster Analysis\",\n",
            "    \"Forecasting & Predictive Analytics\",\n",
            "    \"Python\",\n",
            "    \"Pytorch\",\n",
            "    \"OpenCV\",\n",
            "    \"Huggingface\",\n",
            "    \"Spicy\",\n",
            "    \"Data Mining & Data Visualization\",\n",
            "    \"Pattern Recognition\",\n",
            "    \"Amazon Web Services (AWS)\"\n",
            "  ],\n",
            "  \"work_experience\": [\n",
            "    {\n",
            "      \"organization\": \"Cortica AI\",\n",
            "      \"position\": \"Deep Learning Engineer / Algorithm Developer\",\n",
            "      \"start_date\": \"May 2021\",\n",
            "      \"end_date\": \"Present\",\n",
            "      \"years_spent\": \"0\"\n",
            "    },\n",
            "    {\n",
            "      \"organization\": \"MaxinAI\",\n",
            "      \"position\": \"Deep Learning Engineer / Python Developer\",\n",
            "      \"start_date\": \"Sep 2018\",\n",
            "      \"end_date\": \"May 2021\",\n",
            "      \"years_spent\": \"2.7\"\n",
            "    }\n",
            "  ],\n",
            "  \"current_organization\": \"Cortica AI\",\n",
            "  \"summary\": \"I am a machine learning engineer and algorithm developer with over 5 years of experience in the field. My expertise lies in designing and developing cutting-edge solutions for complex challenges in various domains, such as computer vision, natural language processing, and time-series analysis. I excel in swiftly adjusting to and acquiring new technologies/methods, which enables me to seamlessly blend in with a team. I seek a demanding, dynamic setting where I can continuously enhance my skills.\"\n",
            "}\n",
            "[Resume Summary]  {\n",
            "   \"Candidate Name\": \"AHMED YASSIN\",\n",
            "   \"Mobile Number\": \"+201128832884\",\n",
            "   \"Email ID\": \"ahmedyassin@skiff.com\",\n",
            "   \"Total Years of Experience\": \"Less than 1 year\",\n",
            "   \"Last Education Degree\": \"Bachelor's degree in Engineering\",\n",
            "   \"Last University Attended\": \"Al-Azhar University\",\n",
            "   \"University\": \"Al-Azhar University\",\n",
            "   \"LinkedIn Profile\": \"linkedin.com/in/yassin01\",\n",
            "   \"Technical Skills\": [\n",
            "      \"Python\",\n",
            "      \"SQL\",\n",
            "      \"Jupyter Notebook\",\n",
            "      \"Google Colab\",\n",
            "      \"Git/GitHub\",\n",
            "      \"Scikit-Learn\",\n",
            "      \"Keras\",\n",
            "      \"Tensorflow\",\n",
            "      \"PyTorch\",\n",
            "      \"NumPy\",\n",
            "      \"OpenCV\",\n",
            "      \"NLTK\",\n",
            "      \"Matplotlib\"\n",
            "   ],\n",
            "   \"Years Spent in Jobs\": [\n",
            "      {\n",
            "         \"Job Title\": \"Machine Learning Engineer Graduate\",\n",
            "         \"Company\": \"Zaka AI\",\n",
            "         \"Start Date\": \"Jan 2023\",\n",
            "         \"End Date\": \"May 2023\",\n",
            "         \"Years of Experience\": \"Less than 1 year\"\n",
            "      },\n",
            "      {\n",
            "         \"Job Title\": \"Event Coordinator\",\n",
            "         \"Company\": \"Strategic Holding\",\n",
            "         \"Start Date\": \"Nov 2021\",\n",
            "         \"End Date\": \"Dec 2021\",\n",
            "         \"Years of Experience\": \"Less than 1 year\"\n",
            "      }\n",
            "   ],\n",
            "   \"Current Organization\": \"Zaka AI\",\n",
            "   \"Years Spent in Current Organization\": \"Less than 1 year\",\n",
            "   \"Summary\": \"Highly motivated Computer Engineering graduate with a strong passion for Data Science and AI. Currently expanding AI knowledge through workshops and certifications. Equipped with a solid foundation in Python programming. Demonstrated commitment to continuous learning and professional growth.\"\n",
            "}\n",
            "[Resume Summary]  {\n",
            "  \"Candidate Name\": \"PANKAJ KUMAR GOYAL\",\n",
            "  \"Mobile Number\": \"\",\n",
            "  \"Email ID\": \"pankajgoyal02003@gmail.com\",\n",
            "  \"Total Years of Experience\": null,\n",
            "  \"Last Education Degree\": \"B.Tech in Electronics and Communication Engineering\",\n",
            "  \"Last University Attended\": \"Indian Institute of Information Technology, Allahabad\",\n",
            "  \"LinkedIn Profile\": \"\",\n",
            "  \"Technical Skills\": [\n",
            "    \"Python\",\n",
            "    \"Machine Learning\",\n",
            "    \"Computer Vision\",\n",
            "    \"Deep Learning\",\n",
            "    \"Data Cleaning\",\n",
            "    \"Feature Engineering\",\n",
            "    \"Data Analysis\",\n",
            "    \"Data Science\",\n",
            "    \"Natural Language Processing\",\n",
            "    \"Large Language Models (BERT, Roberta, XLM-R, T5, Distil-BERT)\",\n",
            "    \"Prompt Engineering\",\n",
            "    \"Generative AI\",\n",
            "    \"LangChain\",\n",
            "    \"Pinecone (Vector Databases)\",\n",
            "    \"Chatbot Development\",\n",
            "    \"Numpy\",\n",
            "    \"Pandas\",\n",
            "    \"Scikit Learn\",\n",
            "    \"TensorFlow\",\n",
            "    \"Keras\",\n",
            "    \"Seaborn\",\n",
            "    \"Matplotlib\",\n",
            "    \"SQL\",\n",
            "    \"MYSQL\",\n",
            "    \"Posgresql\",\n",
            "    \"AWS\",\n",
            "    \"Vertex AI (AUTOML, CustomML)\",\n",
            "    \"MLOPS (MLflow)\",\n",
            "    \"PowerBI\"\n",
            "  ],\n",
            "  \"Years Spent in Different Jobs\": [\n",
            "    {\n",
            "      \"Job Title\": \"Cross Lingual and Multilingual Language-Modeling\",\n",
            "      \"Duration\": \"July 2023\"\n",
            "    },\n",
            "    {\n",
            "      \"Job Title\": \"Twitter Hate Speech Detection\",\n",
            "      \"Duration\": \"June 2023\"\n",
            "    },\n",
            "    {\n",
            "      \"Job Title\": \"Skin Cancer MNIST: HAM10000\",\n",
            "      \"Duration\": \"May 2023\"\n",
            "    },\n",
            "    {\n",
            "      \"Job Title\": \"Data Extraction In the NLP\",\n",
            "      \"Duration\": \"August 2023\"\n",
            "    }\n",
            "  ],\n",
            "  \"Years Spent in Current Organization\": null,\n",
            "  \"Present Organization\": \"\",\n",
            "  \"Summary\": \"Hands-on experience in Data Science and Machine learning with expertise in Natural Language Processing, Prompt Engineering, Deep Learning, and Computer Vision\"\n",
            "}\n",
            "[Resume Summary]  {\n",
            "  \"Name\": \"Joseph Adeola\",\n",
            "  \"Mobile Number\": \"+34 610-423-930\",\n",
            "  \"Email ID\": \"adeola.jo@outlook.com\",\n",
            "  \"LinkedIn Profile\": \"linkedin.com/in/adeola-joseph\",\n",
            "  \"Education\": {\n",
            "    \"Last Degree\": \"Erasmus Mundus Masters in Intelligent Field Robotic Systems\",\n",
            "    \"Last University\": \"Universitat De Girona, Spain\"\n",
            "  },\n",
            "  \"Technical Skills\": [\n",
            "    \"Python\",\n",
            "    \"Pandas\",\n",
            "    \"Numpy\",\n",
            "    \"Matplotlib\",\n",
            "    \"PyTorch\",\n",
            "    \"TensorFlow\",\n",
            "    \"Keras\",\n",
            "    \"Scikit-Learn\",\n",
            "    \"Kivy\",\n",
            "    \"Matlab\",\n",
            "    \"C++\",\n",
            "    \"R\",\n",
            "    \"C\",\n",
            "    \"OMPL\",\n",
            "    \"PDDL\",\n",
            "    \"Robot Operating System (ROS)\",\n",
            "    \"Git\",\n",
            "    \"OpenCV\",\n",
            "    \"Linux OS\",\n",
            "    \"MS Office Suites\"\n",
            "  ],\n",
            "  \"Total Years of Experience\": 3,\n",
            "  \"Experience\": [\n",
            "    {\n",
            "      \"Job Title\": \"Deep Learning Research Intern\",\n",
            "      \"Duration\": \"Jun 2023 - Present\",\n",
            "      \"Organization\": \"Computer Vision and Robotics Research Institute, Universitat De Girona\",\n",
            "      \"Location\": \"Girona, Spain\"\n",
            "    },\n",
            "    {\n",
            "      \"Job Title\": \"Financial Risk Management Intern\",\n",
            "      \"Duration\": \"Feb 2022 - Sept 2022\",\n",
            "      \"Organization\": \"KPMG Professional Services\",\n",
            "      \"Location\": \"Lagos, Nigeria\"\n",
            "    },\n",
            "    {\n",
            "      \"Job Title\": \"Graduate Research Assistant\",\n",
            "      \"Duration\": \"June 2021 - Nov 2021\",\n",
            "      \"Organization\": \"University of Ilorin\",\n",
            "      \"Location\": \"Ilorin, Nigeria\"\n",
            "    }\n",
            "  ],\n",
            "  \"Years in Current Organization\": \"0\",\n",
            "  \"Current Organization\": \"\",\n",
            "  \"Summary\": \"Graduate student specializing in Intelligent Robotics, university valedictorian, and bronze medalist at the International Youth Mathematics Challenge. Proficient in robotics, machine learning, computer vision, programming, control, and motion planning, with 3 years experience in software development. Passionate about continual learning and seeking a Master’s thesis research position in Multi-robot Systems. Committed to delivering innovative research contributions and creating a significant impact on the organization’s success.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output:\n",
        "\n",
        "\n",
        "```\n",
        "[Resume Summary]  {\n",
        "  \"Candidate Name\": \"Abhijeet Dhupia\",\n",
        "  \"Mobile Number\": \"+919901656836\",\n",
        "  \"Email ID\": \"abhijeetdhupia@gmail.com\",\n",
        "  \"Total Years of Experience\": \"2\",\n",
        "  \"Last Education Degree\": \"B.Tech. in Electrical and Electronics Engineering (EEE)\",\n",
        "  \"Last University Attended\": \"Manipal Institute of Technology\",\n",
        "  \"University\": \"Manipal, India\",\n",
        "  \"LinkedIn Profile\": \"abhijeetdhupia\",\n",
        "  \"Technical Skills\": [\n",
        "    \"Python\",\n",
        "    \"C++\",\n",
        "    \"CSS\",\n",
        "    \"HTML5\",\n",
        "    \"ImageJ\",\n",
        "    \"LATEX\",\n",
        "    \"MATLAB\",\n",
        "    \"Markdown\",\n",
        "    \"R\",\n",
        "    \"Shell Scripting\",\n",
        "    \"Vim\",\n",
        "    \"Pytorch\",\n",
        "    \"TensorFlow\",\n",
        "    \"AWS\",\n",
        "    \"Docker\",\n",
        "    \"Flask\",\n",
        "    \"Git\",\n",
        "    \"Jira\",\n",
        "    \"OpenCV\"\n",
        "  ],\n",
        "  \"Years in Different Jobs\": [\n",
        "    {\n",
        "      \"Job Title\": \"Research Assistant\",\n",
        "      \"Organization\": \"Spectrum Lab, Indian Institute of Science\",\n",
        "      \"Years of Experience\": \"2\"\n",
        "    },\n",
        "    {\n",
        "      \"Job Title\": \"Research Intern\",\n",
        "      \"Organization\": \"QpiAI (in collaboration with IISc)\",\n",
        "      \"Years of Experience\": \"0.5\"\n",
        "    }\n",
        "  ],\n",
        "  \"Years in Current Organization\": \"2\",\n",
        "  \"Present Organization\": \"Spectrum Lab, Indian Institute of Science\",\n",
        "  \"Summary\": \"Experienced research assistant with a focus on data science and deep learning. Strong background in developing algorithms for medical imaging and healthcare applications. Skilled in Python, Pytorch, and OpenCV. Completed hands-on training in deep learning and data science specializations. Passionate about leveraging AI for impactful healthcare solutions.\"\n",
        "}\n",
        "[Resume Summary]  {\n",
        "  \"Name\": \"Laveena Satwani\",\n",
        "  \"Mobile Number\": \"+91 8989035197\",\n",
        "  \"Email\": \"laveenasatwani52483@gmail.com\",\n",
        "  \"Total Years of Experience\": \"2 years and 2 months\",\n",
        "  \"Last Education Degree\": \"Bachelor of Technology in Computer Science & Engineering\",\n",
        "  \"Last University Attended\": \"Indian Institute of Information Technology Jabalpur, India\",\n",
        "  \"LinkedIn Profile\": \"linkedin.com/in/laveena-satwani-189970153\",\n",
        "  \"Technical Skills\": [\n",
        "    \"Computer Vision\",\n",
        "    \"Machine Learning\",\n",
        "    \"Image Processing\",\n",
        "    \"Deep Learning\",\n",
        "    \"Python\",\n",
        "    \"TensorFlow\",\n",
        "    \"Matlab\",\n",
        "    \"SpringBoot\",\n",
        "    \"scikit-learn\",\n",
        "    \"AngularJS\"\n",
        "  ],\n",
        "  \"Years Spent in Different Jobs\": [\n",
        "    {\n",
        "      \"Job Title\": \"Computer Vision Engineer\",\n",
        "      \"Organization\": \"BigVision LLC\",\n",
        "      \"Years Spent\": \"0 years and 9 months\"\n",
        "    },\n",
        "    {\n",
        "      \"Job Title\": \"Machine Learning Engineer\",\n",
        "      \"Organization\": \"Vassar Labs IT Solutions\",\n",
        "      \"Years Spent\": \"1 year and 1 month\"\n",
        "    },\n",
        "    {\n",
        "      \"Job Title\": \"Software Engineer\",\n",
        "      \"Organization\": \"Vassar Labs IT Solutions\",\n",
        "      \"Years Spent\": \"0 years and 3 months\"\n",
        "    },\n",
        "    {\n",
        "      \"Job Title\": \"Machine Learning Intern\",\n",
        "      \"Organization\": \"Vassar Labs IT Solutions\",\n",
        "      \"Years Spent\": \"0 years and 6 months\"\n",
        "    }\n",
        "  ],\n",
        "  \"Years Spent in Current Organization\": \"0 years and 2 months\",\n",
        "  \"Current Organization\": \"BigVision LLC\",\n",
        "  \"Summary\": \"Experienced Computer Vision Engineer and Machine Learning Engineer with a demonstrated history of working on various projects in the field of image enhancement, car segmentation, product detection, and satellite image analysis. Skilled in Computer Vision, Machine Learning, Deep Learning, and Image Processing. Strong engineering professional with a Bachelor of Technology (B.Tech.) focused in Computer Science & Engineering from Indian Institute of Information Technology Jabalpur.\"\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "b3EkguvjR57r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Version 3:\n",
        "### *Extract key details from the resume and represent the output in JSON format.*\n",
        "\n",
        "### *This is a refined and more explicit prompt to ensure consistency in the output format. By mentioning the exact keys and giving examples for each key, the aim is to guide the model towards a structured and expected output. Additionally, this prompt provides detailed instructions on how certain fields should be represented, ensuring that the output aligns closely with the desired format.*"
      ],
      "metadata": {
        "id": "a7wLBVZCkfUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "prompt_v3=f'''Read the given resume and extract information corresponding to the keys\\\n",
        " \"name_of_candidate\" which stores the candidate name, \\\n",
        " \"mobile_number\" contains the mobile number, \\\n",
        " \"email_id\" records the email id of the candidate, \\\n",
        " total years of experience is stored in \"years_of_experience\", \\\n",
        " \"education\" refers to the candidate's most recent or highest academic degree, \\\n",
        " last university/school/college attended by the candidate is given by \"university\", \\\n",
        " \"linkedin_profile\" contains the linkedin profile, \\\n",
        " record all the technical skills in \"technical_skills\", \\\n",
        " \"years_of_jobs\" showcases the years spent in different jobs, \\\n",
        " years spent in the current organization is given by \"year_in_current_position\", \\\n",
        " \"Present_Organization\" denotes name of the present organization and \"summay\". \\\n",
        " For \"technical_skills\", provide a summary of the programming languages, libraries, and frameworks the candidate has experience with as a list, \\\n",
        " \"years_of_jobs\" is a list of job durations, e.g., [\"2012-current\",\"2010-2012\", (June 22, 2022 - Present)]. \\\n",
        " \"year_in_current_position\" indicates the duration in their current job role integer. Present year is 2023. \\\n",
        " \"years_of_experience\" is the sum of years spent in all jobs including the current one. \\\n",
        " Round off the year to the upper ceiling. So, if it is 3 months, round it off to 1 year. \\\n",
        " Summarize the resume in approximately 100 words for the \"summary\" field. \\\n",
        " The final output must be in JSON\n",
        "'''\n",
        "\n",
        "gpt_response_list = []\n",
        "for application in selected_applications:\n",
        "    if 'resume_path' in application and 'email_id' in application:\n",
        "        resume_text = read_document(os.path.join(resume_path, application['resume_path']))\n",
        "        resume_text, _, _ = check_and_trim(resume_text)\n",
        "        resume_summary = summarize_resume(prompt_v3, resume_text)\n",
        "        gpt_response_list.append(resume_summary)\n",
        "        print(\"[Resume Summary] \", resume_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lvlR79Wi6mH",
        "outputId": "913e857c-da45-4264-dcae-02fa46d5c0f9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Resume Summary]  {\n",
            "  \"name_of_candidate\": \"PAULA RAMOS\",\n",
            "  \"mobile_number\": \"919-786-3615\",\n",
            "  \"email_id\": \"pjramg@gmail.com\",\n",
            "  \"years_of_experience\": 15,\n",
            "  \"education\": \"Ph.D. in Engineering (Computer Science & Image Processing)\",\n",
            "  \"university\": \"Universidad Nacional de Colombia\",\n",
            "  \"linkedin_profile\": \"linkedin.com/in/paula-ramos-41097319\",\n",
            "  \"technical_skills\": [\"Computer Vision\", \"Machine Learning\", \"Signal Processing\", \"Control Systems\", \"Automation\", \"Robotics\", \"Embedded Systems\", \"Mobile Devices\"],\n",
            "  \"years_of_jobs\": [\"2012-2023\", \"2010-2012\", \"June 22, 2022 - Present\"],\n",
            "  \"year_in_current_position\": 1,\n",
            "  \"Present_Organization\": \"Intel Corporation\",\n",
            "  \"summary\": \"Paula Ramos is a Computer Vision and Machine Learning expert with 15 years of experience in research and development. She holds a Ph.D. in Engineering, specializing in Computer Science and Image Processing. Paula has a strong background in signal processing, control systems, automation, robotics, embedded systems, and mobile devices. She has worked as an AI Software Development Engineer at Intel Corporation, leading initiatives to build developer communities and foster relationships with AI Inference ISVs and developers. Paula has also conducted research in agricultural engineering, remote sensing, computer vision, and AI at North Carolina State University. She is skilled in various programming languages, libraries, and frameworks and has presented her work at conferences and workshops. With a passion for innovation and technology, Paula is committed to advancing the field of AI and making a positive impact through her expertise.\"\n",
            "}\n",
            "[Resume Summary]  {\n",
            "  \"name_of_candidate\": \"Soso Sukhitashvili\",\n",
            "  \"mobile_number\": \"+995 598 55 65 74\",\n",
            "  \"email_id\": \"sukhitashvili.soso@gmail.com\",\n",
            "  \"years_of_experience\": 6,\n",
            "  \"education\": \"Master in Artificial Intelligence\",\n",
            "  \"university\": \"IU International University of Applied Sciences\",\n",
            "  \"linkedin_profile\": \"linkedin.com/in/soso-sukhitashvili\",\n",
            "  \"technical_skills\": {\n",
            "    \"programming_languages\": [\"Python\"],\n",
            "    \"libraries\": [\"Pytorch\", \"OpenCV\", \"Huggingface\"],\n",
            "    \"frameworks\": [\"Spicy\"],\n",
            "    \"others\": [\"Data Mining & Data Visualization\", \"Amazon Web Services (AWS)\"]\n",
            "  },\n",
            "  \"years_of_jobs\": [\"2018-2023\"],\n",
            "  \"year_in_current_position\": 2,\n",
            "  \"present_organization\": \"Cortica AI\",\n",
            "  \"summary\": \"Soso Sukhitashvili is a machine learning engineer and algorithm developer with over 6 years of experience. He has a Master's degree in Artificial Intelligence and has expertise in various domains such as computer vision, natural language processing, and time-series analysis. Soso has contributed to increasing the accuracy of AI models and reducing processing speed in his previous roles. He is skilled in programming languages like Python and libraries like Pytorch, OpenCV, and Huggingface. Soso has a strong background in data science, statistical analysis, and mathematics. He is a highly analytical and innovative professional with excellent communication and problem-solving skills.\"\n",
            "}\n",
            "[Resume Summary]  {\n",
            "  \"name_of_candidate\": \"AHMED YASSIN\",\n",
            "  \"mobile_number\": \"+201128832884\",\n",
            "  \"email_id\": \"ahmedyassin@skiff.com\",\n",
            "  \"years_of_experience\": 2,\n",
            "  \"education\": \"Bachelor's degree in Engineering specializing in Computers and Systems Engineering\",\n",
            "  \"university\": \"Al-Azhar University Cairo, Egypt\",\n",
            "  \"linkedin_profile\": \"linkedin.com/in/yassin01\",\n",
            "  \"technical_skills\": [\"Python\", \"SQL\", \"Jupyter Notebook\", \"Google Colab\", \"Git/GitHub\", \"Scikit-Learn\", \"Keras\", \"Tensorflow\", \"PyTorch\", \"NumPy\", \"OpenCV\", \"NLTK\", \"Matplotlib\"],\n",
            "  \"years_of_jobs\": [\"Jan 2023 - May 2023\", \"Nov 2022 - Dec 2022\"],\n",
            "  \"year_in_current_position\": 0,\n",
            "  \"Present_Organization\": \"Zaka AI\",\n",
            "  \"summary\": \"AHMED YASSIN is a highly motivated Computer Engineering graduate with a strong passion for Data Science and AI. With a solid foundation in Python programming, AHMED aims to excel in the field of Machine Learning. He has completed certification programs in AI and has hands-on experience with end-to-end Data Science projects, machine learning algorithms, and deep learning techniques. AHMED is proficient in technical skills, including Python, SQL, Git/GitHub, and various libraries and frameworks. His strong interpersonal and problem-solving skills make him an asset to any team he works with. AHMED is eager to contribute to impactful data science projects and make a difference in the field.\"\n",
            "}\n",
            "[Resume Summary]  {\n",
            "  \"name_of_candidate\": \"PANKAJ KUMAR GOYAL\",\n",
            "  \"mobile_number\": \"pankaj10032\",\n",
            "  \"email_id\": \"pankajgoyal02003@gmail.com\",\n",
            "  \"education\": \"B.Tech in Electronics and Communication Engineering\",\n",
            "  \"university\": \"Indian Institute of Information Technology, Allahabad\",\n",
            "  \"linkedin_profile\": \"pankaj_Goyal\",\n",
            "  \"technical_skills\": [\"Python\", \"Machine learning\", \"Computer Vision\", \"Deep learning\", \"Data Cleaning\", \"Feature engineering\", \"Data Analysis\", \"Data Science\", \"Natural Language Processing\", \"Large Language Models (BERT, Roberta, XLM-R, T5, Distil-BERT)\", \"Prompt Engineering\", \"Generative AI\", \"LangChain\", \"pinecone (vector databases)\", \"chatbot development\", \"Numpy\", \"Pandas\", \"Scikit Learn\", \"TensorFlow\", \"Keras\", \"Seaborn\", \"Matplotlib\", \"SQL\", \"MYSQL\", \"PosgreySQL\", \"AWS\", \"Vertex AI (AUTOML, CustomML)\", \"MLOPS (MLflow)\", \"PowerBI\"],\n",
            "  \"years_of_experience\": 1,\n",
            "  \"year_in_current_position\": 1,\n",
            "  \"Present_Organization\": \"Not Available\",\n",
            "  \"years_of_jobs\": [\"2021-2023\"],\n",
            "  \"summary\": \"PANKAJ KUMAR GOYAL is a Data Scientist with expertise in Natural Language Processing, Machine Learning, Computer Vision, and Deep Learning. They have experience with Python, data cleaning, feature engineering, data analysis, and data science. They are skilled in working with large language models like BERT, Roberta, XLM-R, T5, and Distil-BERT. Additionally, they have experience in prompt engineering, generative AI, LangChain, and chatbot development. They are proficient in using libraries and frameworks such as Numpy, Pandas, Scikit Learn, TensorFlow, Keras, Seaborn, Matplotlib, SQL, and AWS. They hold a B.Tech degree in Electronics and Communication Engineering from the Indian Institute of Information Technology, Allahabad. They have 1 year of total experience, with 1 year in their current position.\"\n",
            "}\n",
            "[Resume Summary]  {\n",
            "  \"name_of_candidate\": \"Joseph Adeola\",\n",
            "  \"mobile_number\": \"+34 610-423-930\",\n",
            "  \"email_id\": \"adeola.jo@outlook.com\",\n",
            "  \"years_of_experience\": 3,\n",
            "  \"education\": \"Erasmus Mundus Masters in Intelligent Field Robotic Systems\",\n",
            "  \"university\": \"Universitat De Girona, Spain\",\n",
            "  \"linkedin_profile\": \"linkedin.com/in/adeola-joseph\",\n",
            "  \"technical_skills\": [\"Python\", \"Pandas\", \"Numpy\", \"Matplotlib\", \"PyTorch\", \"TensorFlow\", \"Keras\", \"Scikit-Learn\", \"Kivy\", \"Matlab\", \"C++\", \"R\", \"C\", \"OMPL\", \"PDDL\", \"Robot Operating System (ROS)\", \"Git\", \"OpenCV\", \"Linux OS\", \"MS Office Suites\"],\n",
            "  \"years_of_jobs\": [\"Jun 2023 - Present\", \"Feb 2022 - Sept 2022\", \"June 2021 - Nov 2021\"],\n",
            "  \"year_in_current_position\": 0,\n",
            "  \"Present_Organization\": \"Computer Vision and Robotics Research Institute, Universitat De Girona\",\n",
            "  \"summary\": \"Joseph Adeola is a graduate student specializing in Intelligent Robotics with 3 years of experience in software development. He is skilled in robotics, machine learning, computer vision, programming, control, and motion planning. Joseph is currently working as a Deep Learning Research Intern at the Computer Vision and Robotics Research Institute, Universitat De Girona. He has also interned at KPMG Professional Services as a Financial Risk Management Intern and worked as a Graduate Research Assistant at the University of Ilorin. Joseph's technical skills include Python, Pandas, Numpy, Matplotlib, PyTorch, TensorFlow, Keras, Scikit-Learn, Kivy, Matlab, C++, R, C, OMPL, PDDL, Robot Operating System (ROS), Git, OpenCV, Linux OS, and MS Office Suites.\",\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output:\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "[Resume Summary]  {\n",
        "  \"name_of_candidate\": \"Pankaj Kumar Goyal\",\n",
        "  \"mobile_number\": \"\",\n",
        "  \"email_id\": \"pankajgoyal02003@gmail.com\",\n",
        "  \"years_of_experience\": 1,\n",
        "  \"education\": \"B.Tech in Electronics and Communication Engineering\",\n",
        "  \"university\": \"Indian Institute of Information Technology, Allahabad\",\n",
        "  \"linkedin_profile\": \"\",\n",
        "  \"technical_skills\": \"Python, Machine learning, Computer Vision, Deep learning, Data Cleaning, Feature engineering, Data Analysis, Data Science, Natural Language Processing, Large language models(BERT, Roberta, XLM-R, T5, Distil-BERT), Prompt Engineering, Generative AI, LangChain, pinecone(vector databases), chatbot development, Numpy, Pandas, Scikit Learn, TensorFlow, Keras, Seaborn, Matplotlib, Database: SQL, MYSQL, PosgreySQL, Cloud: AWS, Vertex AI(AUTOML, CustomML), MLOPS(MLflow), BI tools: PowerBI\",\n",
        "  \"years_of_jobs\": [\"2021-Present\"],\n",
        "  \"year_in_current_position\": 1,\n",
        "  \"present_organization\": \"\",\n",
        "  \"summary\": \"Pankaj Kumar Goyal is a Data Scientist with expertise in Data Science and Machine Learning. He has hands-on experience in Natural Language Processing, Prompt Engineering, Deep Learning, and Computer Vision. Pankaj has completed a B.Tech in Electronics and Communication Engineering from the Indian Institute of Information Technology, Allahabad. He is skilled in Python, Machine Learning, Computer Vision, Deep Learning, Data Cleaning, Feature Engineering, and Data Analysis. Pankaj has worked on various projects in the fields of Cross-Lingual and Multilingual Language Modeling, Twitter Hate Speech Detection, Skin Cancer MNIST, and Data Extraction in NLP. He has a strong knowledge of different programming languages, libraries, frameworks, databases, and cloud technologies. Pankaj has achieved several accomplishments, including participation in Hack-out 2022 and completing an Intro to Deep Learning course provided by Kaggle. He has also successfully deployed NLP projects on HuggingFace.\"\n",
        "}\n",
        "[Resume Summary]  {\n",
        "  \"name_of_candidate\": \"Abhilash Babu\",\n",
        "  \"mobile_number\": \"+49 17647165848\",\n",
        "  \"email_id\": \"abhilashbabuj@gmail.com\",\n",
        "  \"years_of_experience\": 18,\n",
        "  \"education\": \"MS in Communication Engineering\",\n",
        "  \"university\": \"Technische Universität, München\",\n",
        "  \"linkedin_profile\": \"\",\n",
        "  \"technical_skills\": [\n",
        "    \"C++\",\n",
        "    \"C\",\n",
        "    \"C#\",\n",
        "    \"Python\",\n",
        "    \"OpenCV\",\n",
        "    \"Halcon Machine vision library\",\n",
        "    \"Tensorflow\",\n",
        "    \"PyTorch\",\n",
        "    \"PyTorch-Lightning\",\n",
        "    \"Scikit-Learn\",\n",
        "    \"Pandas\",\n",
        "    \"Keras\",\n",
        "    \"ONNX\",\n",
        "    \"ApacheTVM\",\n",
        "    \"MLFlow\",\n",
        "    \"Optuna\",\n",
        "    \"MySQL\",\n",
        "    \"SQLite\",\n",
        "    \"Boost\",\n",
        "    \"ZeroMQ\",\n",
        "    \"Protocol Buffer\",\n",
        "    \"gRPC\",\n",
        "    \"MQTT\",\n",
        "    \"RabbitMQ\",\n",
        "    \"Qt\",\n",
        "    \"WPF\",\n",
        "    \"DearImGUI\",\n",
        "    \"pytest\",\n",
        "    \"GoogleTest\",\n",
        "    \"Catch2\",\n",
        "    \"Docker\",\n",
        "    \"Jenkins\",\n",
        "    \"Bamboo\",\n",
        "    \"Jupyter Notebooks\"\n",
        "  ],\n",
        "  \"years_of_jobs\": [\n",
        "    \"Apr 2022 - Present\",\n",
        "    \"Jan 2020 - Feb 2022\",\n",
        "    \"Aug 2016 - Dec 2019\",\n",
        "    \"Jan 2013 - Jul 2016\",\n",
        "    \"Aug 2011 - Oct 2011\",\n",
        "    \"Apr 2008-Sep 2010\",\n",
        "    \"Nov 2005 - Apr 2008\"\n",
        "  ],\n",
        "  \"year_in_current_position\": \"1 year\",\n",
        "  \"Present_Organization\": \"IDnow GmbH\",\n",
        "  \"summary\": \"Abhilash Babu is a Senior Machine Learning Engineer with 18 years of experience in computer vision and image processing. He has developed and deployed machine learning solutions for various applications such as object detection and image classification. Abhilash is skilled in classical computer vision techniques as well as deep learning frameworks like TensorFlow and PyTorch. He has experience in developing applications for both desktop and embedded domains, and has expertise in microservices for machine learning applications. Abhilash has a strong educational background with a Master's degree in Communication Engineering. He is a certified Software Architect and Scrum Product Owner.\"\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "JWWeD7CRSD8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code offers a function (**save_to_json**) and a series of operations to enrich a list of job applications with resume summaries. The **save_to_json** function is intended to verify if given data can be validly saved as JSON, ensuring it's either a string (that's loadable as JSON) or a dictionary/list. If it's a string, the function attempts to parse it into a Python object to check its validity.\n",
        "\n",
        "In the main block, data is initially loaded from a JSON file (**filtered_applications.json**). For each application in the loaded data, if it has a '**resume_path**' and '**email_id**', the code reads the associated resume document and checks if its content needs trimming. The content is then sent to a summarization function (presumably, **summarize_resume**), which is not provided in the code but assumed to exist. The summarized data is then validated and saved back into the application's '**resume_summary**' field using the **save_to_json** function.\n",
        "\n",
        "Lastly, after iterating through all applications and updating their '**resume_summary**', the enriched data is saved back to the same JSON file, ensuring that the output is neatly formatted with a 4-space indentation."
      ],
      "metadata": {
        "id": "pXQ8uPyo-lSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_json(data):\n",
        "    \"\"\"\n",
        "    Save a Python data structure to a JSON file.\n",
        "\n",
        "    Args:\n",
        "    - data (dict or str): The Python data structure to be saved. Can be a string (that can be loaded as JSON) or a dictionary.\n",
        "    - filename (str): The name of the JSON file.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Check if the data is already a string and try to load it into a Python object.\n",
        "    # If it's already a Python object (like a dictionary or list), then pass.\n",
        "    if isinstance(data, str):\n",
        "        try:\n",
        "            data = json.loads(data)\n",
        "        except json.JSONDecodeError:\n",
        "            raise ValueError(\"The provided string is not valid JSON.\")\n",
        "    elif not isinstance(data, (dict, list)):\n",
        "        raise TypeError(\"The data should either be a valid JSON string, dictionary, or list.\")\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "Th51iAaKoFrw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "prompt_v3=f'''Read the given resume and extract information corresponding to the keys\\\n",
        " \"name_of_candidate\" which stores the candidate name, \\\n",
        " \"mobile_number\" contains the mobile number, \\\n",
        " \"email_id\" records the email id of the candidate, \\\n",
        " total years of experience is stored in \"years_of_experience\", \\\n",
        " \"education\" refers to the candidate's most recent or highest academic degree, \\\n",
        " last university/school/college attended by the candidate is given by \"university\", \\\n",
        " \"linkedin_profile\" contains the linkedin profile, \\\n",
        " record all the technical skills in \"technical_skills\", \\\n",
        " \"years_of_jobs\" showcases the years spent in different jobs, \\\n",
        " years spent in the current organization is given by \"year_in_current_position\", \\\n",
        " \"Present_Organization\" denotes name of the present organization and \"summay\". \\\n",
        " For \"technical_skills\", provide a summary of the programming languages, libraries, and frameworks the candidate has experience with as a list, \\\n",
        " \"years_of_jobs\" is a list of job durations, e.g., [\"2012-current\",\"2010-2012\", (June 22, 2022 - Present)]. \\\n",
        " \"year_in_current_position\" indicates the duration in their current job role integer. Present year is 2023. \\\n",
        " \"years_of_experience\" is the sum of years spent in all jobs including the current one. \\\n",
        " Round off the year to the upper ceiling. So, if it is 3 months, round it off to 1 year. \\\n",
        " Summarize the resume in approximately 100 words for the \"summary\" field. \\\n",
        " The final output must be in JSON\n",
        "'''\n",
        "json_data = read_json('/content/filtered_applications.json')\n",
        "\n",
        "\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "for application in json_data:\n",
        "    if 'resume_path' in application and 'email_id' in application:\n",
        "        resume_text = read_document(os.path.join(resume_path, application['resume_path']))\n",
        "        resume_text, _, _ = check_and_trim(resume_text)\n",
        "\n",
        "        retries = 0\n",
        "        success = False\n",
        "\n",
        "        while not success and retries < MAX_RETRIES:\n",
        "            try:\n",
        "                application['resume_summary'] = save_to_json(summarize_resume(prompt_v3, resume_text))\n",
        "\n",
        "                success = True\n",
        "            except Exception as e:\n",
        "                retries += 1\n",
        "\n",
        "\n",
        "save_to_json(json_data)\n",
        "# Save the updated data back to the same JSON file\n",
        "with open('/content/filtered_applications_summary.json', 'w') as f:\n",
        "    json.dump(json_data, f, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsytGlUj8PSz",
        "outputId": "88b6c7f2-4c20-476a-c5f1-edec0cfb7936"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: The provided string is not valid JSON.. Retrying attempt 1/5...\n",
            "An error occurred: The provided string is not valid JSON.. Retrying attempt 1/5...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the **filtered_applications_summary.json** file to be used in the next assignments."
      ],
      "metadata": {
        "id": "P0BHj2lG-0uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# List of file paths that you want to download\n",
        "file_paths = [\n",
        "    \"/content/filtered_applications_summary.json\",\n",
        "]\n",
        "\n",
        "# Download each file to your local system\n",
        "for path in file_paths:\n",
        "    files.download(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "A4FFCiU9uw_9",
        "outputId": "721e9b63-46d3-482c-8711-f32fdd57d2bf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3f6348c9-7344-42ce-a112-aa6ce03c9202\", \"filtered_applications_summary.json\", 26784)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the above is done we repeat the same for all the applications that we have before filtering."
      ],
      "metadata": {
        "id": "7qg1DTSqnCFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "prompt_v3=f'''Read the given resume and extract information corresponding to the keys\\\n",
        " \"name_of_candidate\" which stores the candidate name, \\\n",
        " \"mobile_number\" contains the mobile number, \\\n",
        " \"email_id\" records the email id of the candidate, \\\n",
        " total years of experience is stored in \"years_of_experience\", \\\n",
        " \"education\" refers to the candidate's most recent or highest academic degree, \\\n",
        " last university/school/college attended by the candidate is given by \"university\", \\\n",
        " \"linkedin_profile\" contains the linkedin profile, \\\n",
        " record all the technical skills in \"technical_skills\", \\\n",
        " \"years_of_jobs\" showcases the years spent in different jobs, \\\n",
        " years spent in the current organization is given by \"year_in_current_position\", \\\n",
        " \"Present_Organization\" denotes name of the present organization and \"summay\". \\\n",
        " For \"technical_skills\", provide a summary of the programming languages, libraries, and frameworks the candidate has experience with as a list, \\\n",
        " \"years_of_jobs\" is a list of job durations, e.g., [\"2012-current\",\"2010-2012\", (June 22, 2022 - Present)]. \\\n",
        " \"year_in_current_position\" indicates the duration in their current job role integer. Present year is 2023. \\\n",
        " \"years_of_experience\" is the sum of years spent in all jobs including the current one. \\\n",
        " Round off the year to the upper ceiling. So, if it is 3 months, round it off to 1 year. \\\n",
        " Summarize the resume in approximately 100 words for the \"summary\" field. \\\n",
        " The final output must be in JSON\n",
        "'''\n",
        "all_applications = read_json('/content/all_applications.json')\n",
        "\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "for application in all_applications:\n",
        "    if 'resume_path' in application and 'email_id' in application:\n",
        "        resume_text = read_document(os.path.join(resume_path, application['resume_path']))\n",
        "        resume_text, _, _ = check_and_trim(resume_text)\n",
        "\n",
        "        retries = 0\n",
        "        success = False\n",
        "\n",
        "        while not success and retries < MAX_RETRIES:\n",
        "            try:\n",
        "                application['resume_summary'] = save_to_json(summarize_resume(prompt_v3, resume_text))\n",
        "                success = True\n",
        "            except Exception as e:\n",
        "                retries += 1\n",
        "\n",
        "\n",
        "save_to_json(all_applications)\n",
        "# Save the updated data back to the same JSON file\n",
        "with open('/content/all_applications_summary.json', 'w') as f:\n",
        "    json.dump(all_applications, f, indent=4)"
      ],
      "metadata": {
        "id": "CekcCxpKnMP4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now download the file '/content/all_applications_summary.json' to be used in Assignment8"
      ],
      "metadata": {
        "id": "skf3XP0srr4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# List of file paths that you want to download\n",
        "file_paths = [\n",
        "    \"/content/all_applications_summary.json\",\n",
        "]\n",
        "\n",
        "# Download each file to your local system\n",
        "for path in file_paths:\n",
        "    files.download(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "SPqA_nCErsJp",
        "outputId": "e6265d61-0e4d-44bf-9f00-7c2237641051"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_155abfbc-b5e9-4e37-a346-a6b8702ded18\", \"all_applications_summary.json\", 32931)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the successive iterations of the prompt for resume summarization we have finally achieved a prompt that can extract valuable information from resumes and give the output in JSON format with consistent keys such as \"years_of_experience\", \"technical_skills\", \"years_of_jobs\", etc."
      ],
      "metadata": {
        "id": "N-Ku9BgcRezc"
      }
    }
  ]
}