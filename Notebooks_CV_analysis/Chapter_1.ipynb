{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin with a warm welcome to the course and delve into the basics of utilizing the OpenAI API with a focus on the functionalities.\n",
        "\n",
        "### Welcome to the ChatGPT API Course!\n",
        "\n",
        "Hello everyone, and welcome to this exciting course where we will be exploring the functionalities and capabilities of the OpenAI API, specifically focusing on working with the ChatGPT model. Throughout this course, you will learn how to build applications leveraging the power of ChatGPT to perform a variety of language processing tasks.\n",
        "\n",
        "### Module 1: Introduction to OpenAI API and ChatGPT\n",
        "\n",
        "#### **1.1 Overview of OpenAI API**\n",
        "OpenAI offers a powerful API that grants access to cutting-edge language models trained to understand and generate text effectively. This API is versatile, capable of handling a wide array of tasks involving language processing, including but not limited to:\n",
        "- Content generation\n",
        "- Summarization\n",
        "- Classification and sentiment analysis\n",
        "- Data extraction\n",
        "- Translation\n",
        "\n",
        "#### **1.2 Understanding the Completions Endpoint**\n",
        "The core of the OpenAI API is the completions endpoint. It operates on a simple yet flexible interface where you provide a text prompt, and the API returns a text completion that aligns with the instructions or context given in the prompt. Essentially, it functions like an advanced autocomplete system, predicting the most probable text to follow your prompt.\n",
        "\n",
        "### Module 2: Building a Sample Application\n",
        "\n",
        "In this module, we will walk you through building a sample application, where you will learn the key concepts and techniques fundamental to using the API for any task.\n",
        "\n",
        "#### **2.1 Crafting Effective Prompts**\n",
        "The first step in utilizing the API effectively is crafting a clear and specific prompt that communicates your request to the model. You will learn how to \"program\" the model through prompt design, making your instructions more specific to obtain desired results.\n",
        "\n",
        "#### **2.2 Incorporating Examples**\n",
        "Sometimes, instructions alone may not suffice. In such cases, adding examples to your prompt can help convey patterns or nuances, guiding the model to provide the kind of responses you're looking for.\n",
        "\n",
        "#### **2.3 Adjusting Settings**\n",
        "Apart from designing prompts, you have the tool of adjusting settings at your disposal. One crucial setting is the \"temperature,\" which controls the diversity of the completions. A higher temperature value (closer to 1) encourages more diverse outputs, while a lower value (closer to 0) makes the completions more deterministic and focused.\n",
        "\n",
        "### Module 3: Building Your Application\n",
        "\n",
        "#### **3.1 Setting Up**\n",
        "In this part of the course, we will guide you through setting up your environment, including installing necessary tools and obtaining your API key.\n",
        "\n",
        "#### **3.2 Understanding and Implementing the Code**\n",
        "We will delve deep into the code structure, helping you understand how to generate prompts dynamically and how to send API requests using specific settings to get the desired outputs."
      ],
      "metadata": {
        "id": "WuYGxoDp8mkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Started with ChatGPT API: Installation and API Key\n",
        "\n",
        "#### **Step 1: Sign Up and API Key**\n",
        "\n",
        "Before you start working with the ChatGPT API, you need to sign up for access to OpenAI's API. Once you have signed up, you will be provided with an API key. This key is essential as it allows you to make requests to the API.\n",
        "\n",
        "```python\n",
        "# Your API key will look something like this:\n",
        "API_KEY = \"sk-abcdef1234567890abcdef1234567890\"\n",
        "```\n",
        "\n",
        "#### **Step 2: Installing OpenAI Python**\n",
        "\n",
        "Next, install the OpenAI Python package, which is a client library that allows you to interact with the OpenAI API easily. You can install it using the following command:\n",
        "\n",
        "```bash\n",
        "pip install openai\n",
        "```\n",
        "\n",
        "#### **Step 3: Setting Up Your Environment**\n",
        "\n",
        "After installing the OpenAI Python package, set up your environment variable to store the API key. This way, you can use the API key in your scripts securely. Use the following command to set the environment variable:\n",
        "\n",
        "```bash\n",
        "export OPENAI_API_KEY=\"sk-abcdef1234567890abcdef1234567890\"\n",
        "```\n",
        "\n",
        "#### **Step 4: Making Your First API Call**\n",
        "\n",
        "Now that everything is set up, you can make your first API call using a Python script. Here is a simple script that sends a prompt to the ChatGPT model and receives a response:\n",
        "\n",
        "```python\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"sk-abcdef1234567890abcdef1234567890\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  prompt=\"Once upon a time,\",\n",
        "  temperature=0.7,\n",
        "  max_tokens=150,\n",
        ")\n",
        "\n",
        "print(response.choices[0].text.strip())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "This is your starting point in working with the ChatGPT API. As you progress through the course, you will learn more about the different parameters and options you can use to tailor the model's responses to your needs. Let's get coding!\n",
        "\n",
        "---\n",
        "\n",
        "Let's run this command:"
      ],
      "metadata": {
        "id": "pUddK8KB-7Ke"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQY6cADM7mlh",
        "outputId": "0460bf68-6296-43be-a6a4-0519dfefd20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "# Installing openai package\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### GEt API Key: [API Keys](https://platform.openai.com/account/api-keys)\n",
        "#### Sample .env file:\n",
        "\n",
        "```\n",
        "OPENAI_API_KEY=sk-abcdef1234567890abcdef1234567890\n",
        "```"
      ],
      "metadata": {
        "id": "3idlvnbVU0xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export your API Key to environment variable\n",
        "# Upload .env file\n",
        "!pip install python-dotenv\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uDZMmYD_qpK",
        "outputId": "51bd3333-b0dc-4c66-8ce7-6be115c26f2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import openai API module\n",
        "import openai\n",
        "\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Complete the sentance\"},\n",
        "            {\"role\": \"user\", \"content\": \"Once upon a time, \"},\n",
        "        ]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        "  temperature=1,\n",
        "  max_tokens=150,\n",
        ")"
      ],
      "metadata": {
        "id": "7yrFXJ8qEDch"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the API call:\n",
        "\n",
        "Let's break down the different components of the code snippet:\n",
        "\n",
        "```python\n",
        "messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Complete the sentance\"},\n",
        "            {\"role\": \"user\", \"content\": \"Once upon a time, \"},\n",
        "        ]\n",
        "```\n",
        "\n",
        "#### **`messages` Array**\n",
        "\n",
        "In this part of the code, we define a list of message objects to interact with the ChatGPT model. Each message object contains two properties:\n",
        "\n",
        "- **`role`**: It specifies the role of the entity sending the message. It can be `\"system\"`, `\"user\"`, or `\"assistant\"`.\n",
        "- **`content`**: It contains the actual content of the message from the entity.\n",
        "\n",
        "Here, we have two messages:\n",
        "1. A system message instructing to \"Complete the sentence\".\n",
        "2. A user message providing the initial part of a sentence to be completed.\n",
        "\n",
        "```python\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        "  temperature=1,\n",
        "  max_tokens=150,\n",
        ")\n",
        "```\n",
        "\n",
        "#### **API Call**\n",
        "\n",
        "In this section, we make a call to the OpenAI API to get a response from the ChatGPT model. Let's break down the parameters used in this API call:\n",
        "\n",
        "- **`model`**: Specifies the version of the ChatGPT model to use. Here, we are using `\"gpt-3.5-turbo\"`, which is one of the latest and most advanced versions.\n",
        "- **`messages`**: We pass the `messages` array we defined earlier to set the conversation history and the current prompt.\n",
        "- **`temperature`**: Set to 1, this parameter controls the randomness in the model's output. A higher value like 1 encourages more diverse and creative responses.\n",
        "- **`max_tokens`**: This parameter limits the response to a maximum of 150 tokens, controlling the length of the output.\n",
        "\n",
        "```python\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())\n",
        "```\n",
        "\n",
        "#### **Printing the Response**\n",
        "\n",
        "Here, we extract and print the content of the message from the response generated by the ChatGPT model. The response object contains a lot of information, but we are specifically interested in the `content` of the first choice in the `choices` array, which contains the generated message from the model. We use the `strip()` method to remove any leading or trailing whitespace from the output.\n",
        "\n",
        "---\n",
        "\n",
        "This code snippet essentially sets up a small conversation with the ChatGPT model, where it is instructed to complete a sentence provided by the user, and then prints the model's response to the console. It is a simple yet powerful demonstration of how you can interact with ChatGPT using the OpenAI API."
      ],
      "metadata": {
        "id": "xJWx5zjTE32z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print entire response:\n",
        "print(\"Response: \")\n",
        "print(response, \"\\n\")\n",
        "\n",
        "# Print only the output:\n",
        "print(\"Output: \")\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIEvp6TeEvbw",
        "outputId": "f5824ba6-6880-41ea-e8a7-9fcd8a17b5dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: \n",
            "{\n",
            "  \"id\": \"chatcmpl-7zatoXn2mzRBX84Ko8iwzDXzuV6z5\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1694914204,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"in a faraway kingdom, there lived a brave and kind-hearted princess.\"\n",
            "      },\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 21,\n",
            "    \"completion_tokens\": 15,\n",
            "    \"total_tokens\": 36\n",
            "  }\n",
            "} \n",
            "\n",
            "Output: \n",
            "in a faraway kingdom, there lived a brave and kind-hearted princess.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the response:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id\": \"chatcmpl-7yyzU9qt1dxH9z03FJXtdODZlJCcM\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"created\": 1694768484,\n",
        "  \"model\": \"gpt-3.5-turbo-0613\",\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"index\": 0,\n",
        "      \"message\": {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"there was a kingdom ruled by a wise and just king.\"\n",
        "      },\n",
        "      \"finish_reason\": \"stop\"\n",
        "    }\n",
        "  ],\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 21,\n",
        "    \"completion_tokens\": 12,\n",
        "    \"total_tokens\": 33\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Response Metadata**\n",
        "\n",
        "- **`id`**: This is a unique identifier for the API response. It can be used for referencing and tracking individual API calls.\n",
        "- **`object`**: This field indicates the type of object returned, which in this case is a \"chat.completion\".\n",
        "- **`created`**: This is a timestamp indicating when the response was created. It is represented in Unix time format.\n",
        "- **`model`**: This field specifies the version of the model used to generate the response. Here, it is \"gpt-3.5-turbo-0613\".\n",
        "\n",
        "```json\n",
        "\"choices\": [\n",
        "    {\n",
        "      \"index\": 0,\n",
        "      \"message\": {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"there was a kingdom ruled by a wise and just king.\"\n",
        "      },\n",
        "      \"finish_reason\": \"stop\"\n",
        "    }\n",
        "  ],\n",
        "```\n",
        "\n",
        "#### **Choices Array**\n",
        "\n",
        "- **`choices`**: This is an array containing the responses generated by the model. Although we requested a single response, it is possible to ask for more choices by setting the `n` parameter in the API call.\n",
        "  - **`index`**: This field indicates the index of the choice in the array. Since we have only one choice, the index is 0.\n",
        "  - **`message`**: This is an object containing the details of the message generated by the assistant.\n",
        "    - **`role`**: Specifies the role of the entity that generated the message, which is \"assistant\" in this case.\n",
        "    - **`content`**: Contains the actual content of the message generated by the assistant.\n",
        "  - **`finish_reason`**: Indicates why the assistant stopped generating further content. Here, it stopped because it reached a logical stopping point, denoted by \"stop\".\n",
        "\n",
        "```json\n",
        "\"usage\": {\n",
        "    \"prompt_tokens\": 21,\n",
        "    \"completion_tokens\": 12,\n",
        "    \"total_tokens\": 33\n",
        "  }\n",
        "```\n",
        "\n",
        "#### **Usage Information**\n",
        "\n",
        "- **`usage`**: This section provides details about the token usage in the API call.\n",
        "  - **`prompt_tokens`**: Indicates the number of tokens used in the prompt. In this case, it is 21 tokens.\n",
        "  - **`completion_tokens`**: Specifies the number of tokens generated in the completion. Here, it is 12 tokens.\n",
        "  - **`total_tokens`**: Represents the total number of tokens used in the API call, which is the sum of the prompt and completion tokens, amounting to 33 tokens.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "09CVPSBgGb2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple responses in the API call:\n",
        "\n",
        "```python\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        "  temperature=1,\n",
        "  max_tokens=150,\n",
        "  n=5\n",
        ")\n",
        "```\n",
        "\n",
        "#### **`n` Parameter**\n",
        "\n",
        "- **`n`**: This parameter controls the number of responses (or \"completions\") we want to receive from the API for a single prompt. By setting it to 5, we are instructing the API to generate 5 different completions for the prompt provided in the `messages` array.\n",
        "\n",
        "```python\n",
        "for i in range(5):\n",
        "  # Print only the output:\n",
        "  print(f\"Output {i+1}: \")\n",
        "  print(response[\"choices\"][i].message[\"content\"].strip())\n",
        "```\n",
        "\n",
        "#### **Printing Multiple Responses**\n",
        "\n",
        "In this section of the code:\n",
        "\n",
        "- We loop through the range of 5 (since we requested 5 responses) using a for loop.\n",
        "- Inside the loop, we print a header indicating the output number (e.g., \"Output 1\", \"Output 2\", etc.).\n",
        "- Following the header, we print each individual response generated by the ChatGPT model. We access each response from the `choices` array in the response object using the loop index `i`.\n",
        "- The `strip()` method is used to remove any leading or trailing whitespace from each output.\n",
        "\n",
        "---\n",
        "\n",
        "This modification to the script allows us to explore a variety of responses that the ChatGPT model can generate for a single prompt, giving us more options to choose from or analyze. It's a great way to see the different directions in which the conversation can go based on a single prompt."
      ],
      "metadata": {
        "id": "39Iwy5twHywi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting multiple response from the API:\n",
        "messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Complete the sentance\"},\n",
        "            {\"role\": \"user\", \"content\": \"Once upon a time, \"},\n",
        "        ]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        "  temperature=1,\n",
        "  max_tokens=150,\n",
        "  n=5\n",
        ")\n",
        "\n",
        "for i in range(5):\n",
        "  # Print only the output:\n",
        "  print(f\"Output {i+1}: \")\n",
        "  print(response[\"choices\"][i].message[\"content\"].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYrayTCGGzQ6",
        "outputId": "5af8b763-1aaf-471e-e21f-3f6dd939f758"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output 1: \n",
            "there was a princess named Aurora.\n",
            "Output 2: \n",
            "there was a young girl named Alice who lived in a small village.\n",
            "Output 3: \n",
            "there was a beautiful princess who lived in a magnificent castle.\n",
            "Output 4: \n",
            "in a faraway land, there lived a brave and adventurous princess.\n",
            "Output 5: \n",
            "in a faraway land, lived a young princess named Aurora.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding temprature\n",
        "\n",
        "Let's discuss the role of the `temperature` parameter in our script and how it influences the responses generated by the ChatGPT model:\n",
        "\n",
        "### Understanding the `temperature` Parameter\n",
        "\n",
        "In our script, we are experimenting with two different temperature settings - 0 and 1.5 - to observe how they affect the output of the ChatGPT model. The valid range for it is [0. 2]. Let's delve into what the `temperature` parameter does and how it is reflected in our script:\n",
        "\n",
        "#### **Temperature Set to 0**\n",
        "\n",
        "```python\n",
        "print(\"Temperature: 0\")\n",
        "for i in range(5):\n",
        "  response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    temperature=0,\n",
        "    max_tokens=150,\n",
        "  )\n",
        "  ...\n",
        "```\n",
        "\n",
        "- **`temperature=0`**: Setting the temperature to 0 makes the model's output completely deterministic, choosing the most likely next word at each step in the generation process. As a result, no matter how many times we run the loop, we will receive the exact same output because the model will always choose the most probable word to follow the prompt.\n",
        "\n",
        "#### **Temperature Set to 1.5**\n",
        "\n",
        "```python\n",
        "print(\"Temperature: 1.5\")\n",
        "for i in range(5):\n",
        "  response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    temperature=1.5,\n",
        "    max_tokens=150,\n",
        "  )\n",
        "  ...\n",
        "```\n",
        "\n",
        "- **`temperature=1.5`**: Setting the temperature to 1.5 allows for a higher degree of randomness in the output. The model is more likely to take \"creative liberties,\" choosing less probable words to follow the prompt, which results in more diverse and varied outputs. When we run the loop multiple times with a temperature of 1.5, we can notice that the outputs can be quite different from each other, showcasing a range of possible completions for the prompt.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Cz8ynS9wILY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting multiple response from the API:\n",
        "messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Complete the sentance\"},\n",
        "            {\"role\": \"user\", \"content\": \"Today is a wonderful day for  \"},\n",
        "        ]\n",
        "\n",
        "print(\"Temprature: 0\")\n",
        "for i in range(5):\n",
        "  response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    temperature=0,\n",
        "    max_tokens=150,\n",
        "  )\n",
        "\n",
        "  # Print only the output:\n",
        "  print(f\"Output: \")\n",
        "  print(response[\"choices\"][0].message[\"content\"].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOeiveayIO8N",
        "outputId": "be5e2656-bec5-4d3a-8f83-1645feaff5b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temprature: 0\n",
            "Output: \n",
            "a picnic in the park.\n",
            "Output: \n",
            "a picnic in the park.\n",
            "Output: \n",
            "a picnic in the park.\n",
            "Output: \n",
            "a picnic in the park.\n",
            "Output: \n",
            "a picnic in the park.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting multiple response from the API:\n",
        "messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Complete the sentance\"},\n",
        "            {\"role\": \"user\", \"content\": \"Today is a wonderful day for  \"},\n",
        "        ]\n",
        "\n",
        "print(\"Temprature: 1.5\")\n",
        "for i in range(5):\n",
        "  response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    temperature=1.5,\n",
        "    max_tokens=150,\n",
        "  )\n",
        "\n",
        "  # Print only the output:\n",
        "  print(f\"Output: \")\n",
        "  print(response[\"choices\"][0].message[\"content\"].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VfIIRNxIaIt",
        "outputId": "c6ca6320-5106-46c3-d9db-b176526c25ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temprature: 1.5\n",
            "Output: \n",
            "experiencing newfound joy and gratitude.\n",
            "Output: \n",
            "celebrating and enjoying the outdoors.\n",
            "Output: \n",
            "relaxation and enjoying the sunshine.\n",
            "Output: \n",
            "outdoor activities.\n",
            "Output: \n",
            "spending time outdoors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding max-tokens\n",
        "\n",
        "Let's delve into the `max_tokens` parameter, which is used to control the length of the response generated by the ChatGPT model.\n",
        "\n",
        "### Understanding the `max_tokens` Parameter\n",
        "\n",
        "The `max_tokens` parameter allows us to limit the length of the response generated by the ChatGPT model. By setting a specific number of tokens, we can control how verbose or concise the model's responses will be. Let's see how it works with different settings:\n",
        "\n",
        "#### **Example 1: Setting `max_tokens` to a Small Value**\n",
        "\n",
        "```python\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        "  temperature=1,\n",
        "  max_tokens=5,\n",
        ")\n",
        "\n",
        "print(\"Max Tokens: 5\")\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())\n",
        "```\n",
        "\n",
        "- **`max_tokens=5`**: In this setting, the output will be very short, limited to just 5 tokens. This might result in responses that are cut-off and don't convey a complete thought.\n",
        "\n",
        "#### **Example 2: Setting `max_tokens` to a Moderate Value**\n",
        "\n",
        "```python\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        "  temperature=1,\n",
        "  max_tokens=20,\n",
        ")\n",
        "\n",
        "print(\"Max Tokens: 20\")\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())\n",
        "```\n",
        "\n",
        "- **`max_tokens=20`**: Here, the model has more room to generate a fuller response, but it is still relatively concise, ensuring that the output remains focused and to the point.\n",
        "\n",
        "#### **Example 3: Setting `max_tokens` to a Large Value**\n",
        "\n",
        "```python\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        "  temperature=1,\n",
        "  max_tokens=150,\n",
        ")\n",
        "\n",
        "print(\"Max Tokens: 150\")\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())\n",
        "```\n",
        "\n",
        "- **`max_tokens=150`**: With this setting, the model can generate much longer and more detailed responses, potentially providing more context or exploring different facets of the topic at hand.\n",
        "\n",
        "---\n",
        "\n",
        "The `max_tokens` parameter is a powerful tool to control the length of the responses generated by the ChatGPT model. Depending on your specific use case, you might choose to limit the responses to a few tokens or allow for longer, more detailed responses. It's a crucial parameter to experiment with to get the desired output from the model.\n",
        "\n",
        "Remember, a \"token\" in GPT-3 can be as short as one character or as long as one word, so the number of tokens doesn't directly translate to the number of words."
      ],
      "metadata": {
        "id": "bdkvVYc0Kl7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_tokens variations\n",
        "messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Complete the sentance\"},\n",
        "            {\"role\": \"user\", \"content\": \"List of things you can do in San Diego are: \"},\n",
        "        ]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        "  temperature=1,\n",
        "  max_tokens=5,\n",
        ")\n",
        "\n",
        "print(\"Max Tokens: 5\")\n",
        "print(f\"List of things you can do in San Diego are: {response['choices'][0].message['content'].strip()}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        "  temperature=1,\n",
        "  max_tokens=20,\n",
        ")\n",
        "\n",
        "print(\"Max Tokens: 20\")\n",
        "print(f\"List of things you can do in San Diego are: {response['choices'][0].message['content'].strip()}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        "  temperature=1,\n",
        "  max_tokens=150,\n",
        ")\n",
        "\n",
        "print(\"Max Tokens: 150\")\n",
        "print(f\"List of things you can do in San Diego are: {response['choices'][0].message['content'].strip()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmJGblrZKv6Q",
        "outputId": "72092bf4-9997-4d4d-982f-3d7244308315"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Tokens: 5\n",
            "List of things you can do in San Diego are: 1. Visit the San\n",
            "\n",
            "\n",
            "Max Tokens: 20\n",
            "List of things you can do in San Diego are: 1. Visit the beaches \n",
            "2. Explore Balboa Park \n",
            "3. Visit the San Diego Zoo\n",
            "\n",
            "\n",
            "Max Tokens: 150\n",
            "List of things you can do in San Diego are: visit the famous San Diego Zoo, explore Balboa Park, relax on the stunning beaches, go whale watching, visit the USS Midway Museum, enjoy a day at SeaWorld, catch a Padres baseball game at Petco Park, take a harbor cruise, hike in Torrey Pines State Natural Reserve, experience the vibrant Gaslamp Quarter, explore the historic Old Town, go shopping at the Fashion Valley Mall, try out the local craft breweries, and dine on delicious seafood cuisine.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note\n",
        "\n",
        "Notice it takes longer when the max_tokens number is high. It takes longer for the API to generate longer responses. This is something to be considered according the API call use case"
      ],
      "metadata": {
        "id": "BsD64ghyM-_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding different model variants\n",
        "\n",
        "Let's delve into the details of the specified models:\n",
        "\n",
        "### **GPT-3.5-turbo**\n",
        "\n",
        "- **Description**: This is the most capable and cost-effective model in the GPT-3.5 series. It has been optimized for chat applications but also performs well in traditional completion tasks.\n",
        "- **Max Tokens**: It can handle up to 4,097 tokens per request.\n",
        "- **Training Data**: The model was trained on data available up until September 2021.\n",
        "- **Usage**: It is recommended for a wide variety of language tasks due to its lower cost and improved performance compared to other models in the GPT-3.5 series.\n",
        "\n",
        "### **GPT-3.5-turbo-16k**\n",
        "\n",
        "- **Description**: This model shares the same capabilities as the standard GPT-3.5-turbo but allows for a larger context window, with up to 16,385 tokens.\n",
        "- **Max Tokens**: It can process up to 16,385 tokens per request, providing a broader scope for understanding and generating content based on extensive prompts.\n",
        "- **Training Data**: Like the standard GPT-3.5-turbo, it was trained on data available until September 2021.\n",
        "- **Usage**: It is suitable for tasks that require a larger context or more detailed responses, facilitating deeper and more nuanced conversations.\n",
        "\n",
        "### **GPT-4**\n",
        "\n",
        "- **Description**: GPT-4 represents a significant advancement over the previous generations, offering greater accuracy in solving complex problems thanks to its broader general knowledge and advanced reasoning capabilities. It is optimized for chat applications but also works well for traditional completion tasks.\n",
        "- **Max Tokens**: The standard GPT-4 model can handle up to 8,192 tokens per request. There is also a variant with a larger context window that can handle up to 32,768 tokens.\n",
        "- **Training Data**: The model was trained on data available up until September 2021.\n",
        "- **Usage**: It is accessible to developers who have made at least one successful payment through the OpenAI developer platform. It is recommended for tasks that require advanced reasoning and a deep understanding of complex topics.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "GPT-4 (Input: $0.03 / 1k tokens, Output: \t$0.06 / 1k tokens) API costs are a lot higher compared to GPT-3.5, For turbo it is \tInput: $0.0015 / 1K tokens, Output:\t$0.002 / 1K tokens and for turbo-16k it is \tInput: $0.003 / 1K tokens, Output:\t$0.004 / 1K tokens\n",
        "\n",
        "For smaller tasks, the cost of 3.5-turbo will be half od 3.5-turbo-16k. But the length of input context in 3.5-turbo is limited to 4k, while it is 16k in 3.5-turbo-16k"
      ],
      "metadata": {
        "id": "hEooJjTnN2SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_tokens variations\n",
        "messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Describe an ideal candidate for a job title\"},\n",
        "            {\"role\": \"user\", \"content\": \"computer vision engineer\"},\n",
        "        ]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        "  temperature=0,\n",
        "  max_tokens=100,\n",
        ")\n",
        "\n",
        "print(\"gpt-3.5-turbo \\n\")\n",
        "print(response['choices'][0].message['content'].strip())\n",
        "print(\"\\n\")\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages,\n",
        "  temperature=0,\n",
        "  max_tokens=100,\n",
        ")\n",
        "\n",
        "print(\"gpt-3.5-turbo-16k \\n\")\n",
        "print(response['choices'][0].message['content'].strip())\n",
        "print(\"\\n\")\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-4\",\n",
        "  messages=messages,\n",
        "  temperature=0,\n",
        "  max_tokens=100,\n",
        ")\n",
        "\n",
        "print(\"gpt-4 \\n\")\n",
        "print(response['choices'][0].message['content'].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAw74TI9M-wj",
        "outputId": "711bae76-6577-476a-ef4e-396746a7aa04"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-3.5-turbo \n",
            "\n",
            "An ideal candidate for the job title of computer vision engineer would possess a combination of technical skills, experience, and personal qualities. \n",
            "\n",
            "First and foremost, the ideal candidate would have a strong background in computer science, with a focus on computer vision and machine learning. They should have a deep understanding of algorithms, image processing techniques, and computer vision frameworks. They should also be proficient in programming languages such as Python, C++, and MATLAB.\n",
            "\n",
            "In terms of experience, the ideal candidate would have a proven track\n",
            "\n",
            "\n",
            "gpt-3.5-turbo-16k \n",
            "\n",
            "An ideal candidate for the job title of computer vision engineer would possess a combination of technical skills, experience, and personal qualities. \n",
            "\n",
            "First and foremost, the ideal candidate would have a strong background in computer science, with a focus on computer vision and machine learning. They should have a deep understanding of algorithms, image processing techniques, and computer vision frameworks such as OpenCV or TensorFlow. They should also be proficient in programming languages commonly used in computer vision, such as Python or C++.\n",
            "\n",
            "In terms of\n",
            "\n",
            "\n",
            "gpt-4 \n",
            "\n",
            "An ideal candidate for a Computer Vision Engineer position should have a strong background in computer science and mathematics, with a focus on machine learning and image processing. They should hold a degree in Computer Science, Electrical Engineering, or a related field, preferably at the master's or PhD level. \n",
            "\n",
            "They should have extensive experience with programming languages such as Python, C++, or Java, and be proficient in using computer vision libraries like OpenCV. Knowledge of deep learning frameworks such as TensorFlow or PyTorch is also\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the differnece between gpt-3.5-turbo and gpt-3.5-turbo-16k\n",
        "\n",
        "Let's explore how the GPT-3.5-turbo and GPT-3.5-turbo-16k models differ, especially in terms of handling different lengths of context:\n",
        "\n",
        "### **GPT-3.5-turbo**\n",
        "\n",
        "#### **Short Context**\n",
        "\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())\n",
        "```\n",
        "\n",
        "- **Description**: In this scenario, we are providing a short context where a user is asking for a joke. The GPT-3.5-turbo model would easily handle this and generate a joke in response.\n",
        "\n",
        "#### **Long Context**\n",
        "\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"...\" * 4000},  # A very long context\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())\n",
        "```\n",
        "\n",
        "- **Description**: Here, we are providing a very long context, nearing the maximum token limit of the model. The model might still be able to respond correctly, but the available space for the response would be very limited due to the large context.\n",
        "\n",
        "#### **Very Long Context**\n",
        "\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"...\" * 6000},  # A very long context\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())\n",
        "```\n",
        "\n",
        "- **Description**: Here, we are providing a very long context, exceeding the maximum token limit of the model. The model will give an error.\n",
        "\n",
        "### **GPT-3.5-turbo-16k**\n",
        "\n",
        "#### **Short Context**\n",
        "\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())\n",
        "```\n",
        "\n",
        "- **Description**: Similar to the GPT-3.5-turbo model, the GPT-3.5-turbo-16k model would easily handle a short context and provide a joke in response.\n",
        "\n",
        "#### **Long Context**\n",
        "\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"...\" * 16000},  # A very long context\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())\n",
        "```\n",
        "\n",
        "- **Description**: In this scenario, we are providing an extremely long context, utilizing the large token capacity of the GPT-3.5-turbo-16k model. Despite the long context, the model would still have ample space to generate a detailed response, answering the question correctly.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "- **GPT-3.5-turbo**: Suitable for most applications, but can struggle with very long contexts due to its maximum token limit of 4,097.\n",
        "- **GPT-3.5-turbo-16k**: Ideal for applications requiring a large context window, as it can handle up to 16,385 tokens, facilitating deeper and more nuanced conversations with extensive contexts.\n",
        "\n",
        "By varying the length of the context given, you can observe how the different models handle short and long contexts and choose the model that best suits your application's needs.\n"
      ],
      "metadata": {
        "id": "B-3cEOoJRHzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Very short context with gpt-3.5-turbo\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr6XhLRUR8hf",
        "outputId": "47d86d15-a4f6-43be-fce3-39c0e5f4461c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here's one for you:\n",
            "\n",
            "Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Long context with gpt-3.5-turbo\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"...\" * 4000},  # A very long context\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a joke\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYZ5Cs9bSGHV",
        "outputId": "279c362f-0cac-4abe-e868-d044400ab7c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here's a classic one for you: \n",
            "\n",
            "Why don't skeletons fight each other?\n",
            "\n",
            "Because they don't have the guts!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Very Long context with gpt-3.5-turbo - exceeding 4k limit\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \". \" * 5000},  # A very long context\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "03ceIDRtSVu1",
        "outputId": "15693b3c-2d61-44f3-de2a-2dc17e8bfe57"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidRequestError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-22178a8b5dd6>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 5019 tokens. Please reduce the length of the messages."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Very Long context with gpt-3.5-turbo-16k\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \". \" * 5000},  # A very long context\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH3CbfX8Synt",
        "outputId": "4b55ac3c-f72e-4e7d-d26e-d9025341a4c4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding tokens\n",
        "\n",
        "We discussed having max tokens and models having limit in tokens, let's delve deeper into what is a token.\n",
        "\n",
        "### Part 1: Understanding Tokenizers\n",
        "\n",
        "#### What are Tokenizers?\n",
        "Tokenizers are tools used in natural language processing (NLP) to convert input text into a format (a series of tokens) that can be fed into a language model. A token can be as small as a character or as long as a word.\n",
        "\n",
        "#### Why are they used in Language Models like ChatGPT?\n",
        "Tokenizers help in breaking down the input text into smaller units, making it easier for the model to analyze and understand the text. It aids in:\n",
        "- Reducing the complexity of the text\n",
        "- Facilitating the identification of patterns in the text\n",
        "- Enhancing the model's ability to generalize and understand grammar through the recognition of common subwords\n",
        "\n",
        "### Part 2: Introduction to Tiktoken [Tokenizer](https://platform.openai.com/tokenizer)\n",
        "\n",
        "#### What is Tiktoken?\n",
        "Tiktoken is a fast Byte Pair Encoding (BPE) tokenizer developed by OpenAI for use with its models, including ChatGPT. It is designed to be faster and more efficient compared to other open-source tokenizers.\n",
        "\n",
        "#### How does Tiktoken work?\n",
        "Tiktoken utilizes BPE to tokenize text. BPE works by iteratively merging frequent pairs of characters in the training data until a desired vocabulary size is reached. This way, it can represent common words as single tokens and rare words as sequences of subword tokens.\n",
        "\n",
        "### Part 3: Using Tiktoken\n",
        "\n",
        "#### Installing Tiktoken\n",
        "To use Tiktoken, you first need to install it from PyPI using the following command:\n",
        "```python\n",
        "pip install tiktoken\n",
        "```\n",
        "\n",
        "#### Basic Usage\n",
        "Here is how you can use Tiktoken to tokenize a piece of text:\n",
        "```python\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "tokens = enc.encode(\"hello world\")\n",
        "print(tokens)\n",
        "```\n",
        "\n",
        "### Part 4: Understanding Components\n",
        "\n",
        "#### Encoding and Decoding\n",
        "In Tiktoken, you can encode a text to tokens and decode it back to the original text using the `encode` and `decode` methods respectively. Here is an example:\n",
        "```python\n",
        "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
        "```\n",
        "\n",
        "#### Educational Submodule\n",
        "Tiktoken contains an educational submodule that helps users learn more about BPE. It includes functionalities to train a BPE tokenizer on a small amount of text and visualize how the GPT-4 encoder encodes text.\n",
        "\n",
        "### Part 5: Visualizing Token Changes\n",
        "\n",
        "To visualize how tokens change as text is altered, we can use Tiktoken's educational submodule. Here is a small piece of code that demonstrates this:\n",
        "```python\n",
        "from tiktoken._educational import *\n",
        "\n",
        "# Train a BPE tokenizer on a small amount of text\n",
        "enc = train_simple_encoding()\n",
        "\n",
        "# Visualize how the GPT-4 encoder encodes text\n",
        "enc = SimpleBytePairEncoding.from_tiktoken(\"cl100k_base\")\n",
        "encoded_text = enc.encode(\"hello world aaaaaaaaaaaa\")\n",
        "print(encoded_text)\n",
        "```\n",
        "\n",
        "In this script:\n",
        "- We first import the educational submodule from Tiktoken.\n",
        "- We train a simple BPE tokenizer using a small amount of text.\n",
        "- We visualize how the GPT-4 encoder encodes a piece of text, and print the encoded text to see the tokens.\n"
      ],
      "metadata": {
        "id": "eIxeWyr9S_78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the tiktoken package\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF82L8w3S_w8",
        "outputId": "b81ccfb2-6130-4e6d-f497-d84e5bd3621e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/2.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m1.4/2.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic usage:\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "tokens = enc.encode(\"hello world\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umPbdm17S7Go",
        "outputId": "28c5fa7a-5a91-453e-97f4-d3261dd191b3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15339, 1917]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# deconde the encoded tokens\n",
        "tokens = enc.encode(\"hello world\")\n",
        "print(enc.decode(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeaqkCmNUR0c",
        "outputId": "130e834f-eec3-4f81-8c93-cd95178f0a6f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the tokens:\n",
        "from tiktoken._educational import *\n",
        "\n",
        "# Visualize how the GPT-4 encoder encodes text\n",
        "enc = SimpleBytePairEncoding.from_tiktoken(\"cl100k_base\")\n",
        "encoded_text = enc.encode(\"Today is a wonderful day for a webinar on ChatGPT API.\")\n",
        "print(encoded_text)\n",
        "print(\"Number of tokens: \", len(encoded_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCgzcqsGTqND",
        "outputId": "37f89ed3-3482-4739-ed70-12a368eb4b4e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[48;5;167mT\u001b[48;5;179mo\u001b[48;5;185md\u001b[48;5;77ma\u001b[48;5;80my\u001b[0m\n",
            "\u001b[48;5;167mT\u001b[48;5;179mod\u001b[48;5;77ma\u001b[48;5;80my\u001b[0m\n",
            "\u001b[48;5;167mT\u001b[48;5;179mod\u001b[48;5;77may\u001b[0m\n",
            "\u001b[48;5;167mT\u001b[48;5;179moday\u001b[0m\n",
            "\u001b[48;5;167mToday\u001b[0m\n",
            "\n",
            "\u001b[48;5;167m \u001b[48;5;179mi\u001b[48;5;185ms\u001b[0m\n",
            "\u001b[48;5;167m \u001b[48;5;179mis\u001b[0m\n",
            "\u001b[48;5;167m is\u001b[0m\n",
            "\n",
            "\u001b[48;5;167m \u001b[48;5;179ma\u001b[0m\n",
            "\u001b[48;5;167m a\u001b[0m\n",
            "\n",
            "\u001b[48;5;167m \u001b[48;5;179mw\u001b[48;5;185mo\u001b[48;5;77mn\u001b[48;5;80md\u001b[48;5;68me\u001b[48;5;134mr\u001b[48;5;167mf\u001b[48;5;179mu\u001b[48;5;185ml\u001b[0m\n",
            "\u001b[48;5;167m \u001b[48;5;179mw\u001b[48;5;185mo\u001b[48;5;77mn\u001b[48;5;80md\u001b[48;5;68mer\u001b[48;5;167mf\u001b[48;5;179mu\u001b[48;5;185ml\u001b[0m\n",
            "\u001b[48;5;167m \u001b[48;5;179mw\u001b[48;5;185mon\u001b[48;5;80md\u001b[48;5;68mer\u001b[48;5;167mf\u001b[48;5;179mu\u001b[48;5;185ml\u001b[0m\n",
            "\u001b[48;5;167m w\u001b[48;5;185mon\u001b[48;5;80md\u001b[48;5;68mer\u001b[48;5;167mf\u001b[48;5;179mu\u001b[48;5;185ml\u001b[0m\n",
            "\u001b[48;5;167m w\u001b[48;5;185mon\u001b[48;5;80md\u001b[48;5;68mer\u001b[48;5;167mf\u001b[48;5;179mul\u001b[0m\n",
            "\u001b[48;5;167m w\u001b[48;5;185mon\u001b[48;5;80mder\u001b[48;5;167mf\u001b[48;5;179mul\u001b[0m\n",
            "\u001b[48;5;167m w\u001b[48;5;185mon\u001b[48;5;80mder\u001b[48;5;167mful\u001b[0m\n",
            "\u001b[48;5;167m won\u001b[48;5;80mder\u001b[48;5;167mful\u001b[0m\n",
            "\u001b[48;5;167m wonder\u001b[48;5;179mful\u001b[0m\n",
            "\u001b[48;5;167m wonderful\u001b[0m\n",
            "\n",
            "\u001b[48;5;167m \u001b[48;5;179md\u001b[48;5;185ma\u001b[48;5;77my\u001b[0m\n",
            "\u001b[48;5;167m d\u001b[48;5;185ma\u001b[48;5;77my\u001b[0m\n",
            "\u001b[48;5;167m d\u001b[48;5;185may\u001b[0m\n",
            "\u001b[48;5;167m day\u001b[0m\n",
            "\n",
            "\u001b[48;5;167m \u001b[48;5;179mf\u001b[48;5;185mo\u001b[48;5;77mr\u001b[0m\n",
            "\u001b[48;5;167m \u001b[48;5;179mf\u001b[48;5;185mor\u001b[0m\n",
            "\u001b[48;5;167m f\u001b[48;5;185mor\u001b[0m\n",
            "\u001b[48;5;167m for\u001b[0m\n",
            "\n",
            "\u001b[48;5;167m \u001b[48;5;179ma\u001b[0m\n",
            "\u001b[48;5;167m a\u001b[0m\n",
            "\n",
            "\u001b[48;5;167m \u001b[48;5;179mw\u001b[48;5;185me\u001b[48;5;77mb\u001b[48;5;80mi\u001b[48;5;68mn\u001b[48;5;134ma\u001b[48;5;167mr\u001b[0m\n",
            "\u001b[48;5;167m \u001b[48;5;179mw\u001b[48;5;185me\u001b[48;5;77mb\u001b[48;5;80min\u001b[48;5;134ma\u001b[48;5;167mr\u001b[0m\n",
            "\u001b[48;5;167m \u001b[48;5;179mw\u001b[48;5;185me\u001b[48;5;77mb\u001b[48;5;80min\u001b[48;5;134mar\u001b[0m\n",
            "\u001b[48;5;167m w\u001b[48;5;185me\u001b[48;5;77mb\u001b[48;5;80min\u001b[48;5;134mar\u001b[0m\n",
            "\u001b[48;5;167m we\u001b[48;5;77mb\u001b[48;5;80min\u001b[48;5;134mar\u001b[0m\n",
            "\u001b[48;5;167m web\u001b[48;5;80min\u001b[48;5;134mar\u001b[0m\n",
            "\u001b[48;5;167m web\u001b[48;5;80minar\u001b[0m\n",
            "\u001b[48;5;167m webinar\u001b[0m\n",
            "\n",
            "\u001b[48;5;167m \u001b[48;5;179mo\u001b[48;5;185mn\u001b[0m\n",
            "\u001b[48;5;167m \u001b[48;5;179mon\u001b[0m\n",
            "\u001b[48;5;167m on\u001b[0m\n",
            "\n",
            "\u001b[48;5;167m \u001b[48;5;179mC\u001b[48;5;185mh\u001b[48;5;77ma\u001b[48;5;80mt\u001b[48;5;68mG\u001b[48;5;134mP\u001b[48;5;167mT\u001b[0m\n",
            "\u001b[48;5;167m \u001b[48;5;179mC\u001b[48;5;185mh\u001b[48;5;77mat\u001b[48;5;68mG\u001b[48;5;134mP\u001b[48;5;167mT\u001b[0m\n",
            "\u001b[48;5;167m C\u001b[48;5;185mh\u001b[48;5;77mat\u001b[48;5;68mG\u001b[48;5;134mP\u001b[48;5;167mT\u001b[0m\n",
            "\u001b[48;5;167m Ch\u001b[48;5;77mat\u001b[48;5;68mG\u001b[48;5;134mP\u001b[48;5;167mT\u001b[0m\n",
            "\u001b[48;5;167m Ch\u001b[48;5;77mat\u001b[48;5;68mG\u001b[48;5;134mPT\u001b[0m\n",
            "\u001b[48;5;167m Chat\u001b[48;5;68mG\u001b[48;5;134mPT\u001b[0m\n",
            "\n",
            "\u001b[48;5;167m \u001b[48;5;179mA\u001b[48;5;185mP\u001b[48;5;77mI\u001b[0m\n",
            "\u001b[48;5;167m A\u001b[48;5;185mP\u001b[48;5;77mI\u001b[0m\n",
            "\u001b[48;5;167m A\u001b[48;5;185mPI\u001b[0m\n",
            "\u001b[48;5;167m API\u001b[0m\n",
            "\n",
            "\u001b[48;5;167m.\u001b[0m\n",
            "\n",
            "[15724, 374, 264, 11364, 1938, 369, 264, 63660, 389, 13149, 38, 2898, 5446, 13]\n",
            "Number of tokens:  14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Understanding Roles in ChatGPT API**\n",
        "\n",
        "In the ChatGPT API, roles are used to define the different entities that can send messages in a conversation. There are three primary roles: `system`, `user`, and `assistant`. Let's understand each role with examples and code snippets:\n",
        "\n",
        "#### **1. System Role**\n",
        "\n",
        "The `system` role is used to set the behavior of the ChatGPT model in a conversation. It helps in defining the context or scope of the conversation.\n",
        "\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a football expert\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())\n",
        "```\n",
        "\n",
        "In this example, the system role instructs the ChatGPT to assume the role of a \"football expert,\" guiding its responses to align with this persona.\n",
        "\n",
        "#### **2. User Role**\n",
        "\n",
        "The `user` role represents the end-user or a human who is interacting with the ChatGPT. It is used to send prompts or questions to the ChatGPT.\n",
        "\n",
        "```python\n",
        "# User role example:\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a football expert\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the FIFA World Cup in 2014?\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())\n",
        "```\n",
        "\n",
        "Here, the user role is utilized to ask a question about the FIFA World Cup 2018, to which the ChatGPT responds accurately.\n",
        "\n",
        "#### **3. Assistant Role**\n",
        "\n",
        "The `assistant` role represents the ChatGPT model itself responding to the user's prompts. It is used to maintain the continuity of the conversation by including previous responses from the model in the current request.\n",
        "\n",
        "```python\n",
        "# Assitant role to maintaine continuity\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a football expert\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the FIFA World Cup in 2014?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Germany won the FIFA World Cup in 2014.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the next world cup after that?\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())\n",
        "```\n",
        "\n",
        "In this snippet, the assistant role is used to include the model's previous response in the current request, ensuring a coherent and continuous conversation.\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Understanding and utilizing roles correctly is vital in crafting conversations with the ChatGPT API. It allows developers to control the behavior of the ChatGPT model, facilitate user interactions, and maintain a smooth conversation flow. By using different roles, one can guide the conversation in a desired direction and obtain responses that align with the specified context or persona."
      ],
      "metadata": {
        "id": "sXQG_ufuaQZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# System Role example:\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a football expert\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1fpRgvwazBI",
        "outputId": "52098f51-edcd-4d05-fd91-e6765b61cdb8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and have been asked to predict the outcome of the upcoming UEFA Champions League final between Chelsea and Manchester City. After analyzing both teams' recent form, playing styles, and key players, I believe Manchester City has a slight edge in this match.\n",
            "\n",
            "Manchester City has had an outstanding season domestically, winning the Premier League title comfortably. They have displayed exceptional attacking prowess, scoring a league-high 83 goals, and are known for their possession-based, free-flowing style of play. With talented players like Kevin De Bruyne, Phil Foden, and Riyad Mahrez, City possesses a deadly attacking force that can unlock any defense.\n",
            "\n",
            "Defensively, Manchester City has been brilliant as well, conceding just 32 goals in 38 league matches. Their rock-solid defense, led by Ruben Dias and John Stones, has been crucial to their success this season.\n",
            "\n",
            "On the other hand, Chelsea has also shown tremendous improvement since Thomas Tuchel took charge. They reached the final of the FA Cup and finished fourth in the Premier League under his guidance. Tuchel's tactical approach has made Chelsea more solid defensively, and they have become hard to break down.\n",
            "\n",
            "Chelsea's midfield duo of N'Golo Kanté and Jorginho has been exceptional, controlling the tempo of games and breaking down opponents' attacks effectively. Adding to their strength is the attacking threat posed by players like Kai Havertz and Mason Mount, who have been in good form recently.\n",
            "\n",
            "However, despite Chelsea's impressive performances, I believe Manchester City will come out on top in the final. City's superior attacking firepower and their ability to control possession will likely give them the upper hand. Their vast experience under Pep Guardiola's management in big games will also play a significant role.\n",
            "\n",
            "Nevertheless, football can be unpredictable, and anything can happen in a one-off final. Both teams are well capable of winning, and it promises to be an exciting and closely contested match.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User role example:\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a football expert\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the FIFA World Cup in 2014?\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2uRqAgwa5ET",
        "outputId": "a4d2afa1-e747-4dec-e480-4a2415c8b8d1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The FIFA World Cup in 2014 was won by Germany. They defeated Argentina 1-0 in the final, with Mario Götze scoring the winning goal in extra time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Without assitant role to maintaine continuity\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a football expert\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the next FIFA World Cup?\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzV6iapfwMtE",
        "outputId": "3b2f745d-fb88-46a5-c85a-6663d16a85ce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As an AI, I don't have real-time information or the ability to predict future events. Therefore, I cannot accurately tell you who will win the next FIFA World Cup. The next World Cup will take place in Qatar in 2022, and only time will tell which team will emerge as the victor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assitant role to maintaine continuity\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a football expert\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the FIFA World Cup in 2014?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Germany won the FIFA World Cup in 2014.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the next world cup after that?\"},\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=messages,\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0].message[\"content\"].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPdlndyybBlS",
        "outputId": "07fef6da-9168-4237-8cc0-379f31dc3697"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The next FIFA World Cup after 2014 was held in 2018, and it was won by France.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xHbc27shwbQX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}